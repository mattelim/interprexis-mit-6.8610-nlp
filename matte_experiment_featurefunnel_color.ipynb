{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple torch model with 1 fully connected layer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "color_words = [\n",
    "    'aqua', 'aquamarine', 'azure', 'beige', 'bisque',\n",
    "    'chartreuse', 'chocolate', 'coral', 'crimson', 'cyan', 'firebrick', 'fuchsia',\n",
    "    'gold', 'gray', 'indigo', 'ivory', 'khaki', 'lavender', 'lime', 'magenta',\n",
    "    'maroon', 'navy', 'olive', 'orchid', 'plum', \n",
    "    'salmon', 'sienna', 'silver', 'tan', 'teal', 'tomato', 'turquoise', \n",
    "    'wheat', 'sienna', 'ochre', 'umber', 'sepia', 'vermillion',\n",
    "    'carmine', 'cerulean', 'auburn', 'viridian', 'ultramarine', 'emerald'\n",
    "]\n",
    "\n",
    "most_common_color_words = [\n",
    "    'red', 'green', 'blue', 'yellow', 'orange', 'purple', 'brown', 'pink', 'violet', 'white', 'black'\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "color_words = list(set(color_words))\n",
    "most_common_color_words = list(set(most_common_color_words))\n",
    "\n",
    "print(len(color_words))\n",
    "print(len(most_common_color_words))\n",
    "\n",
    "assert len(list(set(color_words + most_common_color_words))) == len(color_words) + len(most_common_color_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "non_color_words = [\n",
    "    \"dog\", \"cat\", \"house\", \"car\", \"book\", \"computer\", \"table\", \"chair\", \"tree\", \"river\",\n",
    "    \"song\", \"movie\", \"friend\", \"family\", \"phone\", \"city\", \"food\", \"drink\", \"shoe\", \"hat\",\n",
    "    \"jacket\", \"pen\", \"paper\", \"cloud\", \"sun\", \"moon\", \"star\", \"road\", \"bridge\", \"key\",\n",
    "    \"lock\", \"door\", \"window\", \"mirror\", \"clock\", \"lamp\", \"flower\", \"bird\", \"fish\", \"ship\",\n",
    "    \"plane\", \"train\", \"bus\", \"child\", \"adult\", \"student\", \"teacher\", \"doctor\", \"engineer\",\n",
    "    \"artist\", \"writer\", \"singer\", \"actor\", \"politician\", \"lawyer\", \"doctor\", \"patient\",\n",
    "    \"dream\", \"memory\", \"idea\", \"emotion\", \"love\", \"hate\", \"fear\", \"joy\", \"anger\", \"hope\",\n",
    "    \"doubt\", \"peace\", \"war\", \"freedom\", \"justice\", \"truth\", \"lie\", \"friendship\", \"loneliness\",\n",
    "    \"success\", \"failure\", \"wealth\", \"poverty\", \"nature\", \"technology\", \"culture\", \"history\",\n",
    "    \"science\", \"religion\", \"politics\", \"economy\", \"society\", \"language\", \"knowledge\", \"wisdom\"\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "non_color_words = list(set(non_color_words))\n",
    "\n",
    "print(len(non_color_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of words and returns a list of embeddings\n",
    "def get_embeddings(words):\n",
    "    embeddings = []\n",
    "\n",
    "    for word in words:\n",
    "        input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())\n",
    "    \n",
    "    assert len(embeddings) == len(words)\n",
    "    return embeddings\n",
    "\n",
    "color_embeddings = get_embeddings(color_words)\n",
    "most_common_color_embeddings = get_embeddings(most_common_color_words)\n",
    "non_color_embeddings = get_embeddings(non_color_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word                                          embedding  label\n",
      "0  ultramarine  [0.24822497367858887, -0.5821539759635925, -0....      1\n",
      "1        ivory  [0.16264645755290985, 0.28781405091285706, -0....      1\n",
      "2       sienna  [0.31997525691986084, -0.5059263706207275, 0.0...      1\n",
      "3          tan  [-0.0007681101560592651, -0.07876596599817276,...      1\n",
      "4        olive  [-0.16430577635765076, 0.23664475977420807, -0...      1\n",
      "         word                                          embedding  label\n",
      "109   science  [-0.12869563698768616, 0.1851356029510498, -0....      0\n",
      "110       car  [0.3777954578399658, -0.013787495903670788, -0...      0\n",
      "111   emotion  [-0.494671493768692, 0.11225312203168869, -0.1...      0\n",
      "112     actor  [-0.1514865756034851, 0.24995048344135284, -0....      0\n",
      "113  religion  [0.2626156508922577, 0.5329587459564209, -0.28...      0\n",
      "         word                                          embedding  label\n",
      "0  vermillion  [-0.2022084891796112, -0.6616164445877075, 0.2...      1\n",
      "1      tomato  [0.33348119258880615, 0.33904969692230225, -0....      1\n",
      "2      silver  [0.19768138229846954, 0.1486184000968933, -0.1...      1\n",
      "3       coral  [0.014441009610891342, 0.19550518691539764, -0...      1\n",
      "4     magenta  [0.3218657970428467, -0.3043861389160156, -0.0...      1\n",
      "       word                                          embedding  label\n",
      "26     road  [0.4005007743835449, 0.2577981650829315, -0.04...      0\n",
      "27      dog  [0.34253600239753723, 0.06648474186658859, -0....      0\n",
      "28      pen  [0.5238260626792908, 0.11008309572935104, -0.2...      0\n",
      "29   wisdom  [0.3080526292324066, 0.17283137142658234, -0.1...      0\n",
      "30  society  [-0.2502453923225403, -0.1463952511548996, -0....      0\n"
     ]
    }
   ],
   "source": [
    "# slice the lists into training and test sets\n",
    "color_words_train = color_words[:int(len(color_words)*0.8)]\n",
    "color_words_test = color_words[int(len(color_words)*0.8):]\n",
    "color_embeddings_train = color_embeddings[:int(len(color_embeddings)*0.8)]\n",
    "color_embeddings_test = color_embeddings[int(len(color_embeddings)*0.8):]\n",
    "\n",
    "most_common_color_words_train = most_common_color_words[:int(len(most_common_color_words)*0.8)]\n",
    "most_common_color_words_test = most_common_color_words[int(len(most_common_color_words)*0.8):]\n",
    "most_common_color_embeddings_train = most_common_color_embeddings[:int(len(most_common_color_embeddings)*0.8)]\n",
    "most_common_color_embeddings_test = most_common_color_embeddings[int(len(most_common_color_embeddings)*0.8):]\n",
    "\n",
    "non_color_words_train = non_color_words[:int(len(non_color_words)*0.8)]\n",
    "non_color_words_test = non_color_words[int(len(non_color_words)*0.8):]\n",
    "non_color_embeddings_train = non_color_embeddings[:int(len(non_color_embeddings)*0.8)]\n",
    "non_color_embeddings_test = non_color_embeddings[int(len(non_color_embeddings)*0.8):]\n",
    "\n",
    "# create a dataframe with the training and test sets\n",
    "df_train = pd.DataFrame({\n",
    "    'word': color_words_train + most_common_color_words_train + non_color_words_train,\n",
    "    'embedding': color_embeddings_train + most_common_color_embeddings_train + non_color_embeddings_train,\n",
    "    'label': [1]*len(color_words_train) + [1]*len(most_common_color_words_train) + [0]*len(non_color_words_train)\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'word': color_words_test + most_common_color_words_test + non_color_words_test,\n",
    "    'embedding': color_embeddings_test + most_common_color_embeddings_test + non_color_embeddings_test,\n",
    "    'label': [1]*len(color_words_test) + [1]*len(most_common_color_words_test) + [0]*len(non_color_words_test)\n",
    "})\n",
    "\n",
    "# shuffle the dataframes\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df_train[:5])\n",
    "print(df_train[-5:])\n",
    "print(df_test[:5])\n",
    "print(df_test[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = torch.tensor(tokenized_texts)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train['embedding'], df_train['label'])\n",
    "test_dataset = CustomDataset(df_test['embedding'], df_test['label'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.5581544563174248\n",
      "Epoch 10, loss=0.028347435989417136\n",
      "Epoch 20, loss=0.010821076430147514\n",
      "Epoch 30, loss=0.006283213515416719\n",
      "Epoch 40, loss=0.004778077709488571\n",
      "Epoch 50, loss=0.00310863385675475\n",
      "Epoch 60, loss=0.0026184962480328977\n",
      "Epoch 70, loss=0.0020052643812960014\n",
      "Epoch 80, loss=0.0015348755623563193\n",
      "Epoch 90, loss=0.001361042304779403\n",
      "Epoch 100, loss=0.0011844615437439643\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader)}')\n",
    "        # print(combined_loss / len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of variance over different runs\n",
    "Best so far: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 100.00%\n",
      "Accuracy on test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # get data to GPU if possible\n",
    "            data = batch[0].to(device=device)\n",
    "            targets = batch[1].to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    \n",
    "    return num_correct / num_samples\n",
    "\n",
    "print(f'Accuracy on training set: {check_accuracy(train_loader, net)*100:.2f}%')\n",
    "print(f'Accuracy on test set: {check_accuracy(test_loader, net)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n",
      "(10,)\n",
      "[308 733 727 752 372 381 251 666 191 414]\n",
      "[0.1024159  0.03050245 0.02732667 0.02579284 0.0254136  0.02382957\n",
      " 0.02367691 0.02188338 0.02164853 0.02030942]\n",
      "(10,)\n",
      "[473 163 722 314 472 446  40 590 284 135]\n",
      "[0.01946797 0.01939146 0.01858333 0.01790875 0.01757148 0.0161675\n",
      " 0.01464333 0.01424586 0.01409603 0.01402995]\n"
     ]
    }
   ],
   "source": [
    "# Find the most important features of the model\n",
    "# get the gradients of the model\n",
    "net.eval()\n",
    "gradients = []\n",
    "for batch in train_loader:\n",
    "    # get data to GPU if possible\n",
    "    data = batch[0].to(device=device)\n",
    "    targets = batch[1].to(device=device)\n",
    "\n",
    "    # forward\n",
    "    scores = net(data)\n",
    "    loss = criterion(scores, targets)\n",
    "    loss.backward()\n",
    "    gradients.append(net.fc1.weight.grad.detach().cpu().numpy())\n",
    "\n",
    "# average the gradients\n",
    "gradients = np.mean(gradients, axis=0)\n",
    "print(gradients.shape)\n",
    "\n",
    "# print(len(gradients))\n",
    "# print(len(gradients[0]))\n",
    "# print(len(gradients[0][0]))\n",
    "\n",
    "# find the indices of the 10 most important features\n",
    "most_important_gradients_0 = gradients[0].argsort()[-10:][::-1]\n",
    "most_important_gradients_1 = gradients[1].argsort()[-10:][::-1]\n",
    "\n",
    "# print the 10 most important features\n",
    "print(most_important_gradients_0.shape)\n",
    "print(most_important_gradients_0)\n",
    "print(gradients[0][most_important_gradients_0])\n",
    "\n",
    "print(most_important_gradients_1.shape)\n",
    "print(most_important_gradients_1)\n",
    "print(gradients[1][most_important_gradients_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135, 666, 284, 414, 163, 40, 308, 314, 446, 191, 590, 722, 727, 472, 473, 733, 752, 372, 251, 381]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "combined_importance_grad = list(set(list(most_important_gradients_0) + list(most_important_gradients_1)))\n",
    "print(combined_importance_grad)\n",
    "print(len(combined_importance_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[696 590 472 360 650  47 100  37 663 314]\n",
      "[0.22362967 0.20893602 0.20430925 0.20341647 0.19949287 0.19481428\n",
      " 0.19434766 0.19377029 0.19191854 0.19186114]\n",
      "(10,)\n",
      "[511 611 267 719 271  14  82 626 238  10]\n",
      "[0.31380597 0.29059002 0.25619408 0.2505709  0.2479416  0.24274242\n",
      " 0.23808081 0.23435041 0.2332934  0.2308459 ]\n"
     ]
    }
   ],
   "source": [
    "# Find the most important features for the model\n",
    "# get the weights of the fully connected layer\n",
    "weights = net.fc1.weight.data.cpu().numpy()\n",
    "\n",
    "# find the indices of the 10 most important features\n",
    "most_important_features_0 = weights[0].argsort()[-10:][::-1]\n",
    "most_important_features_1 = weights[1].argsort()[-10:][::-1]\n",
    "\n",
    "# print the 10 most important features and their weights\n",
    "print(most_important_features_0.shape)\n",
    "print(most_important_features_0)\n",
    "print(weights[0][most_important_features_0])\n",
    "\n",
    "print(most_important_features_1.shape)\n",
    "print(most_important_features_1)\n",
    "print(weights[1][most_important_features_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650, 267, 10, 14, 271, 663, 37, 47, 696, 314, 590, 719, 82, 472, 611, 100, 360, 238, 626, 511]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "combined_importance = list(set(list(most_important_features_0) + list(most_important_features_1)))\n",
    "print(combined_importance)\n",
    "print(len(combined_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_small, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word                                          embedding  label\n",
      "0  ultramarine  [0.3691755533218384, 0.12182317674160004, -0.1...      1\n",
      "1        ivory  [-0.8768669366836548, 0.01354244165122509, 0.0...      1\n",
      "2       sienna  [-0.4798089563846588, -0.05054834485054016, 0....      1\n",
      "3          tan  [-0.07974042743444443, 0.3735435903072357, -0....      1\n",
      "4        olive  [0.117793507874012, 0.19942378997802734, -0.02...      1\n",
      "         word                                          embedding  label\n",
      "109   science  [0.19104573130607605, -0.30567416548728943, -0...      0\n",
      "110       car  [-0.04580724239349365, 0.048196613788604736, 0...      0\n",
      "111   emotion  [0.3724481463432312, -0.12157141417264938, -0....      0\n",
      "112     actor  [0.47917813062667847, -0.08011671900749207, 0....      0\n",
      "113  religion  [0.05262870341539383, -0.08452796190977097, -0...      0\n",
      "         word                                          embedding  label\n",
      "0  vermillion  [0.5471291542053223, 0.05379761755466461, -0.0...      1\n",
      "1      tomato  [0.22750723361968994, -0.0013144873082637787, ...      1\n",
      "2      silver  [0.29420915246009827, 0.2140081822872162, -0.1...      1\n",
      "3       coral  [-0.040319815278053284, 0.12037064880132675, 0...      1\n",
      "4     magenta  [0.8496570587158203, 0.19482791423797607, -0.0...      1\n",
      "       word                                          embedding  label\n",
      "26     road  [-0.004823602735996246, 0.06815699487924576, -...      0\n",
      "27      dog  [-0.3701101243495941, 0.13678257167339325, -0....      0\n",
      "28      pen  [0.0011142529547214508, -0.027050446718931198,...      0\n",
      "29   wisdom  [-0.06984914839267731, 0.09383425861597061, -0...      0\n",
      "30  society  [0.5898301601409912, -0.36556392908096313, -0....      0\n",
      "\n",
      "Length of reduced embedding:  20\n"
     ]
    }
   ],
   "source": [
    "# create datasets with the 20 most important features\n",
    "df_train_small = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['embedding'].apply(lambda x: [x[i] for i in combined_importance]),\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "df_test_small = pd.DataFrame({\n",
    "    'word': df_test['word'],\n",
    "    'embedding': df_test['embedding'].apply(lambda x: [x[i] for i in combined_importance]),\n",
    "    'label': df_test['label']\n",
    "})\n",
    "\n",
    "print(df_train_small[:5])\n",
    "print(df_train_small[-5:])\n",
    "print(df_test_small[:5])\n",
    "print(df_test_small[-5:])\n",
    "\n",
    "print(\"\\nLength of reduced embedding: \", len(df_train_small['embedding'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset_small = CustomDataset(df_train_small['embedding'], df_train_small['label'])\n",
    "test_dataset_small = CustomDataset(df_test_small['embedding'], df_test_small['label'])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader_small = DataLoader(train_dataset_small, batch_size=16, shuffle=True)\n",
    "test_loader_small = DataLoader(test_dataset_small, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.6927627995610237\n",
      "Epoch 10, loss=0.6904962435364723\n",
      "Epoch 20, loss=0.6927627995610237\n",
      "Epoch 30, loss=0.6941152662038803\n",
      "Epoch 40, loss=0.6888934075832367\n",
      "Epoch 50, loss=0.6982776075601578\n",
      "Epoch 60, loss=0.6929636225104332\n",
      "Epoch 70, loss=0.6935726255178452\n",
      "Epoch 80, loss=0.6927627921104431\n",
      "Epoch 90, loss=0.6936252266168594\n",
      "Epoch 100, loss=0.693363830447197\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net2 = Net_small()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net2.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net2.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader_small:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net2(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader_small)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader_small)}')\n",
    "        # print(combined_loss / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word                                          embedding  label\n",
      "0  ultramarine  [0.320054292678833, 0.5433744192123413, -0.113...      1\n",
      "1        ivory  [-0.585979163646698, -0.5540187358856201, 0.08...      1\n",
      "2       sienna  [-0.18103229999542236, -0.20594161748886108, 0...      1\n",
      "3          tan  [-0.2616365849971771, -0.29962193965911865, 0....      1\n",
      "4        olive  [-0.06778931617736816, -0.4506070911884308, 0....      1\n",
      "         word                                          embedding  label\n",
      "109   science  [-0.3798688054084778, -0.15420736372470856, 0....      0\n",
      "110       car  [0.07106814533472061, -0.2173701375722885, 0.3...      0\n",
      "111   emotion  [-0.15966369211673737, -0.3579808473587036, 0....      0\n",
      "112     actor  [0.06760074943304062, -0.6156966686248779, -0....      0\n",
      "113  religion  [-0.01598823070526123, -0.15939755737781525, 0...      0\n",
      "         word                                          embedding  label\n",
      "0  vermillion  [0.29682737588882446, -0.12140871584415436, -0...      1\n",
      "1      tomato  [0.03990205377340317, 0.025346539914608, 0.403...      1\n",
      "2      silver  [-0.8129631280899048, -0.4670772850513458, -0....      1\n",
      "3       coral  [-0.1224033460021019, 0.023583784699440002, 0....      1\n",
      "4     magenta  [0.28278863430023193, -0.39333194494247437, -0...      1\n",
      "       word                                          embedding  label\n",
      "26     road  [-0.4607456624507904, -0.3468010425567627, 0.0...      0\n",
      "27      dog  [0.26209911704063416, -0.6376949548721313, 0.0...      0\n",
      "28      pen  [0.07013999670743942, -0.29889634251594543, 0....      0\n",
      "29   wisdom  [0.09484054893255234, -0.41208019852638245, 0....      0\n",
      "30  society  [0.4859743118286133, -0.40387794375419617, 0.5...      0\n",
      "\n",
      "Length of reduced embedding:  20\n"
     ]
    }
   ],
   "source": [
    "# create datasets with the 20 most important features by gradients\n",
    "df_train_small_grad = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['embedding'].apply(lambda x: [x[i] for i in combined_importance_grad]),\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "df_test_small_grad = pd.DataFrame({\n",
    "    'word': df_test['word'],\n",
    "    'embedding': df_test['embedding'].apply(lambda x: [x[i] for i in combined_importance_grad]),\n",
    "    'label': df_test['label']\n",
    "})\n",
    "\n",
    "print(df_train_small_grad[:5])\n",
    "print(df_train_small_grad[-5:])\n",
    "print(df_test_small_grad[:5])\n",
    "print(df_test_small_grad[-5:])\n",
    "\n",
    "print(\"\\nLength of reduced embedding: \", len(df_train_small_grad['embedding'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset_small_grad = CustomDataset(df_train_small_grad['embedding'], df_train_small_grad['label'])\n",
    "test_dataset_small_grad = CustomDataset(df_test_small_grad['embedding'], df_test_small_grad['label'])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader_small_grad = DataLoader(train_dataset_small_grad, batch_size=16, shuffle=True)\n",
    "test_loader_small_grad = DataLoader(test_dataset_small_grad, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.6035684198141098\n",
      "Epoch 10, loss=0.6862051263451576\n",
      "Epoch 20, loss=0.6025184318423271\n",
      "Epoch 30, loss=0.6017399318516254\n",
      "Epoch 40, loss=0.5993422865867615\n",
      "Epoch 50, loss=0.6391497701406479\n",
      "Epoch 60, loss=0.6371235996484756\n",
      "Epoch 70, loss=0.6007024198770523\n",
      "Epoch 80, loss=0.6436096280813217\n",
      "Epoch 90, loss=0.5970332324504852\n",
      "Epoch 100, loss=0.5979795902967453\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net3 = Net_small()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net3.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net3.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader_small_grad:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net3(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader_small_grad)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader_small_grad)}')\n",
    "        # print(combined_loss / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6861",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
