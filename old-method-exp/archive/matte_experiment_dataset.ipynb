{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation using Penn Treebank and expanded set from Coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing an environment like conda is recommended. This notebook last ran on Python 3.8.18 without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: transformers in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (4.35.0)\n",
      "Requirement already satisfied: numpy in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: pandas in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: nltk in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: fsspec in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate transformers numpy pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# load in coco classes from 'coco-classes.json'\n",
    "import json\n",
    "with open('coco-classes.json') as f:\n",
    "  coco_classes = json.load(f)\n",
    "print(coco_classes)\n",
    "print(len(coco_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PennTreebank Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Penn Treebank POS tagset \n",
    "\n",
    "1. CC  Coordinating conjunction  \n",
    "2. CD  Cardinal number           \n",
    "3. DT  Determiner                \n",
    "4. EX  Existential there  \t     \n",
    "5. FW  Foreign word              \n",
    "6. IN  Preposition/subord. conjunction \t   \n",
    "7. JJ  Adjective                  \n",
    "8. JJR Adjective, comparative    \n",
    "9. JJS Adjective, superlative    \n",
    "10. LS  List item marker          \n",
    "11. MD  Modal                     \n",
    "12. NN  Noun, singular or mass    \n",
    "13. NNS Noun, plural              \n",
    "14. NNP Proper noun, singular     \n",
    "15. NNPS Proper noun, plural      \n",
    "16. PDT Predeterminer             \n",
    "17. POS Possessive ending         \n",
    "18. PRP Personal pronoun          \n",
    "19. PP  Possessive pronoun        \n",
    "20. RB  Adverb                    \n",
    "21. RBR Adverb, comparative       \n",
    "22. RBS Adverb, superlative       \n",
    "23. RP  Particle                  \n",
    "24. SYM Symbol \t\t\t             \n",
    "25. TO  to \n",
    "26. UH  Interjection \n",
    "27. VB  Verb, base form \n",
    "28. VBD Verb, past tense \n",
    "29. VBG Verb, gerund/present participle \n",
    "30. VBN Verb, past participle \n",
    "31. VBP Verb, non-3rd ps. sing. present\n",
    "32. VBZ Verb, 3rd ps. sing. present \n",
    "33. WDT wh-determiner \n",
    "34. WP  wh-pronoun \n",
    "35. WP  Possessive wh-pronoun \n",
    "36. WRB wh-adverb \n",
    "37. \\#  Pound sign \n",
    "38. $  Dollar sign \n",
    "39. .  Sentence-final punctuation \n",
    "40. ,  Comma \n",
    "41. :  Colon, semi-colon \n",
    "42. (  Left bracket character \n",
    "43. )  Right bracket character \n",
    "44. \"  Straight double quote \n",
    "45. `  Left open single quote \n",
    "46. \"  Left open double quote \n",
    "47. '  Right close single quote \n",
    "48. \"  Right close double quote\n",
    "\n",
    "For examples: https://www.sketchengine.eu/penn-treebank-tagset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/mattelim/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3914\n",
      "Number of tagged words: 100676\n",
      "\n",
      "Sample of the dataset:\n",
      "Sentence 1: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "POS tags: [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "\n",
      "Sentence 2: ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.']\n",
      "POS tags: [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]\n",
      "\n",
      "Sentence 3: ['Rudolph', 'Agnew', ',', '55', 'years', 'old', 'and', 'former', 'chairman', 'of', 'Consolidated', 'Gold', 'Fields', 'PLC', ',', 'was', 'named', '*-1', 'a', 'nonexecutive', 'director', 'of', 'this', 'British', 'industrial', 'conglomerate', '.']\n",
      "POS tags: [('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('*-1', '-NONE-'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('of', 'IN'), ('this', 'DT'), ('British', 'JJ'), ('industrial', 'JJ'), ('conglomerate', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "Pandas DataFrame:\n",
      "     Word  POS\n",
      "0  Pierre  NNP\n",
      "1  Vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n"
     ]
    }
   ],
   "source": [
    "# Download the Penn Treebank dataset\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Load the Penn Treebank dataset\n",
    "ptb_sentences = treebank.sents()\n",
    "ptb_tagged_words = treebank.tagged_words()\n",
    "\n",
    "# Display some information about the dataset\n",
    "print(f\"Number of sentences: {len(ptb_sentences)}\")\n",
    "print(f\"Number of tagged words: {len(ptb_tagged_words)}\")\n",
    "\n",
    "# Display the first few sentences and their POS tags\n",
    "print(\"\\nSample of the dataset:\")\n",
    "current_word_position = 0\n",
    "for i in range(3):\n",
    "    print(f\"Sentence {i + 1}: {ptb_sentences[i]}\")\n",
    "    ending_word_position = current_word_position + len(ptb_sentences[i])\n",
    "    print(f\"POS tags: {ptb_tagged_words[current_word_position:ending_word_position]}\")\n",
    "    current_word_position = ending_word_position\n",
    "    print()\n",
    "\n",
    "# Convert the dataset to Pandas DataFrame for exploration\n",
    "columns = ['Word', 'POS']\n",
    "ptb_df = pd.DataFrame(data={'Word': [word for (word, _) in ptb_tagged_words],\n",
    "                            'POS': [pos for (_, pos) in ptb_tagged_words]}, columns=columns)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nPandas DataFrame:\")\n",
    "print(ptb_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100676, 2)\n"
     ]
    }
   ],
   "source": [
    "print(ptb_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to csv\n",
    "ptb_df.to_csv('ptb_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13781, 3)\n"
     ]
    }
   ],
   "source": [
    "# reduce dataframe to unique words\n",
    "# ptb_df_unique = ptb_df.drop_duplicates(subset=['Word'])\n",
    "# print(ptb_df_unique.shape)\n",
    "\n",
    "# reduce dataframe to unique words, count the number of times each word appears and add as a column, while keeping the POS column\n",
    "ptb_df_unique = ptb_df.groupby(['Word', 'POS']).size().reset_index(name='count')\n",
    "# ptb_df_unique = ptb_df.drop_duplicates(subset=['Word'])\n",
    "print(ptb_df_unique.shape)\n",
    "\n",
    "# save the dataframe to csv\n",
    "ptb_df_unique.to_csv('ptb_df_unique.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13341, 3)\n",
      "(13337, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove '-NONE-' POS tags\n",
    "ptb_df_unique = ptb_df_unique[ptb_df_unique['POS'] != '-NONE-']\n",
    "print(ptb_df_unique.shape)\n",
    "\n",
    "# Remove '-LRB-' '-RRB-' POS tags\n",
    "ptb_df_unique = ptb_df_unique[ptb_df_unique['POS'] != '-LRB-']\n",
    "ptb_df_unique = ptb_df_unique[ptb_df_unique['POS'] != '-RRB-']\n",
    "print(ptb_df_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 3)\n",
      "  Word POS  count\n",
      "0    #   #     16\n",
      "1    $   $    718\n",
      "2  US$   $      4\n",
      "3   C$   $      2\n",
      "4   ''  ''    684\n",
      "5    '  ''     10\n",
      "6    ,   ,   4885\n",
      "7   Wa   ,      1\n",
      "8    .   .   3828\n",
      "9    ?   .     40\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe that contains the 5 most frequent words of each POS tag\n",
    "ptb_df_top5 = ptb_df_unique.groupby('POS').apply(lambda x: x.nlargest(5, 'count')).reset_index(drop=True)\n",
    "print(ptb_df_top5.shape)\n",
    "print(ptb_df_top5.head(10))\n",
    "\n",
    "# save the dataframe to csv\n",
    "ptb_df_top5.to_csv('ptb_df_top5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get 100 most common words\n",
    "# ptb_df_unique.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "# ptb_df_unique_top100 = ptb_df_unique.head(100)\n",
    "# print(ptb_df_unique_top100.shape)\n",
    "\n",
    "# # save the dataframe to csv\n",
    "# ptb_df_unique_top100.to_csv('ptb_df_unique_top100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "{'elegance', 'vibration', 'protection', 'dry', 'uncork', 'nurture', 'drive', 'growth', 'bite', 'celebration', 'extinguish', 'mobility', 'song', 'hygiene', 'excitement', 'munch', 'decoration', 'fly', 'fasten', 'containment', 'graze', 'tick', 'drink', 'measure', 'crunch', 'cooling', 'container', 'pattern', 'cutlery', 'score', 'companion', 'measurement', 'prong', 'adventure', 'stride', 'sip', 'comfort', 'descent', 'cutting', 'formality', 'cut', 'roar', 'breakfast', 'majesty', 'recreation', 'knowledge', 'conformity', 'gallop', 'catch', 'scoop', 'hold', 'read', 'journey', 'walk', 'heat', 'accessory', 'indulge', 'peel', 'sharpness', 'competition', 'drying', 'chop', 'childhood', 'gathering', 'heating', 'safety', 'devour', 'lounge', 'chill', 'rinse', 'entertainment', 'click', 'aroma', 'poke', 'swing', 'signal', 'time', 'nutrient', 'health', 'halt', 'loyalty', 'indulgence', 'dine', 'commute', 'sanitation', 'ride', 'productivity', 'hug', 'navigation', 'support', 'bark', 'glide', 'pause', 'sail', 'shield', 'flutter', 'brush', 'control', 'shipment', 'tote', 'carry', 'consume', 'individuality', 'voyage', 'surf', 'acidity', 'bake', 'soar', 'purr', 'rest', 'crave', 'flight', 'moo', 'skate', 'grace', 'reflection', 'slice', 'thrill', 'hit', 'flush', 'communication', 'haul', 'throw', 'sit', 'Thanksgiving', 'eat', 'watch', 'call', 'input', 'descend', 'trumpet', 'drainage', 'solitude', 'combination', 'softness', 'type', 'transportation', 'transit', 'toast'}\n"
     ]
    }
   ],
   "source": [
    "# Generated using ChatGPT 3.5, not foolproof\n",
    "\n",
    "verb_equivalents = {\n",
    "    'person': 'walk',\n",
    "    'bicycle': 'ride',\n",
    "    'car': 'drive',\n",
    "    'motorcycle': 'ride',\n",
    "    'airplane': 'fly',\n",
    "    'bus': 'transit',\n",
    "    'train': 'commute',\n",
    "    'truck': 'haul',\n",
    "    'boat': 'sail',\n",
    "    'traffic light': 'control',\n",
    "    'fire hydrant': 'extinguish',\n",
    "    'stop sign': 'halt',\n",
    "    'parking meter': 'measure',\n",
    "    'bench': 'sit',\n",
    "    'bird': 'soar',\n",
    "    'cat': 'purr',\n",
    "    'dog': 'bark',\n",
    "    'horse': 'gallop',\n",
    "    'sheep': 'graze',\n",
    "    'cow': 'moo',\n",
    "    'elephant': 'trumpet',\n",
    "    'bear': 'roar',\n",
    "    'zebra': 'stride',\n",
    "    'giraffe': 'graze',\n",
    "    'backpack': 'carry',\n",
    "    'umbrella': 'shield',\n",
    "    'handbag': 'tote',\n",
    "    'tie': 'fasten',\n",
    "    'suitcase': 'carry',\n",
    "    'frisbee': 'throw',\n",
    "    'skis': 'descend',\n",
    "    'snowboard': 'glide',\n",
    "    'sports ball': 'throw',\n",
    "    'kite': 'flutter',\n",
    "    'baseball bat': 'swing',\n",
    "    'baseball glove': 'catch',\n",
    "    'skateboard': 'skate',\n",
    "    'surfboard': 'surf',\n",
    "    'tennis racket': 'hit',\n",
    "    'bottle': 'uncork',\n",
    "    'wine glass': 'sip',\n",
    "    'cup': 'drink',\n",
    "    'fork': 'poke',\n",
    "    'knife': 'slice',\n",
    "    'spoon': 'scoop',\n",
    "    'bowl': 'eat',\n",
    "    'banana': 'peel',\n",
    "    'apple': 'bite',\n",
    "    'sandwich': 'devour',\n",
    "    'orange': 'peel',\n",
    "    'broccoli': 'munch',\n",
    "    'carrot': 'chop',\n",
    "    'hot dog': 'consume',\n",
    "    'pizza': 'devour',\n",
    "    'donut': 'crave',\n",
    "    'cake': 'indulge',\n",
    "    'chair': 'sit',\n",
    "    'couch': 'lounge',\n",
    "    'potted plant': 'nurture',\n",
    "    'bed': 'rest',\n",
    "    'dining table': 'dine',\n",
    "    'toilet': 'flush',\n",
    "    'tv': 'watch',\n",
    "    'laptop': 'type',\n",
    "    'mouse': 'click',\n",
    "    'remote': 'control',\n",
    "    'keyboard': 'type',\n",
    "    'cell phone': 'call',\n",
    "    'microwave': 'heat',\n",
    "    'oven': 'bake',\n",
    "    'toaster': 'toast',\n",
    "    'sink': 'rinse',\n",
    "    'refrigerator': 'chill',\n",
    "    'book': 'read',\n",
    "    'clock': 'tick',\n",
    "    'vase': 'hold',\n",
    "    'scissors': 'cut',\n",
    "    'teddy bear': 'hug',\n",
    "    'hair drier': 'dry',\n",
    "    'toothbrush': 'brush'\n",
    "}\n",
    "\n",
    "abstract_equivalents = {\n",
    "    'person': 'individuality',\n",
    "    'bicycle': 'mobility',\n",
    "    'car': 'transportation',\n",
    "    'motorcycle': 'vibration',\n",
    "    'airplane': 'flight',\n",
    "    'bus': 'transit',\n",
    "    'train': 'journey',\n",
    "    'truck': 'shipment',\n",
    "    'boat': 'voyage',\n",
    "    'traffic light': 'signal',\n",
    "    'fire hydrant': 'safety',\n",
    "    'stop sign': 'pause',\n",
    "    'parking meter': 'measurement',\n",
    "    'bench': 'reflection',\n",
    "    'bird': 'song',\n",
    "    'cat': 'companion',\n",
    "    'dog': 'loyalty',\n",
    "    'horse': 'grace',\n",
    "    'sheep': 'conformity',\n",
    "    'cow': 'moo',\n",
    "    'elephant': 'majesty',\n",
    "    'bear': 'solitude',\n",
    "    'zebra': 'pattern',\n",
    "    'giraffe': 'elegance',\n",
    "    'backpack': 'adventure',\n",
    "    'umbrella': 'protection',\n",
    "    'handbag': 'accessory',\n",
    "    'tie': 'formality',\n",
    "    'suitcase': 'journey',\n",
    "    'frisbee': 'recreation',\n",
    "    'skis': 'glide',\n",
    "    'snowboard': 'descent',\n",
    "    'sports ball': 'competition',\n",
    "    'kite': 'soar',\n",
    "    'baseball bat': 'swing',\n",
    "    'baseball glove': 'protection',\n",
    "    'skateboard': 'thrill',\n",
    "    'surfboard': 'excitement',\n",
    "    'tennis racket': 'score',\n",
    "    'bottle': 'containment',\n",
    "    'wine glass': 'celebration',\n",
    "    'cup': 'containment',\n",
    "    'fork': 'prong',\n",
    "    'knife': 'sharpness',\n",
    "    'spoon': 'cutlery',\n",
    "    'bowl': 'container',\n",
    "    'banana': 'softness',\n",
    "    'apple': 'crunch',\n",
    "    'sandwich': 'combination',\n",
    "    'orange': 'acidity',\n",
    "    'broccoli': 'nutrient',\n",
    "    'carrot': 'health',\n",
    "    'hot dog': 'indulgence',\n",
    "    'pizza': 'aroma',\n",
    "    'donut': 'indulgence',\n",
    "    'cake': 'celebration',\n",
    "    'chair': 'support',\n",
    "    'couch': 'comfort',\n",
    "    'potted plant': 'growth',\n",
    "    'bed': 'rest',\n",
    "    'dining table': 'gathering',\n",
    "    'toilet': 'sanitation',\n",
    "    'tv': 'entertainment',\n",
    "    'laptop': 'productivity',\n",
    "    'mouse': 'navigation',\n",
    "    'remote': 'control',\n",
    "    'keyboard': 'input',\n",
    "    'cell phone': 'communication',\n",
    "    'microwave': 'heating',\n",
    "    'oven': 'Thanksgiving',\n",
    "    'toaster': 'breakfast',\n",
    "    'sink': 'drainage',\n",
    "    'refrigerator': 'cooling',\n",
    "    'book': 'knowledge',\n",
    "    'clock': 'time',\n",
    "    'vase': 'decoration',\n",
    "    'scissors': 'cutting',\n",
    "    'teddy bear': 'childhood',\n",
    "    'hair drier': 'drying',\n",
    "    'toothbrush': 'hygiene'\n",
    "}\n",
    "\n",
    "# create a unique set using the values from both dictionaries\n",
    "unique_equivalents = set(list(verb_equivalents.values()) + list(abstract_equivalents.values()))\n",
    "print(len(unique_equivalents))\n",
    "print(unique_equivalents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI ChatGPT 3.5 generated POS tags\n",
    "\n",
    "word_pos_dict = {\n",
    "    'companion': 'NN',\n",
    "    'toast': 'NN',\n",
    "    'lounge': 'NN',\n",
    "    'watch': 'VB',\n",
    "    'haul': 'VB',\n",
    "    'combination': 'NN',\n",
    "    'majesty': 'NN',\n",
    "    'extinguish': 'VB',\n",
    "    'nutrient': 'NN',\n",
    "    'bark': 'NN',\n",
    "    'rest': 'NN',\n",
    "    'hold': 'VB',\n",
    "    'drainage': 'NN',\n",
    "    'chill': 'VB',\n",
    "    'time': 'NN',\n",
    "    'drink': 'NN',\n",
    "    'eat': 'VB',\n",
    "    'journey': 'NN',\n",
    "    'surf': 'VB',\n",
    "    'shipment': 'NN',\n",
    "    'measure': 'NN',\n",
    "    'devour': 'VB',\n",
    "    'cutlery': 'NN',\n",
    "    'catch': 'NN',\n",
    "    'drive': 'VB',\n",
    "    'cooling': 'VBG',\n",
    "    'sail': 'VB',\n",
    "    'flutter': 'NN',\n",
    "    'moo': 'NN',\n",
    "    'descend': 'VB',\n",
    "    'commute': 'NN',\n",
    "    'score': 'NN',\n",
    "    'indulgence': 'NN',\n",
    "    'cutting': 'NN',\n",
    "    'containment': 'NN',\n",
    "    'communication': 'NN',\n",
    "    'transportation': 'NN',\n",
    "    'glide': 'VB',\n",
    "    'slice': 'NN',\n",
    "    'drying': 'NN',\n",
    "    'tote': 'NN',\n",
    "    'nurture': 'NN',\n",
    "    'breakfast': 'NN',\n",
    "    'cut': 'NN',\n",
    "    'throw': 'VB',\n",
    "    'brush': 'NN',\n",
    "    'competition': 'NN',\n",
    "    'signal': 'NN',\n",
    "    'carry': 'VB',\n",
    "    'mobility': 'NN',\n",
    "    'dine': 'VB',\n",
    "    'conformity': 'NN',\n",
    "    'Thanksgiving': 'NNP',\n",
    "    'comfort': 'NN',\n",
    "    'call': 'VB',\n",
    "    'soar': 'VB',\n",
    "    'pause': 'NN',\n",
    "    'input': 'NN',\n",
    "    'hygiene': 'NN',\n",
    "    'click': 'NN',\n",
    "    'health': 'NN',\n",
    "    'entertainment': 'NN',\n",
    "    'halt': 'NN',\n",
    "    'crunch': 'NN',\n",
    "    'softness': 'NN',\n",
    "    'control': 'NN',\n",
    "    'loyalty': 'NN',\n",
    "    'formality': 'NN',\n",
    "    'individuality': 'NN',\n",
    "    'gallop': 'NN',\n",
    "    'protection': 'NN',\n",
    "    'gathering': 'NN',\n",
    "    'pattern': 'NN',\n",
    "    'walk': 'VB',\n",
    "    'productivity': 'NN',\n",
    "    'purr': 'NN',\n",
    "    'sip': 'VB',\n",
    "    'decoration': 'NN',\n",
    "    'bite': 'NN',\n",
    "    'accessory': 'NN',\n",
    "    'reflection': 'NN',\n",
    "    'rinse': 'VB',\n",
    "    'uncork': 'VB',\n",
    "    'skate': 'VB',\n",
    "    'grace': 'NN',\n",
    "    'support': 'NN',\n",
    "    'scoop': 'NN',\n",
    "    'tick': 'NN',\n",
    "    'flight': 'NN',\n",
    "    'type': 'VB',\n",
    "    'read': 'VB',\n",
    "    'bake': 'VB',\n",
    "    'navigation': 'NN',\n",
    "    'poke': 'VB',\n",
    "    'descent': 'NN',\n",
    "    'voyage': 'NN',\n",
    "    'heating': 'VBG',\n",
    "    'thrill': 'NN',\n",
    "    'consume': 'VB',\n",
    "    'fly': 'VB',\n",
    "    'hug': 'NN',\n",
    "    'knowledge': 'NN',\n",
    "    'shield': 'NN',\n",
    "    'transit': 'NN',\n",
    "    'solitude': 'NN',\n",
    "    'heat': 'NN',\n",
    "    'fasten': 'VB',\n",
    "    'celebration': 'NN',\n",
    "    'hit': 'VB',\n",
    "    'flush': 'VB',\n",
    "    'adventure': 'NN',\n",
    "    'peel': 'VB',\n",
    "    'song': 'NN',\n",
    "    'elegance': 'NN',\n",
    "    'recreation': 'NN',\n",
    "    'roar': 'NN',\n",
    "    'trumpet': 'NN',\n",
    "    'container': 'NN',\n",
    "    'aroma': 'NN',\n",
    "    'childhood': 'NN',\n",
    "    'measurement': 'NN',\n",
    "    'sanitation': 'NN',\n",
    "    'vibration': 'NN',\n",
    "    'dry': 'VB',\n",
    "    'growth': 'NN',\n",
    "    'safety': 'NN',\n",
    "    'swing': 'NN',\n",
    "    'crave': 'VB',\n",
    "    'ride': 'VB',\n",
    "    'sharpness': 'NN',\n",
    "    'stride': 'NN',\n",
    "    'graze': 'NN',\n",
    "    'excitement': 'NN',\n",
    "    'sit': 'VB',\n",
    "    'indulge': 'VB',\n",
    "    'acidity': 'NN',\n",
    "    'prong': 'NN',\n",
    "    'chop': 'VB',\n",
    "    'munch': 'NN'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>companion</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toast</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lounge</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch</td>\n",
       "      <td>VB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haul</td>\n",
       "      <td>VB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word POS  count\n",
       "0  companion  NN      0\n",
       "1      toast  NN      0\n",
       "2     lounge  NN      0\n",
       "3      watch  VB      0\n",
       "4       haul  VB      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from the dictionary, with the key as 'Word' and the value as 'POS'\n",
    "# for now, set the 'count' column to 0\n",
    "word_pos_df = pd.DataFrame(list(word_pos_dict.items()), columns=['Word', 'POS'])\n",
    "word_pos_df['count'] = 0\n",
    "print(word_pos_df.shape)\n",
    "word_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>motorcycle</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS  count\n",
       "0      person  NN      0\n",
       "1     bicycle  NN      0\n",
       "2         car  NN      0\n",
       "3  motorcycle  NN      0\n",
       "4    airplane  NN      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe using coco classes, with POS tags set to NN and count set to 0\n",
    "coco_classes_df = pd.DataFrame(coco_classes, columns=['Word'])\n",
    "coco_classes_df['POS'] = 'NN'\n",
    "coco_classes_df['count'] = 0\n",
    "print(coco_classes_df.shape)\n",
    "coco_classes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 3)\n",
      "        Word POS  count\n",
      "0  companion  NN      0\n",
      "1      toast  NN      0\n",
      "2     lounge  NN      0\n",
      "3      watch  VB      0\n",
      "4       haul  VB      0\n",
      "(383, 3)\n",
      "        Word POS  count\n",
      "0  companion  NN      0\n",
      "1      toast  NN      0\n",
      "2     lounge  NN      0\n",
      "3      watch  VB      0\n",
      "4       haul  VB      0\n"
     ]
    }
   ],
   "source": [
    "# Combine the word_pos_df, coco_classes_df dataframes\n",
    "combined_df = pd.concat([word_pos_df, coco_classes_df])\n",
    "\n",
    "# Remove duplicates\n",
    "combined_df = combined_df.drop_duplicates(subset=['Word'])\n",
    "\n",
    "# Combine the combined_df and ptb_df_top5, prioritizing the ptb_df_top5 dataframe if there are duplicates\n",
    "combined_df = pd.concat([combined_df, ptb_df_top5])\n",
    "print(combined_df.shape)\n",
    "print(combined_df.head())\n",
    "combined_df = combined_df.drop_duplicates(subset=['Word'], keep='last')\n",
    "print(combined_df.shape)\n",
    "print(combined_df.head())\n",
    "\n",
    "# save the dataframe to csv\n",
    "combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a dataframe from the unique set, with 'Word', 'POS', and 'count' columns\n",
    "# # for now, set 'POS' to 'TBD' and 'count' to 0\n",
    "# equivalents_df = pd.DataFrame(unique_equivalents, columns=['Word'])\n",
    "# equivalents_df['POS'] = 'TBD'\n",
    "# equivalents_df['count'] = 0\n",
    "# print(equivalents_df.shape)\n",
    "# print(equivalents_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine equivalents_df and ptb_df_top5\n",
    "# combined_df = pd.concat([ptb_df_top5, equivalents_df])\n",
    "# print(combined_df.shape)\n",
    "# print(combined_df.head())\n",
    "\n",
    "# # save the dataframe to csv\n",
    "# combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a superset of set of unique words from both dictionaries and the top 5 words from each POS tag and the original coco classes\n",
    "# superset = set(list(unique_equivalents) + list(ptb_df_top5['Word']) + list(coco_classes))\n",
    "# print(len(superset))\n",
    "# print(superset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n",
      "['companion', 'toast', 'lounge', 'watch', 'haul']\n"
     ]
    }
   ],
   "source": [
    "# Convert combined_df['Word'] to list\n",
    "combined_list = combined_df['Word'].tolist()\n",
    "print(len(combined_list))\n",
    "print(combined_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding for each class\n",
    "# ❗️ note: I am only getting the embedding for the first token in each class\n",
    "# ❓ question: are we interested in the final contextual embedding for each class? currently, we're looking at the final hidden state.\n",
    "embeddings = []\n",
    "for i in range(len(combined_list)):\n",
    "# for i in range(1):\n",
    "    input_ids = torch.tensor(tokenizer.encode(combined_list[i])).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]\n",
    "    # skip the first and last token, which is the [CLS] and [SEP] tokens\n",
    "    # take the mean of other tokens (that form the word)    \n",
    "    embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round each val in embedding to 3 decimal places\n",
    "embeddings = [list(np.around(np.array(e),3)) for e in embeddings]\n",
    "\n",
    "# create string of all classes and their embeddings & save to text file\n",
    "# ❗️ note: only taking first 10 axes for now due to context window length\n",
    "# with open(\"output.txt\", \"w\") as text_file:\n",
    "#     for i in range(len(combined_list)):\n",
    "#         class_str = f\"{combined_list[i]}: {embeddings[i][:10]}\\n\"\n",
    "#         text_file.write(class_str)\n",
    "with open(\"output.txt\", \"w\") as text_file:\n",
    "    for i in range(len(combined_list)):\n",
    "        class_str = f\"{combined_list[i]}: {embeddings[i][0]}\\n\"\n",
    "        text_file.write(class_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383, 769)\n"
     ]
    }
   ],
   "source": [
    "# convert embedding list to dataframe\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(embeddings)\n",
    "df.insert(0, 'word', combined_list)\n",
    "print(df.shape)\n",
    "df.head()  # Display the first 5 rows to check the structure\n",
    "\n",
    "# save the dataframe to csv\n",
    "df.to_csv('training_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI ChatGPT 3.5 generated test set\n",
    "\n",
    "test_set = ['coffee mug', 'newspaper', 'shoes', 'headphones', 'umbrella stand', 'trash can', 'escalator', 'delivery van', 'gardening hose', 'street sign', 'mailbox', 'garage door', 'picnic table', 'seagull sculpture', 'houseplant', 'lap desk', 'home office chair', 'calendar', 'wallet', 'sunglasses', 'notebook', 'desktop computer', 'printer', 'office desk lamp', 'USB drive', 'water bottle', 'wine opener', 'mason jar', 'serving spoon', 'chopsticks', 'plate', 'napkin', 'apple slicer', 'cooking spatula', 'baking pan', 'cookie jar', 'tea kettle', 'candle', 'throw pillow', 'blanket', 'house slippers', 'bathroom scale', 'vanity mirror', 'alarm clock', 'picture frame', 'cactus plant', 'bookshelf', 'wall clock', 'wristwatch', 'reading glasses', 'hairbrush', 'hair tie', 'hand mirror', 'shaving razor', 'toilet paper', 'tissue box', 'paper towel holder', 'flashlight', 'laptop sleeve', 'computer mouse pad', 'USB cable', 'keyboard cover', 'wireless router', 'smartphone stand', 'kitchen apron', 'oven mitts', 'pot holder', 'cutting board', 'salt and pepper shakers', 'napkin holder', 'dish rack', 'wine rack', 'picture album', 'canvas tote bag', 'office phone', 'desk organizer', 'magnetic board']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = []\n",
    "for i in range(len(test_set)):\n",
    "# for i in range(1):\n",
    "    input_ids = torch.tensor(tokenizer.encode(test_set[i])).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]\n",
    "    # skip the first and last token, which is the [CLS] and [SEP] tokens\n",
    "    # take the mean of other tokens (that form the word)    \n",
    "    test_embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 769)\n"
     ]
    }
   ],
   "source": [
    "# round each val in embedding to 3 decimal places\n",
    "test_embeddings = [list(np.around(np.array(e),3)) for e in test_embeddings]\n",
    "\n",
    "# convert embedding list to dataframe\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(test_embeddings)\n",
    "test_df.insert(0, 'word', test_set)\n",
    "print(test_df.shape)\n",
    "test_df.head()  # Display the first 5 rows to check the structure\n",
    "\n",
    "# save the dataframe to csv\n",
    "test_df.to_csv('test_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
