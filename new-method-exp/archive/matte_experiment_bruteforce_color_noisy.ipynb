{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple torch model with 1 fully connected layer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "color_words = [\n",
    "    'aqua', 'aquamarine', 'azure', 'beige', 'bisque',\n",
    "    'chartreuse', 'chocolate', 'coral', 'crimson', 'cyan', 'firebrick', 'fuchsia',\n",
    "    'gold', 'gray', 'indigo', 'ivory', 'khaki', 'lavender', 'lime', 'magenta',\n",
    "    'maroon', 'navy', 'olive', 'orchid', 'plum', \n",
    "    'salmon', 'sienna', 'silver', 'tan', 'teal', 'tomato', 'turquoise', \n",
    "    'wheat', 'sienna', 'ochre', 'umber', 'sepia', 'vermillion',\n",
    "    'carmine', 'cerulean', 'auburn', 'viridian', 'ultramarine', 'emerald'\n",
    "]\n",
    "\n",
    "most_common_color_words = [\n",
    "    'red', 'green', 'blue', 'yellow', 'orange', 'purple', 'brown', 'pink', 'violet', 'white', 'black'\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "color_words = list(set(color_words))\n",
    "most_common_color_words = list(set(most_common_color_words))\n",
    "\n",
    "print(len(color_words))\n",
    "print(len(most_common_color_words))\n",
    "\n",
    "assert len(list(set(color_words + most_common_color_words))) == len(color_words) + len(most_common_color_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "non_color_words = [\n",
    "    \"...\", \"acidity\", \"snowboard\", \"shipment\", \"containment\", \":\", \"broccoli\", \"U.S.\", \"accessory\", \"zebra\", \"umbrella\", \"Securities\",\n",
    "    \"Corp.\", \"``\", \"productivity\", \"flutter\", \"frisbee\", \"tennis racket\", \"trading\", \"--\", \";\", \"drying\", \"softness\", \"suitcase\",\n",
    "    \"parking meter\", \"haul\", \"buy\", \"bench\", \"refrigerator\", \"Soviets\", \"microwave\", \"aroma\", \"wine glass\", \"formality\", \"solitude\", \"Wa\",\n",
    "    \"scissors\", \"thrill\", \"baseball glove\", \"sanitation\", \"gathering\", \"toaster\", \"bark\", \"skis\", \"oven\", \"flush\", \"peel\", \"Containers\",\n",
    "    \"shoe\", \"conformity\", \"whom\", \"elegance\", \"surfboard\", \"scoop\", \"cutlery\", \"C$\", \"fasten\", \"prices\", \"sandwich\", \"poke\", \"largest\",\n",
    "    \"glide\", \"container\", \"couch\", \"soar\", \"funds\", \"graze\", \"teddy bear\", \"who\", \"Who\", \"paper\", \"backpack\", \"slice\", \"hygiene\", \"brush\",\n",
    "    \"indulgence\", \"excitement\", \"loneliness\", \"lamp\", \"cow\", \"mobility\", \"stride\", \"%\", \"descent\", \"traffic light\", \"was\", \"lie\", \"drainage\",\n",
    "    \"toothbrush\", \"carrot\", \"banana\", \"etc.\", \".\", \"lawyer\", \"laptop\", \"remote\", \"rinse\", \"Perestroika\", \"consume\", \"toast\", \"hug\", \"market\",\n",
    "    \"`\", \"including\", \"fire hydrant\", \"carry\", \"crunch\", \"chill\", \"moo\"\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "non_color_words = list(set(non_color_words))\n",
    "\n",
    "print(len(non_color_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of words and returns a list of embeddings\n",
    "def get_embeddings(words):\n",
    "    embeddings = []\n",
    "\n",
    "    for word in words:\n",
    "        input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())\n",
    "    \n",
    "    assert len(embeddings) == len(words)\n",
    "    return embeddings\n",
    "\n",
    "color_embeddings = get_embeddings(color_words)\n",
    "most_common_color_embeddings = get_embeddings(most_common_color_words)\n",
    "non_color_embeddings = get_embeddings(non_color_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0 -0.164306  0.236645 -0.328378 -0.170503  0.590267  0.298811 -0.097471   \n",
      "1  0.125199  0.227860 -0.366152 -0.002999  0.656822  0.120573  0.118489   \n",
      "2 -0.041678  0.148708 -0.234286 -0.033252  0.494860  0.622115  0.000841   \n",
      "3  0.308206  0.380914 -0.145341  0.033035  0.216088 -0.055953  0.397435   \n",
      "4  0.242734 -0.032436 -0.144375 -0.357046  0.275547  0.518408  0.114429   \n",
      "\n",
      "          7         8         9  ...       760       761       762       763  \\\n",
      "0  0.248777  0.215423 -0.359507  ...  0.155917 -0.005785  0.234874 -0.098209   \n",
      "1  0.036798  0.533373 -0.229624  ...  0.025945 -0.349541  0.203456 -0.089264   \n",
      "2  0.334732 -0.142980 -0.213030  ... -0.058299 -0.062284  0.417793 -0.012300   \n",
      "3  0.486138 -0.087859 -0.435906  ...  0.249578 -0.232039  0.482491  0.088134   \n",
      "4 -0.048240 -0.216223 -0.843553  ...  0.393864  0.047029  0.359514 -0.004629   \n",
      "\n",
      "        764       765       766       767  label     word  \n",
      "0 -0.066509  0.101298  0.222775 -0.060571      1    olive  \n",
      "1  0.331100  0.120352  0.363870  0.160113      1   orchid  \n",
      "2  0.322776  0.159501  0.140781 -0.169250      1     plum  \n",
      "3 -0.102478  0.052224  0.352675 -0.605642      1    wheat  \n",
      "4  0.079422  0.244877  0.217458 -0.193338      1  crimson  \n",
      "\n",
      "[5 rows x 770 columns]\n",
      "            0         1         2         3         4         5         6  \\\n",
      "124  0.418729 -0.420065 -0.064409  0.041350  0.447161  0.015503 -0.073675   \n",
      "125  0.305462  0.350009  0.028162 -0.021747  0.214598 -0.305974 -0.186284   \n",
      "126  0.093611  0.024591  0.236378  0.100181 -0.006498 -0.246647  0.226492   \n",
      "127 -0.191762 -0.147731 -0.081693 -0.065446  0.390928 -0.163914  0.050367   \n",
      "128  0.096015  0.199141 -0.039224  0.088961  0.671071 -0.353585 -0.314911   \n",
      "\n",
      "            7         8         9  ...       760       761       762  \\\n",
      "124  0.378488  0.009935 -0.073724  ...  0.047305 -0.128640  0.396653   \n",
      "125  0.210726 -0.118364 -0.550844  ...  0.038662 -0.379655  0.399106   \n",
      "126  0.117283 -0.591190  0.115606  ...  0.192943 -0.661780  0.254261   \n",
      "127  0.139438  0.377951 -0.030549  ...  0.341921 -0.154406  0.151673   \n",
      "128  0.396788 -0.686378 -0.347215  ...  0.197999 -0.215692 -0.073675   \n",
      "\n",
      "          763       764       765       766       767  label        word  \n",
      "124 -0.151835  0.464782 -0.045093  0.366986 -0.255342      0     trading  \n",
      "125  0.033166 -0.071866  0.025876  0.248006 -0.018758      0       couch  \n",
      "126 -0.312778  0.107332 -0.325941 -0.052057 -0.512918      0        skis  \n",
      "127  0.033261  0.072102 -0.357412  0.293177  0.003269      0      remote  \n",
      "128 -0.056845  0.243154 -0.176729  0.359618 -0.325763      0  excitement  \n",
      "\n",
      "[5 rows x 770 columns]\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0 -0.164511  0.070430  0.263759  0.017094 -0.000787  0.125815 -0.078838   \n",
      "1  0.002040  0.109842 -0.238287 -0.003682  0.469811  0.030721  0.346308   \n",
      "2  0.069114  0.152995  0.032422  0.135762  0.293487  0.000916  0.002329   \n",
      "3 -0.346483  0.021747  0.027292 -0.023225  0.713148  0.281402 -0.079601   \n",
      "4  0.461140 -0.353692 -0.162086  0.008711  0.377371 -0.106069  0.229397   \n",
      "\n",
      "          7         8         9  ...       760       761       762       763  \\\n",
      "0  0.060285  0.008186 -0.373163  ... -0.143789 -0.129258  0.398210 -0.140141   \n",
      "1  0.237755 -0.071798 -0.390377  ... -0.159584 -0.174967  0.198345 -0.172087   \n",
      "2  0.441793 -0.060234 -0.192476  ... -0.000930 -0.192550  0.339311 -0.095719   \n",
      "3  0.842218 -0.005899 -0.401779  ... -0.077395  0.172417  0.035024 -0.176519   \n",
      "4  0.248776 -0.377236 -0.369541  ...  0.283831  0.091689  0.304215 -0.091861   \n",
      "\n",
      "        764       765       766       767  label        word  \n",
      "0  0.324922  0.366320  0.210054  0.446191      1       beige  \n",
      "1  0.420377  0.047724  0.081307 -0.252026      1     emerald  \n",
      "2  0.548146 -0.036493  0.078880  0.045693      1   turquoise  \n",
      "3 -0.277061 -0.228847  0.079051 -0.256386      1       ochre  \n",
      "4  0.037630  0.057002  0.344141  0.271983      1  aquamarine  \n",
      "\n",
      "[5 rows x 770 columns]\n",
      "           0         1         2         3         4         5         6  \\\n",
      "29 -0.549185  0.219009  0.204582 -0.121877  0.173513  0.026777 -0.001745   \n",
      "30  0.208791  0.411150  0.025523 -0.000128 -0.243398 -0.325066  0.147583   \n",
      "31  0.196428 -0.388666  0.236653 -0.097065  0.417043  0.099827  0.318719   \n",
      "32 -0.320792 -0.063335 -0.288818 -0.268551  0.136651 -0.219204  0.015108   \n",
      "33  0.212064 -0.440753 -0.060741  0.057349  0.022157  0.205033  0.056982   \n",
      "\n",
      "           7         8         9  ...       760       761       762       763  \\\n",
      "29  0.290504 -0.325990 -0.263292  ...  0.277175 -0.232233  0.156713 -0.437600   \n",
      "30  0.238492 -0.119853 -0.684093  ...  0.118672 -0.035871  0.444526 -0.028760   \n",
      "31  0.857411 -0.091236 -0.333502  ...  0.252765  0.053390  0.213124  0.169489   \n",
      "32 -0.005679 -0.127330 -0.133046  ...  0.291125 -0.549825 -0.198434 -0.511994   \n",
      "33  0.216355  0.262822 -0.667497  ...  0.156826 -0.041414  0.132769  0.269969   \n",
      "\n",
      "         764       765       766       767  label        word  \n",
      "29  0.379913  0.199465  0.427292 -0.529384      0     largest  \n",
      "30  0.144608  0.169477  0.364013 -0.097122      0       toast  \n",
      "31 -0.062854 -0.258017  0.162632  0.343505      0  indulgence  \n",
      "32  0.073547 -0.037839  0.190077  0.447823      0   formality  \n",
      "33  0.313772  0.343922 -0.050670  0.051631      0       slice  \n",
      "\n",
      "[5 rows x 770 columns]\n"
     ]
    }
   ],
   "source": [
    "# slice the lists into training and test sets\n",
    "color_words_train = color_words[:int(len(color_words)*0.8)]\n",
    "color_words_test = color_words[int(len(color_words)*0.8):]\n",
    "color_embeddings_train = color_embeddings[:int(len(color_embeddings)*0.8)]\n",
    "color_embeddings_test = color_embeddings[int(len(color_embeddings)*0.8):]\n",
    "\n",
    "most_common_color_words_train = most_common_color_words[:int(len(most_common_color_words)*0.8)]\n",
    "most_common_color_words_test = most_common_color_words[int(len(most_common_color_words)*0.8):]\n",
    "most_common_color_embeddings_train = most_common_color_embeddings[:int(len(most_common_color_embeddings)*0.8)]\n",
    "most_common_color_embeddings_test = most_common_color_embeddings[int(len(most_common_color_embeddings)*0.8):]\n",
    "\n",
    "non_color_words_train = non_color_words[:int(len(non_color_words)*0.8)]\n",
    "non_color_words_test = non_color_words[int(len(non_color_words)*0.8):]\n",
    "non_color_embeddings_train = non_color_embeddings[:int(len(non_color_embeddings)*0.8)]\n",
    "non_color_embeddings_test = non_color_embeddings[int(len(non_color_embeddings)*0.8):]\n",
    "\n",
    "# create a dataframe with the training sets\n",
    "train_embeddings = color_embeddings_train + most_common_color_embeddings_train + non_color_embeddings_train\n",
    "df_train = pd.DataFrame(train_embeddings)\n",
    "# convert the column names to strings\n",
    "df_train.columns = [str(i) for i in df_train.columns]\n",
    "\n",
    "df_train['label'] = [1]*len(color_words_train) + [1]*len(most_common_color_words_train) + [0]*len(non_color_words_train)\n",
    "df_train['word'] = color_words_train + most_common_color_words_train + non_color_words_train\n",
    "\n",
    "# create a dataframe with the test sets\n",
    "test_embeddings = color_embeddings_test + most_common_color_embeddings_test + non_color_embeddings_test\n",
    "df_test = pd.DataFrame(test_embeddings)\n",
    "# convert the column names to strings\n",
    "df_test.columns = [str(i) for i in df_test.columns]\n",
    "\n",
    "df_test['label'] = [1]*len(color_words_test) + [1]*len(most_common_color_words_test) + [0]*len(non_color_words_test)\n",
    "df_test['word'] = color_words_test + most_common_color_words_test + non_color_words_test\n",
    "\n",
    "# df_train = pd.DataFrame({\n",
    "#     'word': color_words_train + most_common_color_words_train + non_color_words_train,\n",
    "#     'embedding': color_embeddings_train + most_common_color_embeddings_train + non_color_embeddings_train,\n",
    "#     'label': [1]*len(color_words_train) + [1]*len(most_common_color_words_train) + [0]*len(non_color_words_train)\n",
    "# })\n",
    "\n",
    "# df_test = pd.DataFrame({\n",
    "#     'word': color_words_test + most_common_color_words_test + non_color_words_test,\n",
    "#     'embedding': color_embeddings_test + most_common_color_embeddings_test + non_color_embeddings_test,\n",
    "#     'label': [1]*len(color_words_test) + [1]*len(most_common_color_words_test) + [0]*len(non_color_words_test)\n",
    "# })\n",
    "\n",
    "# shuffle the dataframes\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df_train[:5])\n",
    "print(df_train[-5:])\n",
    "print(df_test[:5])\n",
    "print(df_test[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = torch.tensor(tokenized_texts)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('mps')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df_train, df_test):\n",
    "  train_dataset = CustomDataset(df_train['embedding'], df_train['label'])\n",
    "  test_dataset = CustomDataset(df_test['embedding'], df_test['label'])\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader):\n",
    "\n",
    "    # train the model\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "    num_epochs = 100\n",
    "    net.to(device)\n",
    "\n",
    "    best_combined_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        combined_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # get data to GPU if possible\n",
    "            data = batch[0].to(device=device, dtype=torch.float32)\n",
    "            data = data.unsqueeze(1)\n",
    "            targets = batch[1].to(device=device)\n",
    "            # print(data.shape)\n",
    "            \n",
    "            # forward\n",
    "            scores = net(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            combined_loss += loss.item()\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "\n",
    "        if combined_loss < best_combined_loss:\n",
    "            best_combined_loss = combined_loss\n",
    "        \n",
    "        # print initial loss\n",
    "        # if epoch == 0:\n",
    "        #     print(f'Initial loss: {combined_loss/len(train_loader)}')\n",
    "            \n",
    "        # print average loss per epoch every 10 epochs\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader)}')\n",
    "        #     # print(combined_loss / len(train_loader))\n",
    "\n",
    "    return net, best_combined_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the net\n",
    "def check_accuracy(loader, net):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # get data to GPU if possible\n",
    "            data = batch[0].to(device=device, dtype=torch.float32)\n",
    "            data = data.unsqueeze(1)\n",
    "            targets = batch[1].to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = net(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    \n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice: 588\n",
      "Slice 588 best combined loss: 5.256365716457367\n",
      "Slice 588 best test accuracy: 64.71% \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# go through all the slices of the dataframe and train and evaluate the model\n",
    "\n",
    "all_test_accs = []\n",
    "all_combined_losses = []\n",
    "\n",
    "# for slice in range(768):\n",
    "slice = 588\n",
    "\n",
    "print(f'Slice: {slice}')\n",
    "# slice = 0\n",
    "\n",
    "# create one column slice of the dataframe\n",
    "df_train_slice = pd.DataFrame({\n",
    "    # 'word': df_train['word'],\n",
    "    'embedding': df_train[str(slice)],\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "df_test_slice = pd.DataFrame({\n",
    "    # 'word': df_test['word'],\n",
    "    'embedding': df_test[str(slice)],\n",
    "    'label': df_test['label']\n",
    "})\n",
    "\n",
    "# print(df_train_slice[:5])\n",
    "# print(df_train_slice[-5:])\n",
    "\n",
    "train_slice_loader, test_slice_loader = create_dataloaders(df_train_slice, df_test_slice)\n",
    "\n",
    "# take the best of 3 runs\n",
    "best_test_acc = 0\n",
    "\n",
    "for i in range(10):\n",
    "    net, best_combined_loss = train_model(train_slice_loader)\n",
    "\n",
    "    train_acc = check_accuracy(train_slice_loader, net)\n",
    "    test_acc = check_accuracy(test_slice_loader, net)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        # best_train_acc = train_acc\n",
    "        # best_net = net\n",
    "\n",
    "all_test_accs.append(best_test_acc)\n",
    "all_combined_losses.append(best_combined_loss)\n",
    "\n",
    "# print(f'Accuracy on training set: {check_accuracy(train_loader, net)*100:.2f}%')\n",
    "# print(f'Accuracy on test set: {check_accuracy(test_loader, net)*100:.2f}%')\n",
    "\n",
    "print(f'Slice {slice} best combined loss: {best_combined_loss}')\n",
    "print(f'Slice {slice} best test accuracy: {best_test_acc*100:.2f}% \\n')\n",
    "\n",
    "print(len(all_test_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.6471)]\n"
     ]
    }
   ],
   "source": [
    "print(all_test_accs[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6470588445663452]\n"
     ]
    }
   ],
   "source": [
    "# convert list of tensors into list of floats\n",
    "all_test_accs_val = [acc.item() for acc in all_test_accs]\n",
    "\n",
    "print(all_test_accs_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_accs_val = np.array(all_test_accs_val)\n",
    "all_combined_losses = np.array(all_combined_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0.64705884]\n",
      "[5.25636572]\n"
     ]
    }
   ],
   "source": [
    "# find the indices of the top 10 slices\n",
    "top_10_indices = np.argsort(all_test_accs_val)[-10:]\n",
    "print(top_10_indices)\n",
    "print(all_test_accs_val[top_10_indices])\n",
    "print(all_combined_losses[top_10_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy      loss\n",
      "0  0.647059  5.256366\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe with accuracy and loss\n",
    "df_acc_loss = pd.DataFrame({\n",
    "    'accuracy': all_test_accs_val,\n",
    "    'loss': all_combined_losses\n",
    "})\n",
    "\n",
    "print(df_acc_loss[:10])\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "# df_acc_loss.to_csv('llm-outputs/color_xl_bruteforce_acc_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index     color  non_color      diff       acc      loss\n",
      "0      0  0.042866   0.154596  0.111729  0.647059  5.256366\n"
     ]
    }
   ],
   "source": [
    "# for each of the top 10 slices,\n",
    "# average the embeddings of the color words and the non-color words\n",
    "\n",
    "color_emb_avg = []\n",
    "non_color_emb_avg = []\n",
    "\n",
    "for i in top_10_indices:\n",
    "    df_slice = pd.DataFrame({\n",
    "        'word': df_train['word'],\n",
    "        'embedding': df_train[str(i)],\n",
    "        'label': df_train['label']\n",
    "    })\n",
    "\n",
    "    df_slice_color = df_slice[df_slice['label'] == 1]\n",
    "    df_slice_non_color = df_slice[df_slice['label'] == 0]\n",
    "\n",
    "    color_emb_avg.append(np.mean(df_slice_color['embedding']))\n",
    "    non_color_emb_avg.append(np.mean(df_slice_non_color['embedding']))\n",
    "\n",
    "# create a dataframe with the average embeddings\n",
    "df_avg = pd.DataFrame({\n",
    "    'index': top_10_indices,\n",
    "    'color': color_emb_avg,\n",
    "    'non_color': non_color_emb_avg,\n",
    "    'diff': abs(np.array(color_emb_avg) - np.array(non_color_emb_avg)),\n",
    "    'acc': all_test_accs_val[top_10_indices],\n",
    "    'loss': all_combined_losses[top_10_indices]\n",
    "})\n",
    "\n",
    "print(df_avg)\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "# df_avg.to_csv('llm-outputs/color_xl_bruteforce_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add words to df_train_slice\n",
    "df_train_slice['word'] = df_train['word']\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df_train_slice.to_csv('llm-outputs/color_xl_bruteforce_slice_588.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_axis_hist(slice):\n",
    "    df_slice = pd.DataFrame({\n",
    "        # 'word': df_train['word'],\n",
    "        'embedding': df_train[str(slice)],\n",
    "        'label': df_train['label']\n",
    "    })\n",
    "\n",
    "    df_slice_color = df_slice[df_slice['label'] == 1]\n",
    "    df_slice_non_color = df_slice[df_slice['label'] == 0]\n",
    "\n",
    "    # draw a histogram of the embeddings, with the color words in red and the non-color words in blue\n",
    "    plt.hist(df_slice_color['embedding'], bins=30, color='red', alpha=0.5)\n",
    "    plt.hist(df_slice_non_color['embedding'], bins=30, color='blue', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_axis_hist(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6861",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
