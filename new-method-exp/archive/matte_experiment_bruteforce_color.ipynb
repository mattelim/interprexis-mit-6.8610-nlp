{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple torch model with 1 fully connected layer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "color_words = [\n",
    "    'aqua', 'aquamarine', 'azure', 'beige', 'bisque',\n",
    "    'chartreuse', 'chocolate', 'coral', 'crimson', 'cyan', 'firebrick', 'fuchsia',\n",
    "    'gold', 'gray', 'indigo', 'ivory', 'khaki', 'lavender', 'lime', 'magenta',\n",
    "    'maroon', 'navy', 'olive', 'orchid', 'plum', \n",
    "    'salmon', 'sienna', 'silver', 'tan', 'teal', 'tomato', 'turquoise', \n",
    "    'wheat', 'sienna', 'ochre', 'umber', 'sepia', 'vermillion',\n",
    "    'carmine', 'cerulean', 'auburn', 'viridian', 'ultramarine', 'emerald'\n",
    "]\n",
    "\n",
    "most_common_color_words = [\n",
    "    'red', 'green', 'blue', 'yellow', 'orange', 'purple', 'brown', 'pink', 'violet', 'white', 'black'\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "color_words = list(set(color_words))\n",
    "most_common_color_words = list(set(most_common_color_words))\n",
    "\n",
    "print(len(color_words))\n",
    "print(len(most_common_color_words))\n",
    "\n",
    "assert len(list(set(color_words + most_common_color_words))) == len(color_words) + len(most_common_color_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "non_color_words = [\n",
    "    \"dog\", \"cat\", \"house\", \"car\", \"book\", \"computer\", \"table\", \"chair\", \"tree\", \"river\",\n",
    "    \"song\", \"movie\", \"friend\", \"family\", \"phone\", \"city\", \"food\", \"drink\", \"shoe\", \"hat\",\n",
    "    \"jacket\", \"pen\", \"paper\", \"cloud\", \"sun\", \"moon\", \"star\", \"road\", \"bridge\", \"key\",\n",
    "    \"lock\", \"door\", \"window\", \"mirror\", \"clock\", \"lamp\", \"flower\", \"bird\", \"fish\", \"ship\",\n",
    "    \"plane\", \"train\", \"bus\", \"child\", \"adult\", \"student\", \"teacher\", \"doctor\", \"engineer\",\n",
    "    \"artist\", \"writer\", \"singer\", \"actor\", \"politician\", \"lawyer\", \"doctor\", \"patient\",\n",
    "    \"dream\", \"memory\", \"idea\", \"emotion\", \"love\", \"hate\", \"fear\", \"joy\", \"anger\", \"hope\",\n",
    "    \"doubt\", \"peace\", \"war\", \"freedom\", \"justice\", \"truth\", \"lie\", \"friendship\", \"loneliness\",\n",
    "    \"success\", \"failure\", \"wealth\", \"poverty\", \"nature\", \"technology\", \"culture\", \"history\",\n",
    "    \"science\", \"religion\", \"politics\", \"economy\", \"society\", \"language\", \"knowledge\", \"wisdom\"\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "non_color_words = list(set(non_color_words))\n",
    "\n",
    "print(len(non_color_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of words and returns a list of embeddings\n",
    "def get_embeddings(words):\n",
    "    embeddings = []\n",
    "\n",
    "    for word in words:\n",
    "        input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())\n",
    "    \n",
    "    assert len(embeddings) == len(words)\n",
    "    return embeddings\n",
    "\n",
    "color_embeddings = get_embeddings(color_words)\n",
    "most_common_color_embeddings = get_embeddings(most_common_color_words)\n",
    "non_color_embeddings = get_embeddings(non_color_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0 -0.041678  0.148708 -0.234286 -0.033252  0.494860  0.622115  0.000841   \n",
      "1 -0.827585 -0.896846  0.254223 -0.193303  0.349100  0.395511  0.112490   \n",
      "2  0.002040  0.109842 -0.238287 -0.003682  0.469811  0.030721  0.346308   \n",
      "3  0.385726 -0.074940 -0.140404  0.059326  0.366880  0.120377  0.186069   \n",
      "4 -0.138055 -0.053237  0.060839  0.007547  0.434347  0.081092  0.252117   \n",
      "\n",
      "          7         8         9  ...       760       761       762       763  \\\n",
      "0  0.334732 -0.142980 -0.213030  ... -0.058299 -0.062284  0.417793 -0.012300   \n",
      "1  0.549485 -0.285964 -0.402029  ...  0.291153  0.110757 -0.147386  0.099284   \n",
      "2  0.237755 -0.071798 -0.390377  ... -0.159584 -0.174967  0.198345 -0.172087   \n",
      "3  0.305407  0.346098 -0.088989  ... -0.154134 -0.205948  0.394289 -0.059338   \n",
      "4  0.487236 -0.014518 -0.217729  ...  0.213379 -0.025966  0.354676 -0.184217   \n",
      "\n",
      "        764       765       766       767  label      word  \n",
      "0  0.322776  0.159501  0.140781 -0.169250      1      plum  \n",
      "1 -0.199796 -0.004336 -0.064936  0.218755      1     umber  \n",
      "2  0.420377  0.047724  0.081307 -0.252026      1   emerald  \n",
      "3  0.401560  0.028277  0.039687  0.117044      1  lavender  \n",
      "4 -0.685783  0.151444  0.333758  0.124797      1     sepia  \n",
      "\n",
      "[5 rows x 770 columns]\n",
      "            0         1         2         3         4         5         6  \\\n",
      "109 -0.146738  0.129713  0.083132 -0.539107  0.563239  0.128597  0.271279   \n",
      "110  0.047206 -0.225810 -0.218537 -0.193638  0.507414  0.141688  0.241249   \n",
      "111  0.290766 -0.108031 -0.185265 -0.273597  0.299481  0.216749  0.347278   \n",
      "112  0.141818 -0.057983 -0.442358 -0.182030  0.488916  0.086198 -0.034673   \n",
      "113 -0.338178 -0.089369 -0.176818 -0.346291  0.705371 -0.460152  0.258290   \n",
      "\n",
      "            7         8         9  ...       760       761       762  \\\n",
      "109  0.087957 -0.112653 -0.791648  ... -0.007532  0.060341  0.259289   \n",
      "110  0.027033  0.102065 -0.622487  ...  0.372422 -0.064496  0.175508   \n",
      "111 -0.152930 -0.604586 -0.500982  ...  0.128703 -0.117606  0.405429   \n",
      "112  0.410475  0.220826 -0.481746  ...  0.697546 -0.088217  0.202779   \n",
      "113 -0.146433 -0.114553 -0.254384  ...  0.359253 -0.097834  0.330574   \n",
      "\n",
      "          763       764       765       766       767  label     word  \n",
      "109  0.165460  0.046187  0.235187  0.476254 -0.316520      0      sun  \n",
      "110  0.011360 -0.036907  0.161966  0.704968 -0.461690      0     star  \n",
      "111  0.011293  0.039863  0.137743  0.617798 -0.672510      0  justice  \n",
      "112 -0.189614  0.162399 -0.250696  0.522243  0.574230      0   family  \n",
      "113 -0.054477 -0.029562 -0.153859  0.243657  0.199061      0      key  \n",
      "\n",
      "[5 rows x 770 columns]\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.321866 -0.304386 -0.026468 -0.062151  0.629500 -0.096223 -0.102057   \n",
      "1  0.349948  0.296686 -0.737630  0.055064  0.074024  0.083423  0.265809   \n",
      "2  0.319975 -0.505926  0.052281  0.013909  0.177459  0.183554 -0.004022   \n",
      "3 -0.202208 -0.661616  0.271655  0.076829  0.430605  0.135173 -0.192456   \n",
      "4  0.461140 -0.353692 -0.162086  0.008711  0.377371 -0.106069  0.229397   \n",
      "\n",
      "          7         8         9  ...       760       761       762       763  \\\n",
      "0  0.337909  0.332677 -0.261460  ...  0.088625 -0.067584  0.220221 -0.272488   \n",
      "1  0.561713 -0.251711 -0.539855  ... -0.215103  0.006335  0.281783  0.097750   \n",
      "2  0.323612  0.299608 -0.423579  ...  0.207875 -0.185798  0.246340 -0.044465   \n",
      "3  0.651584 -0.634212 -0.064537  ...  0.272369 -0.285443 -0.065204  0.113049   \n",
      "4  0.248776 -0.377236 -0.369541  ...  0.283831  0.091689  0.304215 -0.091861   \n",
      "\n",
      "        764       765       766       767  label        word  \n",
      "0 -0.241842 -0.006823  0.305522 -0.087767      1     magenta  \n",
      "1  0.505218  0.139610  0.154430 -0.298752      1      salmon  \n",
      "2  0.214319  0.096531  0.018976  0.214072      1      sienna  \n",
      "3 -0.024030  0.116707  0.302369  0.122049      1  vermillion  \n",
      "4  0.037630  0.057002  0.344141  0.271983      1  aquamarine  \n",
      "\n",
      "[5 rows x 770 columns]\n",
      "           0         1         2         3         4         5         6  \\\n",
      "26  0.342536  0.066485 -0.220430 -0.353014  0.421823  0.354513  0.396531   \n",
      "27  0.339843  0.227372 -0.369423 -0.412076  0.888152  0.279329 -0.045159   \n",
      "28 -0.102579 -0.178336 -0.094495 -0.385456  0.465792  0.179829  0.594975   \n",
      "29  0.215590  0.221426 -0.663242 -0.264389  0.549313  0.659787  0.378421   \n",
      "30  0.775862  0.253050 -0.103731 -0.364700  0.731718  0.110191  0.282084   \n",
      "\n",
      "           7         8         9  ...       760       761       762       763  \\\n",
      "26  0.193945  0.341388 -0.359186  ... -0.025374  0.196869  0.186752 -0.325863   \n",
      "27  0.310702  0.020811 -0.770537  ...  0.161082 -0.049799  0.000546  0.152074   \n",
      "28  0.509926 -0.177097 -0.361155  ... -0.206767 -0.055807  0.320932 -0.172367   \n",
      "29  0.438917  0.288774 -0.800134  ...  0.331498 -0.065795  0.027580 -0.033944   \n",
      "30 -0.454165 -0.385224 -0.796547  ...  0.434471 -0.007268  0.196388 -0.178879   \n",
      "\n",
      "         764       765       766       767  label    word  \n",
      "26 -0.176221 -0.130114  0.589207  0.032245      0     dog  \n",
      "27  0.060824  0.343852  0.379356 -0.345383      0  window  \n",
      "28 -0.226440  0.064617  0.616699  0.570033      0     cat  \n",
      "29 -0.064721 -0.112070  0.425459 -0.415371      0   river  \n",
      "30 -0.047258 -0.092031  0.563597  0.384265      0   house  \n",
      "\n",
      "[5 rows x 770 columns]\n"
     ]
    }
   ],
   "source": [
    "# slice the lists into training and test sets\n",
    "color_words_train = color_words[:int(len(color_words)*0.8)]\n",
    "color_words_test = color_words[int(len(color_words)*0.8):]\n",
    "color_embeddings_train = color_embeddings[:int(len(color_embeddings)*0.8)]\n",
    "color_embeddings_test = color_embeddings[int(len(color_embeddings)*0.8):]\n",
    "\n",
    "most_common_color_words_train = most_common_color_words[:int(len(most_common_color_words)*0.8)]\n",
    "most_common_color_words_test = most_common_color_words[int(len(most_common_color_words)*0.8):]\n",
    "most_common_color_embeddings_train = most_common_color_embeddings[:int(len(most_common_color_embeddings)*0.8)]\n",
    "most_common_color_embeddings_test = most_common_color_embeddings[int(len(most_common_color_embeddings)*0.8):]\n",
    "\n",
    "non_color_words_train = non_color_words[:int(len(non_color_words)*0.8)]\n",
    "non_color_words_test = non_color_words[int(len(non_color_words)*0.8):]\n",
    "non_color_embeddings_train = non_color_embeddings[:int(len(non_color_embeddings)*0.8)]\n",
    "non_color_embeddings_test = non_color_embeddings[int(len(non_color_embeddings)*0.8):]\n",
    "\n",
    "# create a dataframe with the training sets\n",
    "train_embeddings = color_embeddings_train + most_common_color_embeddings_train + non_color_embeddings_train\n",
    "df_train = pd.DataFrame(train_embeddings)\n",
    "# convert the column names to strings\n",
    "df_train.columns = [str(i) for i in df_train.columns]\n",
    "\n",
    "df_train['label'] = [1]*len(color_words_train) + [1]*len(most_common_color_words_train) + [0]*len(non_color_words_train)\n",
    "df_train['word'] = color_words_train + most_common_color_words_train + non_color_words_train\n",
    "\n",
    "# create a dataframe with the test sets\n",
    "test_embeddings = color_embeddings_test + most_common_color_embeddings_test + non_color_embeddings_test\n",
    "df_test = pd.DataFrame(test_embeddings)\n",
    "# convert the column names to strings\n",
    "df_test.columns = [str(i) for i in df_test.columns]\n",
    "\n",
    "df_test['label'] = [1]*len(color_words_test) + [1]*len(most_common_color_words_test) + [0]*len(non_color_words_test)\n",
    "df_test['word'] = color_words_test + most_common_color_words_test + non_color_words_test\n",
    "\n",
    "# df_train = pd.DataFrame({\n",
    "#     'word': color_words_train + most_common_color_words_train + non_color_words_train,\n",
    "#     'embedding': color_embeddings_train + most_common_color_embeddings_train + non_color_embeddings_train,\n",
    "#     'label': [1]*len(color_words_train) + [1]*len(most_common_color_words_train) + [0]*len(non_color_words_train)\n",
    "# })\n",
    "\n",
    "# df_test = pd.DataFrame({\n",
    "#     'word': color_words_test + most_common_color_words_test + non_color_words_test,\n",
    "#     'embedding': color_embeddings_test + most_common_color_embeddings_test + non_color_embeddings_test,\n",
    "#     'label': [1]*len(color_words_test) + [1]*len(most_common_color_words_test) + [0]*len(non_color_words_test)\n",
    "# })\n",
    "\n",
    "# shuffle the dataframes\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df_train[:5])\n",
    "print(df_train[-5:])\n",
    "print(df_test[:5])\n",
    "print(df_test[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = torch.tensor(tokenized_texts)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('mps')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df_train, df_test):\n",
    "  train_dataset = CustomDataset(df_train['embedding'], df_train['label'])\n",
    "  test_dataset = CustomDataset(df_test['embedding'], df_test['label'])\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader):\n",
    "\n",
    "    # train the model\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "    num_epochs = 100\n",
    "    net.to(device)\n",
    "\n",
    "    best_combined_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        combined_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # get data to GPU if possible\n",
    "            data = batch[0].to(device=device, dtype=torch.float32)\n",
    "            data = data.unsqueeze(1)\n",
    "            targets = batch[1].to(device=device)\n",
    "            # print(data.shape)\n",
    "            \n",
    "            # forward\n",
    "            scores = net(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            combined_loss += loss.item()\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "\n",
    "        if combined_loss < best_combined_loss:\n",
    "            best_combined_loss = combined_loss\n",
    "        \n",
    "        # print initial loss\n",
    "        # if epoch == 0:\n",
    "        #     print(f'Initial loss: {combined_loss/len(train_loader)}')\n",
    "            \n",
    "        # print average loss per epoch every 10 epochs\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader)}')\n",
    "        #     # print(combined_loss / len(train_loader))\n",
    "\n",
    "    return net, best_combined_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the net\n",
    "def check_accuracy(loader, net):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # get data to GPU if possible\n",
    "            data = batch[0].to(device=device, dtype=torch.float32)\n",
    "            data = data.unsqueeze(1)\n",
    "            targets = batch[1].to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = net(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    \n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice: 0\n",
      "Slice 0 best combined loss: 4.771129250526428\n",
      "Slice 0 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 1\n",
      "Slice 1 best combined loss: 4.871202111244202\n",
      "Slice 1 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 2\n",
      "Slice 2 best combined loss: 4.939927458763123\n",
      "Slice 2 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 3\n",
      "Slice 3 best combined loss: 4.792909502983093\n",
      "Slice 3 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 4\n",
      "Slice 4 best combined loss: 4.9441253542900085\n",
      "Slice 4 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 5\n",
      "Slice 5 best combined loss: 5.047808349132538\n",
      "Slice 5 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 6\n",
      "Slice 6 best combined loss: 4.953889191150665\n",
      "Slice 6 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 7\n",
      "Slice 7 best combined loss: 4.838975667953491\n",
      "Slice 7 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 8\n",
      "Slice 8 best combined loss: 4.7594287395477295\n",
      "Slice 8 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 9\n",
      "Slice 9 best combined loss: 4.956266850233078\n",
      "Slice 9 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 10\n",
      "Slice 10 best combined loss: 4.817738354206085\n",
      "Slice 10 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 11\n",
      "Slice 11 best combined loss: 5.097119987010956\n",
      "Slice 11 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 12\n",
      "Slice 12 best combined loss: 4.750204503536224\n",
      "Slice 12 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 13\n",
      "Slice 13 best combined loss: 5.145833194255829\n",
      "Slice 13 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 14\n",
      "Slice 14 best combined loss: 4.545928508043289\n",
      "Slice 14 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 15\n",
      "Slice 15 best combined loss: 4.039368361234665\n",
      "Slice 15 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 16\n",
      "Slice 16 best combined loss: 5.055962175130844\n",
      "Slice 16 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 17\n",
      "Slice 17 best combined loss: 5.048658937215805\n",
      "Slice 17 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 18\n",
      "Slice 18 best combined loss: 4.675347089767456\n",
      "Slice 18 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 19\n",
      "Slice 19 best combined loss: 4.527964413166046\n",
      "Slice 19 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 20\n",
      "Slice 20 best combined loss: 4.9731162786483765\n",
      "Slice 20 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 21\n",
      "Slice 21 best combined loss: 5.048308789730072\n",
      "Slice 21 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 22\n",
      "Slice 22 best combined loss: 4.598686903715134\n",
      "Slice 22 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 23\n",
      "Slice 23 best combined loss: 5.083897083997726\n",
      "Slice 23 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 24\n",
      "Slice 24 best combined loss: 4.2985614240169525\n",
      "Slice 24 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 25\n",
      "Slice 25 best combined loss: 4.95643025636673\n",
      "Slice 25 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 26\n",
      "Slice 26 best combined loss: 5.09811532497406\n",
      "Slice 26 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 27\n",
      "Slice 27 best combined loss: 5.023380726575851\n",
      "Slice 27 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 28\n",
      "Slice 28 best combined loss: 5.066824346780777\n",
      "Slice 28 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 29\n",
      "Slice 29 best combined loss: 4.312641948461533\n",
      "Slice 29 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 30\n",
      "Slice 30 best combined loss: 4.827310651540756\n",
      "Slice 30 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 31\n",
      "Slice 31 best combined loss: 4.8754023015499115\n",
      "Slice 31 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 32\n",
      "Slice 32 best combined loss: 4.965341866016388\n",
      "Slice 32 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 33\n",
      "Slice 33 best combined loss: 4.983487546443939\n",
      "Slice 33 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 34\n",
      "Slice 34 best combined loss: 4.831881374120712\n",
      "Slice 34 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 35\n",
      "Slice 35 best combined loss: 4.964982390403748\n",
      "Slice 35 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 36\n",
      "Slice 36 best combined loss: 5.085390150547028\n",
      "Slice 36 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 37\n",
      "Slice 37 best combined loss: 4.706101149320602\n",
      "Slice 37 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 38\n",
      "Slice 38 best combined loss: 5.05762904882431\n",
      "Slice 38 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 39\n",
      "Slice 39 best combined loss: 4.713425099849701\n",
      "Slice 39 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 40\n",
      "Slice 40 best combined loss: 3.869065523147583\n",
      "Slice 40 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 41\n",
      "Slice 41 best combined loss: 4.751593708992004\n",
      "Slice 41 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 42\n",
      "Slice 42 best combined loss: 4.734677851200104\n",
      "Slice 42 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 43\n",
      "Slice 43 best combined loss: 4.657898873090744\n",
      "Slice 43 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 44\n",
      "Slice 44 best combined loss: 5.0515585243701935\n",
      "Slice 44 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 45\n",
      "Slice 45 best combined loss: 5.081245064735413\n",
      "Slice 45 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 46\n",
      "Slice 46 best combined loss: 5.08439040184021\n",
      "Slice 46 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 47\n",
      "Slice 47 best combined loss: 4.205697417259216\n",
      "Slice 47 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 48\n",
      "Slice 48 best combined loss: 5.082906782627106\n",
      "Slice 48 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 49\n",
      "Slice 49 best combined loss: 5.041304230690002\n",
      "Slice 49 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 50\n",
      "Slice 50 best combined loss: 4.745031833648682\n",
      "Slice 50 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 51\n",
      "Slice 51 best combined loss: 5.0645716190338135\n",
      "Slice 51 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 52\n",
      "Slice 52 best combined loss: 5.056299388408661\n",
      "Slice 52 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 53\n",
      "Slice 53 best combined loss: 4.476842224597931\n",
      "Slice 53 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 54\n",
      "Slice 54 best combined loss: 5.102455049753189\n",
      "Slice 54 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 55\n",
      "Slice 55 best combined loss: 4.460763156414032\n",
      "Slice 55 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 56\n",
      "Slice 56 best combined loss: 5.086177587509155\n",
      "Slice 56 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 57\n",
      "Slice 57 best combined loss: 4.847521901130676\n",
      "Slice 57 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 58\n",
      "Slice 58 best combined loss: 4.956827849149704\n",
      "Slice 58 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 59\n",
      "Slice 59 best combined loss: 5.09206348657608\n",
      "Slice 59 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 60\n",
      "Slice 60 best combined loss: 4.762498587369919\n",
      "Slice 60 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 61\n",
      "Slice 61 best combined loss: 3.8597241938114166\n",
      "Slice 61 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 62\n",
      "Slice 62 best combined loss: 4.061592683196068\n",
      "Slice 62 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 63\n",
      "Slice 63 best combined loss: 4.838772773742676\n",
      "Slice 63 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 64\n",
      "Slice 64 best combined loss: 4.672811508178711\n",
      "Slice 64 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 65\n",
      "Slice 65 best combined loss: 5.069316953420639\n",
      "Slice 65 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 66\n",
      "Slice 66 best combined loss: 5.0337227284908295\n",
      "Slice 66 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 67\n",
      "Slice 67 best combined loss: 4.675228297710419\n",
      "Slice 67 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 68\n",
      "Slice 68 best combined loss: 5.14984256029129\n",
      "Slice 68 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 69\n",
      "Slice 69 best combined loss: 4.759843677282333\n",
      "Slice 69 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 70\n",
      "Slice 70 best combined loss: 5.075463116168976\n",
      "Slice 70 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 71\n",
      "Slice 71 best combined loss: 5.024263560771942\n",
      "Slice 71 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 72\n",
      "Slice 72 best combined loss: 4.826935946941376\n",
      "Slice 72 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 73\n",
      "Slice 73 best combined loss: 5.040479958057404\n",
      "Slice 73 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 74\n",
      "Slice 74 best combined loss: 5.0635828375816345\n",
      "Slice 74 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 75\n",
      "Slice 75 best combined loss: 4.800880193710327\n",
      "Slice 75 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 76\n",
      "Slice 76 best combined loss: 4.186343044042587\n",
      "Slice 76 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 77\n",
      "Slice 77 best combined loss: 4.907773315906525\n",
      "Slice 77 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 78\n",
      "Slice 78 best combined loss: 4.980985939502716\n",
      "Slice 78 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 79\n",
      "Slice 79 best combined loss: 5.08130544424057\n",
      "Slice 79 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 80\n",
      "Slice 80 best combined loss: 4.6356250792741776\n",
      "Slice 80 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 81\n",
      "Slice 81 best combined loss: 5.070552706718445\n",
      "Slice 81 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 82\n",
      "Slice 82 best combined loss: 4.716074109077454\n",
      "Slice 82 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 83\n",
      "Slice 83 best combined loss: 4.357181161642075\n",
      "Slice 83 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 84\n",
      "Slice 84 best combined loss: 4.728620529174805\n",
      "Slice 84 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 85\n",
      "Slice 85 best combined loss: 4.736848920583725\n",
      "Slice 85 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 86\n",
      "Slice 86 best combined loss: 4.709637254476547\n",
      "Slice 86 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 87\n",
      "Slice 87 best combined loss: 5.034242689609528\n",
      "Slice 87 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 88\n",
      "Slice 88 best combined loss: 4.926208019256592\n",
      "Slice 88 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 89\n",
      "Slice 89 best combined loss: 4.646923243999481\n",
      "Slice 89 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 90\n",
      "Slice 90 best combined loss: 4.189890593290329\n",
      "Slice 90 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 91\n",
      "Slice 91 best combined loss: 5.051866173744202\n",
      "Slice 91 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 92\n",
      "Slice 92 best combined loss: 4.675903618335724\n",
      "Slice 92 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 93\n",
      "Slice 93 best combined loss: 4.563452243804932\n",
      "Slice 93 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 94\n",
      "Slice 94 best combined loss: 4.936292886734009\n",
      "Slice 94 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 95\n",
      "Slice 95 best combined loss: 4.8374247550964355\n",
      "Slice 95 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 96\n",
      "Slice 96 best combined loss: 4.891724705696106\n",
      "Slice 96 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 97\n",
      "Slice 97 best combined loss: 5.051075577735901\n",
      "Slice 97 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 98\n",
      "Slice 98 best combined loss: 5.077495455741882\n",
      "Slice 98 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 99\n",
      "Slice 99 best combined loss: 4.607771337032318\n",
      "Slice 99 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 100\n",
      "Slice 100 best combined loss: 5.079974949359894\n",
      "Slice 100 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 101\n",
      "Slice 101 best combined loss: 4.772198110818863\n",
      "Slice 101 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 102\n",
      "Slice 102 best combined loss: 5.050822198390961\n",
      "Slice 102 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 103\n",
      "Slice 103 best combined loss: 4.260112255811691\n",
      "Slice 103 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 104\n",
      "Slice 104 best combined loss: 5.053301930427551\n",
      "Slice 104 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 105\n",
      "Slice 105 best combined loss: 4.624720931053162\n",
      "Slice 105 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 106\n",
      "Slice 106 best combined loss: 5.041068285703659\n",
      "Slice 106 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 107\n",
      "Slice 107 best combined loss: 5.08042848110199\n",
      "Slice 107 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 108\n",
      "Slice 108 best combined loss: 5.091697812080383\n",
      "Slice 108 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 109\n",
      "Slice 109 best combined loss: 4.360799938440323\n",
      "Slice 109 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 110\n",
      "Slice 110 best combined loss: 4.154737278819084\n",
      "Slice 110 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 111\n",
      "Slice 111 best combined loss: 4.506966322660446\n",
      "Slice 111 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 112\n",
      "Slice 112 best combined loss: 4.885492205619812\n",
      "Slice 112 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 113\n",
      "Slice 113 best combined loss: 4.756620138883591\n",
      "Slice 113 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 114\n",
      "Slice 114 best combined loss: 5.029284238815308\n",
      "Slice 114 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 115\n",
      "Slice 115 best combined loss: 5.062208473682404\n",
      "Slice 115 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 116\n",
      "Slice 116 best combined loss: 4.942081093788147\n",
      "Slice 116 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 117\n",
      "Slice 117 best combined loss: 5.036215037107468\n",
      "Slice 117 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 118\n",
      "Slice 118 best combined loss: 4.920866668224335\n",
      "Slice 118 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 119\n",
      "Slice 119 best combined loss: 5.049068421125412\n",
      "Slice 119 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 120\n",
      "Slice 120 best combined loss: 4.95246160030365\n",
      "Slice 120 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 121\n",
      "Slice 121 best combined loss: 4.582960546016693\n",
      "Slice 121 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 122\n",
      "Slice 122 best combined loss: 4.95717716217041\n",
      "Slice 122 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 123\n",
      "Slice 123 best combined loss: 4.77576220035553\n",
      "Slice 123 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 124\n",
      "Slice 124 best combined loss: 4.94530189037323\n",
      "Slice 124 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 125\n",
      "Slice 125 best combined loss: 5.081288933753967\n",
      "Slice 125 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 126\n",
      "Slice 126 best combined loss: 5.044535160064697\n",
      "Slice 126 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 127\n",
      "Slice 127 best combined loss: 4.889050364494324\n",
      "Slice 127 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 128\n",
      "Slice 128 best combined loss: 4.908455580472946\n",
      "Slice 128 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 129\n",
      "Slice 129 best combined loss: 4.236669898033142\n",
      "Slice 129 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 130\n",
      "Slice 130 best combined loss: 5.085521221160889\n",
      "Slice 130 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 131\n",
      "Slice 131 best combined loss: 4.769807517528534\n",
      "Slice 131 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 132\n",
      "Slice 132 best combined loss: 4.878640592098236\n",
      "Slice 132 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 133\n",
      "Slice 133 best combined loss: 5.044181555509567\n",
      "Slice 133 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 134\n",
      "Slice 134 best combined loss: 4.940933585166931\n",
      "Slice 134 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 135\n",
      "Slice 135 best combined loss: 4.3026513159275055\n",
      "Slice 135 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 136\n",
      "Slice 136 best combined loss: 4.591071724891663\n",
      "Slice 136 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 137\n",
      "Slice 137 best combined loss: 4.742769211530685\n",
      "Slice 137 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 138\n",
      "Slice 138 best combined loss: 4.529622882604599\n",
      "Slice 138 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 139\n",
      "Slice 139 best combined loss: 5.051519453525543\n",
      "Slice 139 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 140\n",
      "Slice 140 best combined loss: 4.669804126024246\n",
      "Slice 140 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 141\n",
      "Slice 141 best combined loss: 4.909471809864044\n",
      "Slice 141 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 142\n",
      "Slice 142 best combined loss: 4.639693811535835\n",
      "Slice 142 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 143\n",
      "Slice 143 best combined loss: 5.082964360713959\n",
      "Slice 143 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 144\n",
      "Slice 144 best combined loss: 4.952304899692535\n",
      "Slice 144 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 145\n",
      "Slice 145 best combined loss: 4.707745254039764\n",
      "Slice 145 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 146\n",
      "Slice 146 best combined loss: 4.859687149524689\n",
      "Slice 146 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 147\n",
      "Slice 147 best combined loss: 4.528046697378159\n",
      "Slice 147 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 148\n",
      "Slice 148 best combined loss: 4.544010043144226\n",
      "Slice 148 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 149\n",
      "Slice 149 best combined loss: 4.940091013908386\n",
      "Slice 149 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 150\n",
      "Slice 150 best combined loss: 4.560823410749435\n",
      "Slice 150 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 151\n",
      "Slice 151 best combined loss: 4.503095060586929\n",
      "Slice 151 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 152\n",
      "Slice 152 best combined loss: 5.026888906955719\n",
      "Slice 152 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 153\n",
      "Slice 153 best combined loss: 5.024681329727173\n",
      "Slice 153 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 154\n",
      "Slice 154 best combined loss: 4.704673647880554\n",
      "Slice 154 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 155\n",
      "Slice 155 best combined loss: 5.038545191287994\n",
      "Slice 155 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 156\n",
      "Slice 156 best combined loss: 4.754432886838913\n",
      "Slice 156 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 157\n",
      "Slice 157 best combined loss: 5.105097621679306\n",
      "Slice 157 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 158\n",
      "Slice 158 best combined loss: 4.653583198785782\n",
      "Slice 158 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 159\n",
      "Slice 159 best combined loss: 5.044890373945236\n",
      "Slice 159 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 160\n",
      "Slice 160 best combined loss: 4.702984154224396\n",
      "Slice 160 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 161\n",
      "Slice 161 best combined loss: 5.054002523422241\n",
      "Slice 161 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 162\n",
      "Slice 162 best combined loss: 4.593561470508575\n",
      "Slice 162 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 163\n",
      "Slice 163 best combined loss: 4.7159770131111145\n",
      "Slice 163 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 164\n",
      "Slice 164 best combined loss: 4.531858295202255\n",
      "Slice 164 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 165\n",
      "Slice 165 best combined loss: 4.953474223613739\n",
      "Slice 165 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 166\n",
      "Slice 166 best combined loss: 4.561946243047714\n",
      "Slice 166 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 167\n",
      "Slice 167 best combined loss: 5.075256794691086\n",
      "Slice 167 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 168\n",
      "Slice 168 best combined loss: 4.733866840600967\n",
      "Slice 168 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 169\n",
      "Slice 169 best combined loss: 4.7557260394096375\n",
      "Slice 169 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 170\n",
      "Slice 170 best combined loss: 4.805633544921875\n",
      "Slice 170 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 171\n",
      "Slice 171 best combined loss: 5.049835383892059\n",
      "Slice 171 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 172\n",
      "Slice 172 best combined loss: 5.09457665681839\n",
      "Slice 172 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 173\n",
      "Slice 173 best combined loss: 5.018685042858124\n",
      "Slice 173 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 174\n",
      "Slice 174 best combined loss: 5.068868339061737\n",
      "Slice 174 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 175\n",
      "Slice 175 best combined loss: 4.990844398736954\n",
      "Slice 175 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 176\n",
      "Slice 176 best combined loss: 5.103111445903778\n",
      "Slice 176 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 177\n",
      "Slice 177 best combined loss: 4.44174200296402\n",
      "Slice 177 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 178\n",
      "Slice 178 best combined loss: 5.016695499420166\n",
      "Slice 178 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 179\n",
      "Slice 179 best combined loss: 4.977935373783112\n",
      "Slice 179 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 180\n",
      "Slice 180 best combined loss: 4.906664252281189\n",
      "Slice 180 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 181\n",
      "Slice 181 best combined loss: 4.768959403038025\n",
      "Slice 181 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 182\n",
      "Slice 182 best combined loss: 4.614542067050934\n",
      "Slice 182 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 183\n",
      "Slice 183 best combined loss: 5.053456395864487\n",
      "Slice 183 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 184\n",
      "Slice 184 best combined loss: 4.77540048956871\n",
      "Slice 184 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 185\n",
      "Slice 185 best combined loss: 4.834866106510162\n",
      "Slice 185 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 186\n",
      "Slice 186 best combined loss: 4.362813323736191\n",
      "Slice 186 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 187\n",
      "Slice 187 best combined loss: 4.963993936777115\n",
      "Slice 187 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 188\n",
      "Slice 188 best combined loss: 4.847678691148758\n",
      "Slice 188 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 189\n",
      "Slice 189 best combined loss: 4.936394929885864\n",
      "Slice 189 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 190\n",
      "Slice 190 best combined loss: 4.98334801197052\n",
      "Slice 190 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 191\n",
      "Slice 191 best combined loss: 4.287310853600502\n",
      "Slice 191 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 192\n",
      "Slice 192 best combined loss: 4.9676856100559235\n",
      "Slice 192 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 193\n",
      "Slice 193 best combined loss: 4.899299442768097\n",
      "Slice 193 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 194\n",
      "Slice 194 best combined loss: 4.0712925642728806\n",
      "Slice 194 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 195\n",
      "Slice 195 best combined loss: 5.047417342662811\n",
      "Slice 195 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 196\n",
      "Slice 196 best combined loss: 4.962751597166061\n",
      "Slice 196 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 197\n",
      "Slice 197 best combined loss: 4.996110260486603\n",
      "Slice 197 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 198\n",
      "Slice 198 best combined loss: 4.846413105726242\n",
      "Slice 198 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 199\n",
      "Slice 199 best combined loss: 5.089064836502075\n",
      "Slice 199 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 200\n",
      "Slice 200 best combined loss: 4.733070969581604\n",
      "Slice 200 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 201\n",
      "Slice 201 best combined loss: 4.669153571128845\n",
      "Slice 201 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 202\n",
      "Slice 202 best combined loss: 5.039846122264862\n",
      "Slice 202 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 203\n",
      "Slice 203 best combined loss: 4.690511077642441\n",
      "Slice 203 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 204\n",
      "Slice 204 best combined loss: 5.020467400550842\n",
      "Slice 204 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 205\n",
      "Slice 205 best combined loss: 4.438332438468933\n",
      "Slice 205 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 206\n",
      "Slice 206 best combined loss: 4.962844222784042\n",
      "Slice 206 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 207\n",
      "Slice 207 best combined loss: 5.005181580781937\n",
      "Slice 207 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 208\n",
      "Slice 208 best combined loss: 4.926503509283066\n",
      "Slice 208 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 209\n",
      "Slice 209 best combined loss: 5.012057662010193\n",
      "Slice 209 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 210\n",
      "Slice 210 best combined loss: 5.035262703895569\n",
      "Slice 210 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 211\n",
      "Slice 211 best combined loss: 5.15467032790184\n",
      "Slice 211 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 212\n",
      "Slice 212 best combined loss: 4.494348347187042\n",
      "Slice 212 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 213\n",
      "Slice 213 best combined loss: 5.014103293418884\n",
      "Slice 213 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 214\n",
      "Slice 214 best combined loss: 4.980405330657959\n",
      "Slice 214 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 215\n",
      "Slice 215 best combined loss: 5.0138387978076935\n",
      "Slice 215 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 216\n",
      "Slice 216 best combined loss: 4.995389431715012\n",
      "Slice 216 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 217\n",
      "Slice 217 best combined loss: 4.944488763809204\n",
      "Slice 217 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 218\n",
      "Slice 218 best combined loss: 5.061961472034454\n",
      "Slice 218 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 219\n",
      "Slice 219 best combined loss: 4.978129833936691\n",
      "Slice 219 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 220\n",
      "Slice 220 best combined loss: 4.389255255460739\n",
      "Slice 220 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 221\n",
      "Slice 221 best combined loss: 5.080350309610367\n",
      "Slice 221 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 222\n",
      "Slice 222 best combined loss: 4.597026914358139\n",
      "Slice 222 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 223\n",
      "Slice 223 best combined loss: 5.06532096862793\n",
      "Slice 223 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 224\n",
      "Slice 224 best combined loss: 4.980022102594376\n",
      "Slice 224 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 225\n",
      "Slice 225 best combined loss: 5.029838502407074\n",
      "Slice 225 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 226\n",
      "Slice 226 best combined loss: 5.086365908384323\n",
      "Slice 226 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 227\n",
      "Slice 227 best combined loss: 4.048810720443726\n",
      "Slice 227 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 228\n",
      "Slice 228 best combined loss: 5.0792878568172455\n",
      "Slice 228 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 229\n",
      "Slice 229 best combined loss: 4.857602298259735\n",
      "Slice 229 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 230\n",
      "Slice 230 best combined loss: 4.852011859416962\n",
      "Slice 230 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 231\n",
      "Slice 231 best combined loss: 5.040919363498688\n",
      "Slice 231 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 232\n",
      "Slice 232 best combined loss: 4.675847262144089\n",
      "Slice 232 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 233\n",
      "Slice 233 best combined loss: 4.786588370800018\n",
      "Slice 233 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 234\n",
      "Slice 234 best combined loss: 4.155503332614899\n",
      "Slice 234 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 235\n",
      "Slice 235 best combined loss: 5.096605122089386\n",
      "Slice 235 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 236\n",
      "Slice 236 best combined loss: 4.994264721870422\n",
      "Slice 236 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 237\n",
      "Slice 237 best combined loss: 4.782246649265289\n",
      "Slice 237 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 238\n",
      "Slice 238 best combined loss: 4.637180209159851\n",
      "Slice 238 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 239\n",
      "Slice 239 best combined loss: 4.559209853410721\n",
      "Slice 239 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 240\n",
      "Slice 240 best combined loss: 4.861804932355881\n",
      "Slice 240 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 241\n",
      "Slice 241 best combined loss: 5.0712777972221375\n",
      "Slice 241 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 242\n",
      "Slice 242 best combined loss: 4.672417789697647\n",
      "Slice 242 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 243\n",
      "Slice 243 best combined loss: 4.739969938993454\n",
      "Slice 243 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 244\n",
      "Slice 244 best combined loss: 4.939750701189041\n",
      "Slice 244 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 245\n",
      "Slice 245 best combined loss: 4.996211409568787\n",
      "Slice 245 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 246\n",
      "Slice 246 best combined loss: 5.068641901016235\n",
      "Slice 246 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 247\n",
      "Slice 247 best combined loss: 4.765132933855057\n",
      "Slice 247 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 248\n",
      "Slice 248 best combined loss: 4.921125560998917\n",
      "Slice 248 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 249\n",
      "Slice 249 best combined loss: 4.66137769818306\n",
      "Slice 249 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 250\n",
      "Slice 250 best combined loss: 5.02523547410965\n",
      "Slice 250 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 251\n",
      "Slice 251 best combined loss: 4.459759145975113\n",
      "Slice 251 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 252\n",
      "Slice 252 best combined loss: 4.670744597911835\n",
      "Slice 252 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 253\n",
      "Slice 253 best combined loss: 4.479601502418518\n",
      "Slice 253 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 254\n",
      "Slice 254 best combined loss: 4.953085839748383\n",
      "Slice 254 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 255\n",
      "Slice 255 best combined loss: 5.11831134557724\n",
      "Slice 255 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 256\n",
      "Slice 256 best combined loss: 4.880630850791931\n",
      "Slice 256 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 257\n",
      "Slice 257 best combined loss: 4.4569467306137085\n",
      "Slice 257 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 258\n",
      "Slice 258 best combined loss: 5.089086204767227\n",
      "Slice 258 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 259\n",
      "Slice 259 best combined loss: 5.099103808403015\n",
      "Slice 259 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 260\n",
      "Slice 260 best combined loss: 4.818912386894226\n",
      "Slice 260 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 261\n",
      "Slice 261 best combined loss: 5.054533749818802\n",
      "Slice 261 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 262\n",
      "Slice 262 best combined loss: 5.061178803443909\n",
      "Slice 262 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 263\n",
      "Slice 263 best combined loss: 5.071018278598785\n",
      "Slice 263 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 264\n",
      "Slice 264 best combined loss: 4.246679872274399\n",
      "Slice 264 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 265\n",
      "Slice 265 best combined loss: 4.94629430770874\n",
      "Slice 265 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 266\n",
      "Slice 266 best combined loss: 4.9006993770599365\n",
      "Slice 266 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 267\n",
      "Slice 267 best combined loss: 4.201750159263611\n",
      "Slice 267 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 268\n",
      "Slice 268 best combined loss: 5.033138632774353\n",
      "Slice 268 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 269\n",
      "Slice 269 best combined loss: 4.663599580526352\n",
      "Slice 269 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 270\n",
      "Slice 270 best combined loss: 5.0312298238277435\n",
      "Slice 270 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 271\n",
      "Slice 271 best combined loss: 3.0793561935424805\n",
      "Slice 271 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 272\n",
      "Slice 272 best combined loss: 4.616540372371674\n",
      "Slice 272 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 273\n",
      "Slice 273 best combined loss: 4.113721162080765\n",
      "Slice 273 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 274\n",
      "Slice 274 best combined loss: 5.032063901424408\n",
      "Slice 274 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 275\n",
      "Slice 275 best combined loss: 5.043087840080261\n",
      "Slice 275 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 276\n",
      "Slice 276 best combined loss: 4.224506914615631\n",
      "Slice 276 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 277\n",
      "Slice 277 best combined loss: 5.055535674095154\n",
      "Slice 277 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 278\n",
      "Slice 278 best combined loss: 4.906428515911102\n",
      "Slice 278 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 279\n",
      "Slice 279 best combined loss: 4.13507704436779\n",
      "Slice 279 best test accuracy: 48.39% \n",
      "\n",
      "Slice: 280\n",
      "Slice 280 best combined loss: 5.070889830589294\n",
      "Slice 280 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 281\n",
      "Slice 281 best combined loss: 4.802293479442596\n",
      "Slice 281 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 282\n",
      "Slice 282 best combined loss: 4.5439742505550385\n",
      "Slice 282 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 283\n",
      "Slice 283 best combined loss: 5.095889508724213\n",
      "Slice 283 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 284\n",
      "Slice 284 best combined loss: 4.537446975708008\n",
      "Slice 284 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 285\n",
      "Slice 285 best combined loss: 4.475394695997238\n",
      "Slice 285 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 286\n",
      "Slice 286 best combined loss: 4.985436886548996\n",
      "Slice 286 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 287\n",
      "Slice 287 best combined loss: 4.444114863872528\n",
      "Slice 287 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 288\n",
      "Slice 288 best combined loss: 4.9838347136974335\n",
      "Slice 288 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 289\n",
      "Slice 289 best combined loss: 5.077014625072479\n",
      "Slice 289 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 290\n",
      "Slice 290 best combined loss: 4.678456485271454\n",
      "Slice 290 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 291\n",
      "Slice 291 best combined loss: 4.251067906618118\n",
      "Slice 291 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 292\n",
      "Slice 292 best combined loss: 4.836511313915253\n",
      "Slice 292 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 293\n",
      "Slice 293 best combined loss: 4.488173425197601\n",
      "Slice 293 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 294\n",
      "Slice 294 best combined loss: 4.985951066017151\n",
      "Slice 294 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 295\n",
      "Slice 295 best combined loss: 4.806506246328354\n",
      "Slice 295 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 296\n",
      "Slice 296 best combined loss: 4.4978538900613785\n",
      "Slice 296 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 297\n",
      "Slice 297 best combined loss: 5.054087191820145\n",
      "Slice 297 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 298\n",
      "Slice 298 best combined loss: 4.816366225481033\n",
      "Slice 298 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 299\n",
      "Slice 299 best combined loss: 4.786432772874832\n",
      "Slice 299 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 300\n",
      "Slice 300 best combined loss: 5.019917011260986\n",
      "Slice 300 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 301\n",
      "Slice 301 best combined loss: 4.702115952968597\n",
      "Slice 301 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 302\n",
      "Slice 302 best combined loss: 4.969565033912659\n",
      "Slice 302 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 303\n",
      "Slice 303 best combined loss: 5.064589440822601\n",
      "Slice 303 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 304\n",
      "Slice 304 best combined loss: 5.074674546718597\n",
      "Slice 304 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 305\n",
      "Slice 305 best combined loss: 4.889451622962952\n",
      "Slice 305 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 306\n",
      "Slice 306 best combined loss: 4.837700337171555\n",
      "Slice 306 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 307\n",
      "Slice 307 best combined loss: 4.9213374853134155\n",
      "Slice 307 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 308\n",
      "Slice 308 best combined loss: 5.07173764705658\n",
      "Slice 308 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 309\n",
      "Slice 309 best combined loss: 4.91660475730896\n",
      "Slice 309 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 310\n",
      "Slice 310 best combined loss: 5.046168297529221\n",
      "Slice 310 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 311\n",
      "Slice 311 best combined loss: 4.957535266876221\n",
      "Slice 311 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 312\n",
      "Slice 312 best combined loss: 4.990069687366486\n",
      "Slice 312 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 313\n",
      "Slice 313 best combined loss: 5.065617084503174\n",
      "Slice 313 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 314\n",
      "Slice 314 best combined loss: 3.9424169063568115\n",
      "Slice 314 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 315\n",
      "Slice 315 best combined loss: 4.87822812795639\n",
      "Slice 315 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 316\n",
      "Slice 316 best combined loss: 4.612572282552719\n",
      "Slice 316 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 317\n",
      "Slice 317 best combined loss: 4.810925543308258\n",
      "Slice 317 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 318\n",
      "Slice 318 best combined loss: 4.95425146818161\n",
      "Slice 318 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 319\n",
      "Slice 319 best combined loss: 4.99821925163269\n",
      "Slice 319 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 320\n",
      "Slice 320 best combined loss: 5.079840660095215\n",
      "Slice 320 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 321\n",
      "Slice 321 best combined loss: 4.555760085582733\n",
      "Slice 321 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 322\n",
      "Slice 322 best combined loss: 4.994696855545044\n",
      "Slice 322 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 323\n",
      "Slice 323 best combined loss: 4.621880769729614\n",
      "Slice 323 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 324\n",
      "Slice 324 best combined loss: 4.766639351844788\n",
      "Slice 324 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 325\n",
      "Slice 325 best combined loss: 5.087452530860901\n",
      "Slice 325 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 326\n",
      "Slice 326 best combined loss: 5.047434896230698\n",
      "Slice 326 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 327\n",
      "Slice 327 best combined loss: 5.071902722120285\n",
      "Slice 327 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 328\n",
      "Slice 328 best combined loss: 5.233490884304047\n",
      "Slice 328 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 329\n",
      "Slice 329 best combined loss: 4.983130067586899\n",
      "Slice 329 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 330\n",
      "Slice 330 best combined loss: 4.624366611242294\n",
      "Slice 330 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 331\n",
      "Slice 331 best combined loss: 4.760484486818314\n",
      "Slice 331 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 332\n",
      "Slice 332 best combined loss: 4.904926359653473\n",
      "Slice 332 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 333\n",
      "Slice 333 best combined loss: 4.7856961488723755\n",
      "Slice 333 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 334\n",
      "Slice 334 best combined loss: 4.788342624902725\n",
      "Slice 334 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 335\n",
      "Slice 335 best combined loss: 4.587989866733551\n",
      "Slice 335 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 336\n",
      "Slice 336 best combined loss: 4.9852911829948425\n",
      "Slice 336 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 337\n",
      "Slice 337 best combined loss: 5.080580413341522\n",
      "Slice 337 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 338\n",
      "Slice 338 best combined loss: 3.857037276029587\n",
      "Slice 338 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 339\n",
      "Slice 339 best combined loss: 4.762456178665161\n",
      "Slice 339 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 340\n",
      "Slice 340 best combined loss: 4.9450942277908325\n",
      "Slice 340 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 341\n",
      "Slice 341 best combined loss: 4.846126914024353\n",
      "Slice 341 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 342\n",
      "Slice 342 best combined loss: 4.994853258132935\n",
      "Slice 342 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 343\n",
      "Slice 343 best combined loss: 4.987283021211624\n",
      "Slice 343 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 344\n",
      "Slice 344 best combined loss: 4.555351734161377\n",
      "Slice 344 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 345\n",
      "Slice 345 best combined loss: 4.027782365679741\n",
      "Slice 345 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 346\n",
      "Slice 346 best combined loss: 4.625857383012772\n",
      "Slice 346 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 347\n",
      "Slice 347 best combined loss: 5.025779902935028\n",
      "Slice 347 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 348\n",
      "Slice 348 best combined loss: 5.06645205616951\n",
      "Slice 348 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 349\n",
      "Slice 349 best combined loss: 4.134146094322205\n",
      "Slice 349 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 350\n",
      "Slice 350 best combined loss: 3.9819152653217316\n",
      "Slice 350 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 351\n",
      "Slice 351 best combined loss: 4.671942085027695\n",
      "Slice 351 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 352\n",
      "Slice 352 best combined loss: 5.035616338253021\n",
      "Slice 352 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 353\n",
      "Slice 353 best combined loss: 4.568852186203003\n",
      "Slice 353 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 354\n",
      "Slice 354 best combined loss: 4.525763750076294\n",
      "Slice 354 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 355\n",
      "Slice 355 best combined loss: 5.014363050460815\n",
      "Slice 355 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 356\n",
      "Slice 356 best combined loss: 4.6306058168411255\n",
      "Slice 356 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 357\n",
      "Slice 357 best combined loss: 5.050823122262955\n",
      "Slice 357 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 358\n",
      "Slice 358 best combined loss: 4.722260683774948\n",
      "Slice 358 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 359\n",
      "Slice 359 best combined loss: 5.081312447786331\n",
      "Slice 359 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 360\n",
      "Slice 360 best combined loss: 4.248046576976776\n",
      "Slice 360 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 361\n",
      "Slice 361 best combined loss: 3.9984308779239655\n",
      "Slice 361 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 362\n",
      "Slice 362 best combined loss: 5.0741656720638275\n",
      "Slice 362 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 363\n",
      "Slice 363 best combined loss: 4.52829110622406\n",
      "Slice 363 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 364\n",
      "Slice 364 best combined loss: 5.050998687744141\n",
      "Slice 364 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 365\n",
      "Slice 365 best combined loss: 5.02190226316452\n",
      "Slice 365 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 366\n",
      "Slice 366 best combined loss: 4.984045177698135\n",
      "Slice 366 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 367\n",
      "Slice 367 best combined loss: 5.033913493156433\n",
      "Slice 367 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 368\n",
      "Slice 368 best combined loss: 4.1751004457473755\n",
      "Slice 368 best test accuracy: 87.10% \n",
      "\n",
      "Slice: 369\n",
      "Slice 369 best combined loss: 4.930139392614365\n",
      "Slice 369 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 370\n",
      "Slice 370 best combined loss: 5.044683575630188\n",
      "Slice 370 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 371\n",
      "Slice 371 best combined loss: 4.647687584161758\n",
      "Slice 371 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 372\n",
      "Slice 372 best combined loss: 3.741394519805908\n",
      "Slice 372 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 373\n",
      "Slice 373 best combined loss: 4.310452729463577\n",
      "Slice 373 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 374\n",
      "Slice 374 best combined loss: 5.046290844678879\n",
      "Slice 374 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 375\n",
      "Slice 375 best combined loss: 4.758363336324692\n",
      "Slice 375 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 376\n",
      "Slice 376 best combined loss: 4.873005717992783\n",
      "Slice 376 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 377\n",
      "Slice 377 best combined loss: 5.055249094963074\n",
      "Slice 377 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 378\n",
      "Slice 378 best combined loss: 4.914471626281738\n",
      "Slice 378 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 379\n",
      "Slice 379 best combined loss: 4.739958018064499\n",
      "Slice 379 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 380\n",
      "Slice 380 best combined loss: 4.90106263756752\n",
      "Slice 380 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 381\n",
      "Slice 381 best combined loss: 4.664431214332581\n",
      "Slice 381 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 382\n",
      "Slice 382 best combined loss: 4.763584136962891\n",
      "Slice 382 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 383\n",
      "Slice 383 best combined loss: 4.712828576564789\n",
      "Slice 383 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 384\n",
      "Slice 384 best combined loss: 4.944008946418762\n",
      "Slice 384 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 385\n",
      "Slice 385 best combined loss: 5.065623909235001\n",
      "Slice 385 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 386\n",
      "Slice 386 best combined loss: 4.893153697252274\n",
      "Slice 386 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 387\n",
      "Slice 387 best combined loss: 4.792111873626709\n",
      "Slice 387 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 388\n",
      "Slice 388 best combined loss: 5.050757557153702\n",
      "Slice 388 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 389\n",
      "Slice 389 best combined loss: 5.039572179317474\n",
      "Slice 389 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 390\n",
      "Slice 390 best combined loss: 5.046279758214951\n",
      "Slice 390 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 391\n",
      "Slice 391 best combined loss: 4.862257659435272\n",
      "Slice 391 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 392\n",
      "Slice 392 best combined loss: 4.881215810775757\n",
      "Slice 392 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 393\n",
      "Slice 393 best combined loss: 5.045918643474579\n",
      "Slice 393 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 394\n",
      "Slice 394 best combined loss: 4.996195912361145\n",
      "Slice 394 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 395\n",
      "Slice 395 best combined loss: 4.944427669048309\n",
      "Slice 395 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 396\n",
      "Slice 396 best combined loss: 4.736542880535126\n",
      "Slice 396 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 397\n",
      "Slice 397 best combined loss: 4.603333532810211\n",
      "Slice 397 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 398\n",
      "Slice 398 best combined loss: 4.571126878261566\n",
      "Slice 398 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 399\n",
      "Slice 399 best combined loss: 4.706731915473938\n",
      "Slice 399 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 400\n",
      "Slice 400 best combined loss: 4.943568289279938\n",
      "Slice 400 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 401\n",
      "Slice 401 best combined loss: 4.807434022426605\n",
      "Slice 401 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 402\n",
      "Slice 402 best combined loss: 5.082647323608398\n",
      "Slice 402 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 403\n",
      "Slice 403 best combined loss: 4.917090982198715\n",
      "Slice 403 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 404\n",
      "Slice 404 best combined loss: 4.751754343509674\n",
      "Slice 404 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 405\n",
      "Slice 405 best combined loss: 4.8951791524887085\n",
      "Slice 405 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 406\n",
      "Slice 406 best combined loss: 4.678808569908142\n",
      "Slice 406 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 407\n",
      "Slice 407 best combined loss: 4.747495591640472\n",
      "Slice 407 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 408\n",
      "Slice 408 best combined loss: 5.059416979551315\n",
      "Slice 408 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 409\n",
      "Slice 409 best combined loss: 5.114185869693756\n",
      "Slice 409 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 410\n",
      "Slice 410 best combined loss: 4.984782457351685\n",
      "Slice 410 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 411\n",
      "Slice 411 best combined loss: 4.6971065402030945\n",
      "Slice 411 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 412\n",
      "Slice 412 best combined loss: 4.675562083721161\n",
      "Slice 412 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 413\n",
      "Slice 413 best combined loss: 4.927184164524078\n",
      "Slice 413 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 414\n",
      "Slice 414 best combined loss: 5.025036096572876\n",
      "Slice 414 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 415\n",
      "Slice 415 best combined loss: 4.928869187831879\n",
      "Slice 415 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 416\n",
      "Slice 416 best combined loss: 5.035638332366943\n",
      "Slice 416 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 417\n",
      "Slice 417 best combined loss: 4.969947040081024\n",
      "Slice 417 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 418\n",
      "Slice 418 best combined loss: 4.771605163812637\n",
      "Slice 418 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 419\n",
      "Slice 419 best combined loss: 4.835005939006805\n",
      "Slice 419 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 420\n",
      "Slice 420 best combined loss: 4.8004332184791565\n",
      "Slice 420 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 421\n",
      "Slice 421 best combined loss: 4.998700261116028\n",
      "Slice 421 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 422\n",
      "Slice 422 best combined loss: 4.970245182514191\n",
      "Slice 422 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 423\n",
      "Slice 423 best combined loss: 4.95988854765892\n",
      "Slice 423 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 424\n",
      "Slice 424 best combined loss: 4.942288398742676\n",
      "Slice 424 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 425\n",
      "Slice 425 best combined loss: 4.981534630060196\n",
      "Slice 425 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 426\n",
      "Slice 426 best combined loss: 4.891385555267334\n",
      "Slice 426 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 427\n",
      "Slice 427 best combined loss: 4.975670903921127\n",
      "Slice 427 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 428\n",
      "Slice 428 best combined loss: 5.027395009994507\n",
      "Slice 428 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 429\n",
      "Slice 429 best combined loss: 5.018062978982925\n",
      "Slice 429 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 430\n",
      "Slice 430 best combined loss: 4.500375464558601\n",
      "Slice 430 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 431\n",
      "Slice 431 best combined loss: 5.0345823764801025\n",
      "Slice 431 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 432\n",
      "Slice 432 best combined loss: 4.646258503198624\n",
      "Slice 432 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 433\n",
      "Slice 433 best combined loss: 4.9166310131549835\n",
      "Slice 433 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 434\n",
      "Slice 434 best combined loss: 4.1841999888420105\n",
      "Slice 434 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 435\n",
      "Slice 435 best combined loss: 5.062809228897095\n",
      "Slice 435 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 436\n",
      "Slice 436 best combined loss: 5.00839376449585\n",
      "Slice 436 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 437\n",
      "Slice 437 best combined loss: 5.083008736371994\n",
      "Slice 437 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 438\n",
      "Slice 438 best combined loss: 4.895721435546875\n",
      "Slice 438 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 439\n",
      "Slice 439 best combined loss: 4.749473750591278\n",
      "Slice 439 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 440\n",
      "Slice 440 best combined loss: 4.793358266353607\n",
      "Slice 440 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 441\n",
      "Slice 441 best combined loss: 4.9031462371349335\n",
      "Slice 441 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 442\n",
      "Slice 442 best combined loss: 4.790254443883896\n",
      "Slice 442 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 443\n",
      "Slice 443 best combined loss: 4.8703286945819855\n",
      "Slice 443 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 444\n",
      "Slice 444 best combined loss: 5.063558459281921\n",
      "Slice 444 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 445\n",
      "Slice 445 best combined loss: 5.137464463710785\n",
      "Slice 445 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 446\n",
      "Slice 446 best combined loss: 4.227290719747543\n",
      "Slice 446 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 447\n",
      "Slice 447 best combined loss: 4.741475969552994\n",
      "Slice 447 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 448\n",
      "Slice 448 best combined loss: 5.083155989646912\n",
      "Slice 448 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 449\n",
      "Slice 449 best combined loss: 4.204506888985634\n",
      "Slice 449 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 450\n",
      "Slice 450 best combined loss: 5.027388542890549\n",
      "Slice 450 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 451\n",
      "Slice 451 best combined loss: 4.26921620965004\n",
      "Slice 451 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 452\n",
      "Slice 452 best combined loss: 5.073010563850403\n",
      "Slice 452 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 453\n",
      "Slice 453 best combined loss: 5.073288857936859\n",
      "Slice 453 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 454\n",
      "Slice 454 best combined loss: 4.858730137348175\n",
      "Slice 454 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 455\n",
      "Slice 455 best combined loss: 4.907269835472107\n",
      "Slice 455 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 456\n",
      "Slice 456 best combined loss: 4.934088349342346\n",
      "Slice 456 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 457\n",
      "Slice 457 best combined loss: 4.737734019756317\n",
      "Slice 457 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 458\n",
      "Slice 458 best combined loss: 5.052074909210205\n",
      "Slice 458 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 459\n",
      "Slice 459 best combined loss: 5.06657487154007\n",
      "Slice 459 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 460\n",
      "Slice 460 best combined loss: 5.0079076290130615\n",
      "Slice 460 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 461\n",
      "Slice 461 best combined loss: 3.8071643263101578\n",
      "Slice 461 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 462\n",
      "Slice 462 best combined loss: 4.950782656669617\n",
      "Slice 462 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 463\n",
      "Slice 463 best combined loss: 5.000115782022476\n",
      "Slice 463 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 464\n",
      "Slice 464 best combined loss: 4.970202744007111\n",
      "Slice 464 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 465\n",
      "Slice 465 best combined loss: 5.012088894844055\n",
      "Slice 465 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 466\n",
      "Slice 466 best combined loss: 4.81154328584671\n",
      "Slice 466 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 467\n",
      "Slice 467 best combined loss: 4.903356373310089\n",
      "Slice 467 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 468\n",
      "Slice 468 best combined loss: 3.9272619634866714\n",
      "Slice 468 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 469\n",
      "Slice 469 best combined loss: 4.965381890535355\n",
      "Slice 469 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 470\n",
      "Slice 470 best combined loss: 4.538452506065369\n",
      "Slice 470 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 471\n",
      "Slice 471 best combined loss: 4.889306426048279\n",
      "Slice 471 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 472\n",
      "Slice 472 best combined loss: 4.470756530761719\n",
      "Slice 472 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 473\n",
      "Slice 473 best combined loss: 4.7613571882247925\n",
      "Slice 473 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 474\n",
      "Slice 474 best combined loss: 4.778362184762955\n",
      "Slice 474 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 475\n",
      "Slice 475 best combined loss: 4.40810889005661\n",
      "Slice 475 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 476\n",
      "Slice 476 best combined loss: 4.89675372838974\n",
      "Slice 476 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 477\n",
      "Slice 477 best combined loss: 4.859758168458939\n",
      "Slice 477 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 478\n",
      "Slice 478 best combined loss: 4.798815965652466\n",
      "Slice 478 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 479\n",
      "Slice 479 best combined loss: 5.087632566690445\n",
      "Slice 479 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 480\n",
      "Slice 480 best combined loss: 5.075348764657974\n",
      "Slice 480 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 481\n",
      "Slice 481 best combined loss: 4.688162297010422\n",
      "Slice 481 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 482\n",
      "Slice 482 best combined loss: 5.026851236820221\n",
      "Slice 482 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 483\n",
      "Slice 483 best combined loss: 4.022340804338455\n",
      "Slice 483 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 484\n",
      "Slice 484 best combined loss: 4.530647695064545\n",
      "Slice 484 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 485\n",
      "Slice 485 best combined loss: 4.730571389198303\n",
      "Slice 485 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 486\n",
      "Slice 486 best combined loss: 4.94711709022522\n",
      "Slice 486 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 487\n",
      "Slice 487 best combined loss: 4.897500216960907\n",
      "Slice 487 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 488\n",
      "Slice 488 best combined loss: 5.0669876635074615\n",
      "Slice 488 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 489\n",
      "Slice 489 best combined loss: 4.9044464230537415\n",
      "Slice 489 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 490\n",
      "Slice 490 best combined loss: 4.714788109064102\n",
      "Slice 490 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 491\n",
      "Slice 491 best combined loss: 5.047539055347443\n",
      "Slice 491 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 492\n",
      "Slice 492 best combined loss: 4.420810639858246\n",
      "Slice 492 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 493\n",
      "Slice 493 best combined loss: 5.119412481784821\n",
      "Slice 493 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 494\n",
      "Slice 494 best combined loss: 4.864497572183609\n",
      "Slice 494 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 495\n",
      "Slice 495 best combined loss: 5.106173366308212\n",
      "Slice 495 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 496\n",
      "Slice 496 best combined loss: 5.051992952823639\n",
      "Slice 496 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 497\n",
      "Slice 497 best combined loss: 5.019851624965668\n",
      "Slice 497 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 498\n",
      "Slice 498 best combined loss: 5.014983177185059\n",
      "Slice 498 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 499\n",
      "Slice 499 best combined loss: 5.074483215808868\n",
      "Slice 499 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 500\n",
      "Slice 500 best combined loss: 4.73527729511261\n",
      "Slice 500 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 501\n",
      "Slice 501 best combined loss: 5.061307370662689\n",
      "Slice 501 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 502\n",
      "Slice 502 best combined loss: 4.4048303961753845\n",
      "Slice 502 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 503\n",
      "Slice 503 best combined loss: 4.946024954319\n",
      "Slice 503 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 504\n",
      "Slice 504 best combined loss: 4.938492655754089\n",
      "Slice 504 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 505\n",
      "Slice 505 best combined loss: 5.065680503845215\n",
      "Slice 505 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 506\n",
      "Slice 506 best combined loss: 4.771285533905029\n",
      "Slice 506 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 507\n",
      "Slice 507 best combined loss: 5.086996465921402\n",
      "Slice 507 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 508\n",
      "Slice 508 best combined loss: 5.078587502241135\n",
      "Slice 508 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 509\n",
      "Slice 509 best combined loss: 5.086341977119446\n",
      "Slice 509 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 510\n",
      "Slice 510 best combined loss: 5.092354834079742\n",
      "Slice 510 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 511\n",
      "Slice 511 best combined loss: 4.413971275091171\n",
      "Slice 511 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 512\n",
      "Slice 512 best combined loss: 4.523893713951111\n",
      "Slice 512 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 513\n",
      "Slice 513 best combined loss: 5.021321535110474\n",
      "Slice 513 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 514\n",
      "Slice 514 best combined loss: 4.92772513628006\n",
      "Slice 514 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 515\n",
      "Slice 515 best combined loss: 5.018665611743927\n",
      "Slice 515 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 516\n",
      "Slice 516 best combined loss: 5.044568479061127\n",
      "Slice 516 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 517\n",
      "Slice 517 best combined loss: 4.947885870933533\n",
      "Slice 517 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 518\n",
      "Slice 518 best combined loss: 4.374990910291672\n",
      "Slice 518 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 519\n",
      "Slice 519 best combined loss: 4.6699923276901245\n",
      "Slice 519 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 520\n",
      "Slice 520 best combined loss: 4.3114359974861145\n",
      "Slice 520 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 521\n",
      "Slice 521 best combined loss: 4.694235414266586\n",
      "Slice 521 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 522\n",
      "Slice 522 best combined loss: 4.432875603437424\n",
      "Slice 522 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 523\n",
      "Slice 523 best combined loss: 4.538454636931419\n",
      "Slice 523 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 524\n",
      "Slice 524 best combined loss: 5.061547577381134\n",
      "Slice 524 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 525\n",
      "Slice 525 best combined loss: 5.110332131385803\n",
      "Slice 525 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 526\n",
      "Slice 526 best combined loss: 4.791559875011444\n",
      "Slice 526 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 527\n",
      "Slice 527 best combined loss: 5.00641006231308\n",
      "Slice 527 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 528\n",
      "Slice 528 best combined loss: 4.807513177394867\n",
      "Slice 528 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 529\n",
      "Slice 529 best combined loss: 5.091990649700165\n",
      "Slice 529 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 530\n",
      "Slice 530 best combined loss: 4.68708735704422\n",
      "Slice 530 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 531\n",
      "Slice 531 best combined loss: 5.014506280422211\n",
      "Slice 531 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 532\n",
      "Slice 532 best combined loss: 4.648337632417679\n",
      "Slice 532 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 533\n",
      "Slice 533 best combined loss: 4.789526849985123\n",
      "Slice 533 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 534\n",
      "Slice 534 best combined loss: 4.846388041973114\n",
      "Slice 534 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 535\n",
      "Slice 535 best combined loss: 4.420831024646759\n",
      "Slice 535 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 536\n",
      "Slice 536 best combined loss: 5.0617837607860565\n",
      "Slice 536 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 537\n",
      "Slice 537 best combined loss: 4.60260009765625\n",
      "Slice 537 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 538\n",
      "Slice 538 best combined loss: 5.027804136276245\n",
      "Slice 538 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 539\n",
      "Slice 539 best combined loss: 4.899124026298523\n",
      "Slice 539 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 540\n",
      "Slice 540 best combined loss: 4.822185814380646\n",
      "Slice 540 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 541\n",
      "Slice 541 best combined loss: 4.924492120742798\n",
      "Slice 541 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 542\n",
      "Slice 542 best combined loss: 5.007898509502411\n",
      "Slice 542 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 543\n",
      "Slice 543 best combined loss: 3.705087333917618\n",
      "Slice 543 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 544\n",
      "Slice 544 best combined loss: 4.969017446041107\n",
      "Slice 544 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 545\n",
      "Slice 545 best combined loss: 4.862033724784851\n",
      "Slice 545 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 546\n",
      "Slice 546 best combined loss: 5.023098975419998\n",
      "Slice 546 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 547\n",
      "Slice 547 best combined loss: 4.979142069816589\n",
      "Slice 547 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 548\n",
      "Slice 548 best combined loss: 4.768156915903091\n",
      "Slice 548 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 549\n",
      "Slice 549 best combined loss: 5.085801959037781\n",
      "Slice 549 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 550\n",
      "Slice 550 best combined loss: 5.070414632558823\n",
      "Slice 550 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 551\n",
      "Slice 551 best combined loss: 5.026702374219894\n",
      "Slice 551 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 552\n",
      "Slice 552 best combined loss: 5.086401283740997\n",
      "Slice 552 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 553\n",
      "Slice 553 best combined loss: 4.926766097545624\n",
      "Slice 553 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 554\n",
      "Slice 554 best combined loss: 4.57016022503376\n",
      "Slice 554 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 555\n",
      "Slice 555 best combined loss: 5.057728171348572\n",
      "Slice 555 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 556\n",
      "Slice 556 best combined loss: 5.016384243965149\n",
      "Slice 556 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 557\n",
      "Slice 557 best combined loss: 5.056789755821228\n",
      "Slice 557 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 558\n",
      "Slice 558 best combined loss: 4.912165105342865\n",
      "Slice 558 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 559\n",
      "Slice 559 best combined loss: 5.083013951778412\n",
      "Slice 559 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 560\n",
      "Slice 560 best combined loss: 4.959193974733353\n",
      "Slice 560 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 561\n",
      "Slice 561 best combined loss: 4.984476208686829\n",
      "Slice 561 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 562\n",
      "Slice 562 best combined loss: 4.5759041756391525\n",
      "Slice 562 best test accuracy: 83.87% \n",
      "\n",
      "Slice: 563\n",
      "Slice 563 best combined loss: 5.0806417763233185\n",
      "Slice 563 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 564\n",
      "Slice 564 best combined loss: 4.650109171867371\n",
      "Slice 564 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 565\n",
      "Slice 565 best combined loss: 4.30903634428978\n",
      "Slice 565 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 566\n",
      "Slice 566 best combined loss: 4.9029675126075745\n",
      "Slice 566 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 567\n",
      "Slice 567 best combined loss: 4.754222422838211\n",
      "Slice 567 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 568\n",
      "Slice 568 best combined loss: 4.917874693870544\n",
      "Slice 568 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 569\n",
      "Slice 569 best combined loss: 4.906796157360077\n",
      "Slice 569 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 570\n",
      "Slice 570 best combined loss: 4.862958133220673\n",
      "Slice 570 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 571\n",
      "Slice 571 best combined loss: 5.046536833047867\n",
      "Slice 571 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 572\n",
      "Slice 572 best combined loss: 4.997575640678406\n",
      "Slice 572 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 573\n",
      "Slice 573 best combined loss: 4.0259683430194855\n",
      "Slice 573 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 574\n",
      "Slice 574 best combined loss: 4.819557309150696\n",
      "Slice 574 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 575\n",
      "Slice 575 best combined loss: 4.927236586809158\n",
      "Slice 575 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 576\n",
      "Slice 576 best combined loss: 4.912612497806549\n",
      "Slice 576 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 577\n",
      "Slice 577 best combined loss: 5.070835530757904\n",
      "Slice 577 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 578\n",
      "Slice 578 best combined loss: 4.132331624627113\n",
      "Slice 578 best test accuracy: 83.87% \n",
      "\n",
      "Slice: 579\n",
      "Slice 579 best combined loss: 5.028848260641098\n",
      "Slice 579 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 580\n",
      "Slice 580 best combined loss: 4.882665634155273\n",
      "Slice 580 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 581\n",
      "Slice 581 best combined loss: 4.432897567749023\n",
      "Slice 581 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 582\n",
      "Slice 582 best combined loss: 5.036855518817902\n",
      "Slice 582 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 583\n",
      "Slice 583 best combined loss: 4.760800331830978\n",
      "Slice 583 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 584\n",
      "Slice 584 best combined loss: 5.01972895860672\n",
      "Slice 584 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 585\n",
      "Slice 585 best combined loss: 5.086081862449646\n",
      "Slice 585 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 586\n",
      "Slice 586 best combined loss: 5.014916777610779\n",
      "Slice 586 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 587\n",
      "Slice 587 best combined loss: 4.160991191864014\n",
      "Slice 587 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 588\n",
      "Slice 588 best combined loss: 3.9037241637706757\n",
      "Slice 588 best test accuracy: 87.10% \n",
      "\n",
      "Slice: 589\n",
      "Slice 589 best combined loss: 4.4687575697898865\n",
      "Slice 589 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 590\n",
      "Slice 590 best combined loss: 4.987461984157562\n",
      "Slice 590 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 591\n",
      "Slice 591 best combined loss: 4.788151383399963\n",
      "Slice 591 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 592\n",
      "Slice 592 best combined loss: 4.4367184937000275\n",
      "Slice 592 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 593\n",
      "Slice 593 best combined loss: 4.534736514091492\n",
      "Slice 593 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 594\n",
      "Slice 594 best combined loss: 5.052934646606445\n",
      "Slice 594 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 595\n",
      "Slice 595 best combined loss: 4.865978091955185\n",
      "Slice 595 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 596\n",
      "Slice 596 best combined loss: 4.967441648244858\n",
      "Slice 596 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 597\n",
      "Slice 597 best combined loss: 5.086898565292358\n",
      "Slice 597 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 598\n",
      "Slice 598 best combined loss: 4.929674714803696\n",
      "Slice 598 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 599\n",
      "Slice 599 best combined loss: 4.661870658397675\n",
      "Slice 599 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 600\n",
      "Slice 600 best combined loss: 5.077619820833206\n",
      "Slice 600 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 601\n",
      "Slice 601 best combined loss: 4.6789660304784775\n",
      "Slice 601 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 602\n",
      "Slice 602 best combined loss: 4.708332419395447\n",
      "Slice 602 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 603\n",
      "Slice 603 best combined loss: 4.773117125034332\n",
      "Slice 603 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 604\n",
      "Slice 604 best combined loss: 5.081160068511963\n",
      "Slice 604 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 605\n",
      "Slice 605 best combined loss: 4.359232261776924\n",
      "Slice 605 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 606\n",
      "Slice 606 best combined loss: 4.928947567939758\n",
      "Slice 606 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 607\n",
      "Slice 607 best combined loss: 4.984306424856186\n",
      "Slice 607 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 608\n",
      "Slice 608 best combined loss: 4.836474925279617\n",
      "Slice 608 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 609\n",
      "Slice 609 best combined loss: 5.0841768980026245\n",
      "Slice 609 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 610\n",
      "Slice 610 best combined loss: 5.005651533603668\n",
      "Slice 610 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 611\n",
      "Slice 611 best combined loss: 4.344717189669609\n",
      "Slice 611 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 612\n",
      "Slice 612 best combined loss: 5.035045742988586\n",
      "Slice 612 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 613\n",
      "Slice 613 best combined loss: 4.927191197872162\n",
      "Slice 613 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 614\n",
      "Slice 614 best combined loss: 5.106681644916534\n",
      "Slice 614 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 615\n",
      "Slice 615 best combined loss: 4.890804320573807\n",
      "Slice 615 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 616\n",
      "Slice 616 best combined loss: 4.61298531293869\n",
      "Slice 616 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 617\n",
      "Slice 617 best combined loss: 4.713273048400879\n",
      "Slice 617 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 618\n",
      "Slice 618 best combined loss: 4.79853618144989\n",
      "Slice 618 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 619\n",
      "Slice 619 best combined loss: 4.965758860111237\n",
      "Slice 619 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 620\n",
      "Slice 620 best combined loss: 4.747891068458557\n",
      "Slice 620 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 621\n",
      "Slice 621 best combined loss: 4.9111286997795105\n",
      "Slice 621 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 622\n",
      "Slice 622 best combined loss: 4.035795599222183\n",
      "Slice 622 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 623\n",
      "Slice 623 best combined loss: 4.815787315368652\n",
      "Slice 623 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 624\n",
      "Slice 624 best combined loss: 5.099535524845123\n",
      "Slice 624 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 625\n",
      "Slice 625 best combined loss: 4.647435039281845\n",
      "Slice 625 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 626\n",
      "Slice 626 best combined loss: 3.468146800994873\n",
      "Slice 626 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 627\n",
      "Slice 627 best combined loss: 5.067189037799835\n",
      "Slice 627 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 628\n",
      "Slice 628 best combined loss: 4.368541419506073\n",
      "Slice 628 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 629\n",
      "Slice 629 best combined loss: 4.868857979774475\n",
      "Slice 629 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 630\n",
      "Slice 630 best combined loss: 4.440786808729172\n",
      "Slice 630 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 631\n",
      "Slice 631 best combined loss: 5.040605038404465\n",
      "Slice 631 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 632\n",
      "Slice 632 best combined loss: 4.643271028995514\n",
      "Slice 632 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 633\n",
      "Slice 633 best combined loss: 4.930388033390045\n",
      "Slice 633 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 634\n",
      "Slice 634 best combined loss: 4.988526463508606\n",
      "Slice 634 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 635\n",
      "Slice 635 best combined loss: 4.905695974826813\n",
      "Slice 635 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 636\n",
      "Slice 636 best combined loss: 4.825397849082947\n",
      "Slice 636 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 637\n",
      "Slice 637 best combined loss: 4.058681920170784\n",
      "Slice 637 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 638\n",
      "Slice 638 best combined loss: 4.964100897312164\n",
      "Slice 638 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 639\n",
      "Slice 639 best combined loss: 5.070651113986969\n",
      "Slice 639 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 640\n",
      "Slice 640 best combined loss: 4.779313087463379\n",
      "Slice 640 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 641\n",
      "Slice 641 best combined loss: 4.681730538606644\n",
      "Slice 641 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 642\n",
      "Slice 642 best combined loss: 4.568583577871323\n",
      "Slice 642 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 643\n",
      "Slice 643 best combined loss: 5.011034905910492\n",
      "Slice 643 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 644\n",
      "Slice 644 best combined loss: 4.605144530534744\n",
      "Slice 644 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 645\n",
      "Slice 645 best combined loss: 4.6802390813827515\n",
      "Slice 645 best test accuracy: 83.87% \n",
      "\n",
      "Slice: 646\n",
      "Slice 646 best combined loss: 4.95684540271759\n",
      "Slice 646 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 647\n",
      "Slice 647 best combined loss: 5.030420005321503\n",
      "Slice 647 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 648\n",
      "Slice 648 best combined loss: 4.828748345375061\n",
      "Slice 648 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 649\n",
      "Slice 649 best combined loss: 4.966226935386658\n",
      "Slice 649 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 650\n",
      "Slice 650 best combined loss: 5.052685737609863\n",
      "Slice 650 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 651\n",
      "Slice 651 best combined loss: 4.491018131375313\n",
      "Slice 651 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 652\n",
      "Slice 652 best combined loss: 4.882868766784668\n",
      "Slice 652 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 653\n",
      "Slice 653 best combined loss: 4.1755611300468445\n",
      "Slice 653 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 654\n",
      "Slice 654 best combined loss: 5.067197144031525\n",
      "Slice 654 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 655\n",
      "Slice 655 best combined loss: 4.273870497941971\n",
      "Slice 655 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 656\n",
      "Slice 656 best combined loss: 4.739619344472885\n",
      "Slice 656 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 657\n",
      "Slice 657 best combined loss: 5.009881019592285\n",
      "Slice 657 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 658\n",
      "Slice 658 best combined loss: 5.025715470314026\n",
      "Slice 658 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 659\n",
      "Slice 659 best combined loss: 4.726558655500412\n",
      "Slice 659 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 660\n",
      "Slice 660 best combined loss: 5.1523131132125854\n",
      "Slice 660 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 661\n",
      "Slice 661 best combined loss: 4.794038861989975\n",
      "Slice 661 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 662\n",
      "Slice 662 best combined loss: 4.837683290243149\n",
      "Slice 662 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 663\n",
      "Slice 663 best combined loss: 4.809663891792297\n",
      "Slice 663 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 664\n",
      "Slice 664 best combined loss: 4.764307051897049\n",
      "Slice 664 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 665\n",
      "Slice 665 best combined loss: 4.860129952430725\n",
      "Slice 665 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 666\n",
      "Slice 666 best combined loss: 4.609903395175934\n",
      "Slice 666 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 667\n",
      "Slice 667 best combined loss: 4.469342201948166\n",
      "Slice 667 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 668\n",
      "Slice 668 best combined loss: 5.051996409893036\n",
      "Slice 668 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 669\n",
      "Slice 669 best combined loss: 4.610434263944626\n",
      "Slice 669 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 670\n",
      "Slice 670 best combined loss: 5.0059592723846436\n",
      "Slice 670 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 671\n",
      "Slice 671 best combined loss: 4.928433835506439\n",
      "Slice 671 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 672\n",
      "Slice 672 best combined loss: 3.776481866836548\n",
      "Slice 672 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 673\n",
      "Slice 673 best combined loss: 4.694616734981537\n",
      "Slice 673 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 674\n",
      "Slice 674 best combined loss: 4.799103677272797\n",
      "Slice 674 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 675\n",
      "Slice 675 best combined loss: 4.471239387989044\n",
      "Slice 675 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 676\n",
      "Slice 676 best combined loss: 5.043000161647797\n",
      "Slice 676 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 677\n",
      "Slice 677 best combined loss: 5.054241389036179\n",
      "Slice 677 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 678\n",
      "Slice 678 best combined loss: 4.82396012544632\n",
      "Slice 678 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 679\n",
      "Slice 679 best combined loss: 4.8544140458106995\n",
      "Slice 679 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 680\n",
      "Slice 680 best combined loss: 5.078344225883484\n",
      "Slice 680 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 681\n",
      "Slice 681 best combined loss: 5.015090882778168\n",
      "Slice 681 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 682\n",
      "Slice 682 best combined loss: 5.0360493659973145\n",
      "Slice 682 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 683\n",
      "Slice 683 best combined loss: 4.791478931903839\n",
      "Slice 683 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 684\n",
      "Slice 684 best combined loss: 4.443723618984222\n",
      "Slice 684 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 685\n",
      "Slice 685 best combined loss: 3.6487580239772797\n",
      "Slice 685 best test accuracy: 83.87% \n",
      "\n",
      "Slice: 686\n",
      "Slice 686 best combined loss: 4.755983829498291\n",
      "Slice 686 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 687\n",
      "Slice 687 best combined loss: 4.932073354721069\n",
      "Slice 687 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 688\n",
      "Slice 688 best combined loss: 3.562100648880005\n",
      "Slice 688 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 689\n",
      "Slice 689 best combined loss: 5.081224173307419\n",
      "Slice 689 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 690\n",
      "Slice 690 best combined loss: 4.748045325279236\n",
      "Slice 690 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 691\n",
      "Slice 691 best combined loss: 4.707341551780701\n",
      "Slice 691 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 692\n",
      "Slice 692 best combined loss: 4.950374826788902\n",
      "Slice 692 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 693\n",
      "Slice 693 best combined loss: 5.0840224623680115\n",
      "Slice 693 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 694\n",
      "Slice 694 best combined loss: 4.956002771854401\n",
      "Slice 694 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 695\n",
      "Slice 695 best combined loss: 4.486974895000458\n",
      "Slice 695 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 696\n",
      "Slice 696 best combined loss: 3.8258619606494904\n",
      "Slice 696 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 697\n",
      "Slice 697 best combined loss: 4.7671569883823395\n",
      "Slice 697 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 698\n",
      "Slice 698 best combined loss: 4.879058539867401\n",
      "Slice 698 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 699\n",
      "Slice 699 best combined loss: 4.897789031267166\n",
      "Slice 699 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 700\n",
      "Slice 700 best combined loss: 4.940107583999634\n",
      "Slice 700 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 701\n",
      "Slice 701 best combined loss: 4.994421094655991\n",
      "Slice 701 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 702\n",
      "Slice 702 best combined loss: 5.074813008308411\n",
      "Slice 702 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 703\n",
      "Slice 703 best combined loss: 4.6347024738788605\n",
      "Slice 703 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 704\n",
      "Slice 704 best combined loss: 4.985208421945572\n",
      "Slice 704 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 705\n",
      "Slice 705 best combined loss: 4.974941849708557\n",
      "Slice 705 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 706\n",
      "Slice 706 best combined loss: 5.037493407726288\n",
      "Slice 706 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 707\n",
      "Slice 707 best combined loss: 4.381367087364197\n",
      "Slice 707 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 708\n",
      "Slice 708 best combined loss: 4.797086477279663\n",
      "Slice 708 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 709\n",
      "Slice 709 best combined loss: 4.92308384180069\n",
      "Slice 709 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 710\n",
      "Slice 710 best combined loss: 4.899906784296036\n",
      "Slice 710 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 711\n",
      "Slice 711 best combined loss: 4.6634461581707\n",
      "Slice 711 best test accuracy: 83.87% \n",
      "\n",
      "Slice: 712\n",
      "Slice 712 best combined loss: 4.9619768261909485\n",
      "Slice 712 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 713\n",
      "Slice 713 best combined loss: 4.709621489048004\n",
      "Slice 713 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 714\n",
      "Slice 714 best combined loss: 4.401970475912094\n",
      "Slice 714 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 715\n",
      "Slice 715 best combined loss: 4.515005603432655\n",
      "Slice 715 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 716\n",
      "Slice 716 best combined loss: 4.616054445505142\n",
      "Slice 716 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 717\n",
      "Slice 717 best combined loss: 5.0654571652412415\n",
      "Slice 717 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 718\n",
      "Slice 718 best combined loss: 5.056149363517761\n",
      "Slice 718 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 719\n",
      "Slice 719 best combined loss: 4.7663945853710175\n",
      "Slice 719 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 720\n",
      "Slice 720 best combined loss: 4.842642962932587\n",
      "Slice 720 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 721\n",
      "Slice 721 best combined loss: 4.9843123853206635\n",
      "Slice 721 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 722\n",
      "Slice 722 best combined loss: 3.94876030087471\n",
      "Slice 722 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 723\n",
      "Slice 723 best combined loss: 4.652084171772003\n",
      "Slice 723 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 724\n",
      "Slice 724 best combined loss: 4.952064871788025\n",
      "Slice 724 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 725\n",
      "Slice 725 best combined loss: 4.174034044146538\n",
      "Slice 725 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 726\n",
      "Slice 726 best combined loss: 4.869913816452026\n",
      "Slice 726 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 727\n",
      "Slice 727 best combined loss: 4.497165381908417\n",
      "Slice 727 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 728\n",
      "Slice 728 best combined loss: 4.793759107589722\n",
      "Slice 728 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 729\n",
      "Slice 729 best combined loss: 4.947442650794983\n",
      "Slice 729 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 730\n",
      "Slice 730 best combined loss: 4.829069256782532\n",
      "Slice 730 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 731\n",
      "Slice 731 best combined loss: 4.938597530126572\n",
      "Slice 731 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 732\n",
      "Slice 732 best combined loss: 4.946907460689545\n",
      "Slice 732 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 733\n",
      "Slice 733 best combined loss: 4.249495148658752\n",
      "Slice 733 best test accuracy: 77.42% \n",
      "\n",
      "Slice: 734\n",
      "Slice 734 best combined loss: 4.98912388086319\n",
      "Slice 734 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 735\n",
      "Slice 735 best combined loss: 4.8830273151397705\n",
      "Slice 735 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 736\n",
      "Slice 736 best combined loss: 5.056157976388931\n",
      "Slice 736 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 737\n",
      "Slice 737 best combined loss: 4.704113066196442\n",
      "Slice 737 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 738\n",
      "Slice 738 best combined loss: 5.003629982471466\n",
      "Slice 738 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 739\n",
      "Slice 739 best combined loss: 4.9055807292461395\n",
      "Slice 739 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 740\n",
      "Slice 740 best combined loss: 4.0153636783361435\n",
      "Slice 740 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 741\n",
      "Slice 741 best combined loss: 5.0327238738536835\n",
      "Slice 741 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 742\n",
      "Slice 742 best combined loss: 5.048885226249695\n",
      "Slice 742 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 743\n",
      "Slice 743 best combined loss: 4.769335687160492\n",
      "Slice 743 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 744\n",
      "Slice 744 best combined loss: 4.495999842882156\n",
      "Slice 744 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 745\n",
      "Slice 745 best combined loss: 4.799087643623352\n",
      "Slice 745 best test accuracy: 58.06% \n",
      "\n",
      "Slice: 746\n",
      "Slice 746 best combined loss: 3.6464253067970276\n",
      "Slice 746 best test accuracy: 80.65% \n",
      "\n",
      "Slice: 747\n",
      "Slice 747 best combined loss: 4.977235436439514\n",
      "Slice 747 best test accuracy: 64.52% \n",
      "\n",
      "Slice: 748\n",
      "Slice 748 best combined loss: 4.9402342438697815\n",
      "Slice 748 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 749\n",
      "Slice 749 best combined loss: 4.512450754642487\n",
      "Slice 749 best test accuracy: 83.87% \n",
      "\n",
      "Slice: 750\n",
      "Slice 750 best combined loss: 5.069744795560837\n",
      "Slice 750 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 751\n",
      "Slice 751 best combined loss: 4.2510726153850555\n",
      "Slice 751 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 752\n",
      "Slice 752 best combined loss: 4.260410904884338\n",
      "Slice 752 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 753\n",
      "Slice 753 best combined loss: 5.029411047697067\n",
      "Slice 753 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 754\n",
      "Slice 754 best combined loss: 4.693734586238861\n",
      "Slice 754 best test accuracy: 74.19% \n",
      "\n",
      "Slice: 755\n",
      "Slice 755 best combined loss: 5.068959832191467\n",
      "Slice 755 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 756\n",
      "Slice 756 best combined loss: 5.080641567707062\n",
      "Slice 756 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 757\n",
      "Slice 757 best combined loss: 4.719077855348587\n",
      "Slice 757 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 758\n",
      "Slice 758 best combined loss: 4.31625559926033\n",
      "Slice 758 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 759\n",
      "Slice 759 best combined loss: 4.507453888654709\n",
      "Slice 759 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 760\n",
      "Slice 760 best combined loss: 4.256862089037895\n",
      "Slice 760 best test accuracy: 70.97% \n",
      "\n",
      "Slice: 761\n",
      "Slice 761 best combined loss: 4.9079993069171906\n",
      "Slice 761 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 762\n",
      "Slice 762 best combined loss: 5.025050401687622\n",
      "Slice 762 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 763\n",
      "Slice 763 best combined loss: 5.034495770931244\n",
      "Slice 763 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 764\n",
      "Slice 764 best combined loss: 5.050645172595978\n",
      "Slice 764 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 765\n",
      "Slice 765 best combined loss: 5.04975962638855\n",
      "Slice 765 best test accuracy: 61.29% \n",
      "\n",
      "Slice: 766\n",
      "Slice 766 best combined loss: 4.339368790388107\n",
      "Slice 766 best test accuracy: 67.74% \n",
      "\n",
      "Slice: 767\n",
      "Slice 767 best combined loss: 4.674237757921219\n",
      "Slice 767 best test accuracy: 61.29% \n",
      "\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# go through all the slices of the dataframe and train and evaluate the model\n",
    "\n",
    "all_test_accs = []\n",
    "all_combined_losses = []\n",
    "\n",
    "for slice in range(768):\n",
    "    print(f'Slice: {slice}')\n",
    "    # slice = 0\n",
    "\n",
    "    # create one column slice of the dataframe\n",
    "    df_train_slice = pd.DataFrame({\n",
    "        # 'word': df_train['word'],\n",
    "        'embedding': df_train[str(slice)],\n",
    "        'label': df_train['label']\n",
    "    })\n",
    "\n",
    "    df_test_slice = pd.DataFrame({\n",
    "        # 'word': df_test['word'],\n",
    "        'embedding': df_test[str(slice)],\n",
    "        'label': df_test['label']\n",
    "    })\n",
    "\n",
    "    # print(df_train_slice[:5])\n",
    "    # print(df_train_slice[-5:])\n",
    "\n",
    "    train_slice_loader, test_slice_loader = create_dataloaders(df_train_slice, df_test_slice)\n",
    "\n",
    "    # take the best of 3 runs\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for i in range(10):\n",
    "        net, best_combined_loss = train_model(train_slice_loader)\n",
    "\n",
    "        train_acc = check_accuracy(train_slice_loader, net)\n",
    "        test_acc = check_accuracy(test_slice_loader, net)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            # best_train_acc = train_acc\n",
    "            # best_net = net\n",
    "    \n",
    "    all_test_accs.append(best_test_acc)\n",
    "    all_combined_losses.append(best_combined_loss)\n",
    "\n",
    "    # print(f'Accuracy on training set: {check_accuracy(train_loader, net)*100:.2f}%')\n",
    "    # print(f'Accuracy on test set: {check_accuracy(test_loader, net)*100:.2f}%')\n",
    "\n",
    "    print(f'Slice {slice} best combined loss: {best_combined_loss}')\n",
    "    print(f'Slice {slice} best test accuracy: {best_test_acc*100:.2f}% \\n')\n",
    "\n",
    "print(len(all_test_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.6452), tensor(0.7097), tensor(0.6452), tensor(0.6452), tensor(0.6129), tensor(0.6129), tensor(0.6129), tensor(0.6774), tensor(0.6774), tensor(0.6129)]\n"
     ]
    }
   ],
   "source": [
    "print(all_test_accs[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6451612710952759, 0.7096773982048035, 0.6451612710952759, 0.6451612710952759, 0.6129032373428345, 0.6129032373428345, 0.6129032373428345, 0.6774193644523621, 0.6774193644523621, 0.6129032373428345]\n"
     ]
    }
   ],
   "source": [
    "# convert list of tensors into list of floats\n",
    "all_test_accs_val = [acc.item() for acc in all_test_accs]\n",
    "\n",
    "print(all_test_accs_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_accs_val = np.array(all_test_accs_val)\n",
    "all_combined_losses = np.array(all_combined_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57 511 685 749 578 645 711 562 588 368]\n",
      "[0.80645162 0.80645162 0.83870965 0.83870965 0.83870965 0.83870965\n",
      " 0.83870965 0.83870965 0.87096775 0.87096775]\n",
      "[4.8475219  4.41397128 3.64875802 4.51245075 4.13233162 4.68023908\n",
      " 4.66344616 4.57590418 3.90372416 4.17510045]\n"
     ]
    }
   ],
   "source": [
    "# find the indices of the top 10 slices\n",
    "top_10_indices = np.argsort(all_test_accs_val)[-10:]\n",
    "print(top_10_indices)\n",
    "print(all_test_accs_val[top_10_indices])\n",
    "print(all_combined_losses[top_10_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy      loss\n",
      "0  0.645161  4.771129\n",
      "1  0.709677  4.871202\n",
      "2  0.645161  4.939927\n",
      "3  0.645161  4.792910\n",
      "4  0.612903  4.944125\n",
      "5  0.612903  5.047808\n",
      "6  0.612903  4.953889\n",
      "7  0.677419  4.838976\n",
      "8  0.677419  4.759429\n",
      "9  0.612903  4.956267\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe with accuracy and loss\n",
    "df_acc_loss = pd.DataFrame({\n",
    "    'accuracy': all_test_accs_val,\n",
    "    'loss': all_combined_losses\n",
    "})\n",
    "\n",
    "print(df_acc_loss[:10])\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df_acc_loss.to_csv('llm-outputs/color_bruteforce_acc_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  embedding  label\n",
      "0      plum   0.352192      1\n",
      "1     umber   0.197500      1\n",
      "2   emerald   0.047766      1\n",
      "3  lavender  -0.188735      1\n",
      "4     sepia  -0.318106      1\n"
     ]
    }
   ],
   "source": [
    "df_368 = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['368'],\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "print(df_368[:5])\n",
    "\n",
    "df_368.to_csv('llm-outputs/color_bruteforce_368.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  embedding  label\n",
      "0      plum   0.073558      1\n",
      "1     umber  -0.220961      1\n",
      "2   emerald  -0.171672      1\n",
      "3  lavender   0.025043      1\n",
      "4     sepia   0.263970      1\n"
     ]
    }
   ],
   "source": [
    "df_588 = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['588'],\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "print(df_588[:5])\n",
    "\n",
    "df_588.to_csv('llm-outputs/color_bruteforce_588.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axis 588 seems to be doing a very good job discerning between colors and non-colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  embedding  label\n",
      "0      plum   0.073558      1\n",
      "1     umber  -0.220961      1\n",
      "2   emerald  -0.171672      1\n",
      "3  lavender   0.025043      1\n",
      "4     sepia   0.263970      1\n",
      "      word  embedding  label\n",
      "42  bridge   0.599472      0\n",
      "43  friend   0.424847      0\n",
      "44   doubt   0.525485      0\n",
      "45  mirror   0.221172      0\n",
      "46   adult   0.470498      0\n"
     ]
    }
   ],
   "source": [
    "# separate the color and non-color embeddings\n",
    "df_588_color = df_588[df_588['label'] == 1]\n",
    "df_588_non_color = df_588[df_588['label'] == 0]\n",
    "\n",
    "# print(df_588_color[:5])\n",
    "# print(df_588_non_color[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw a histogram of the embeddings, with the color words in red and the non-color words in blue\n",
    "plt.hist(df_588_color['embedding'], bins=30, color='red', alpha=0.5)\n",
    "plt.hist(df_588_non_color['embedding'], bins=30, color='blue', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGdCAYAAAArNcgqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAadUlEQVR4nO3de5CVdf3A8c/KugfTXfCGgqBpJoqKmgShmaiUkTpWTjlqRUyjToOZUo1tWSzdFhtLrWHQSLN/DLUZyjEvUyaWcgkpZzAvieKwIpeU2gW0g8D398fv5/5cBeI5u+e7y9nXa+b54zw8zz6f892FfXMuu3UppRQAABnt0dsDAAD9jwABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDs6nNfcNu2bfHyyy9HY2Nj1NXV5b48AFCBlFJs2LAhhg0bFnvs0f3HL7IHyMsvvxwjRozIfVkAoAe0tbXF8OHDu/1xsgdIY2NjRPzvHWhqasp9eQCgAh0dHTFixIjO7+PdlT1A3nzapampSYAAwG6mp14+4UWoAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMiucICsWrUqPvOZz8T+++8fe+21Vxx//PHx+OOPV2M2AKBGFfpdMP/617/i1FNPjTPOOCPuv//+OPDAA+O5556Lfffdt1rzAQA1qFCAXHfddTFixIj4xS9+0bnv8MMP7/GhAIDaVugpmHvuuSfGjBkTn/rUp2LIkCFx0kknxZw5c3Z6Trlcjo6Oji4bANC/FXoE5IUXXojZs2fHtGnT4hvf+EYsWbIkrrzyymhoaIjJkydv95zW1taYMWNGjwwLsDMtLXnOAbqvLqWUdvXghoaGGDNmTCxYsKBz35VXXhlLliyJhQsXbveccrkc5XK583ZHR0eMGDEi2tvbo6mpqRujA3QlQKB6Ojo6YtCgQT32/bvQUzBDhw6NUaNGddl3zDHHxMqVK3d4TqlUiqampi4bANC/FQqQU089NZ599tku+/7xj3/EYYcd1qNDAQC1rVCAXH311bFo0aL4wQ9+EMuXL4877rgjfvazn8XUqVOrNR8AUIMKBcj73//+mDdvXvzqV7+K4447Lr773e/GjTfeGJdcckm15gMAalChd8FERJx77rlx7rnnVmMWAKCf8LtgAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZFcoQFpaWqKurq7LdvTRR1drNgCgRtUXPeHYY4+NP/zhD///AeoLfwgAoJ8rXA/19fVx8MEHV2MWAKCfKPwakOeeey6GDRsWRxxxRFxyySWxcuXKnR5fLpejo6OjywYA9G91KaW0qwfff//9sXHjxhg5cmSsXr06ZsyYEatWrYonn3wyGhsbt3tOS0tLzJgx4x3729vbo6mpqfLJYTfS0tJ3z+mL16hUrtl6ZQ2KXrQvf6LYLXV0dMSgQYN67Pt3oUdAJk2aFJ/61Kdi9OjRcfbZZ8d9990X//73v+Ouu+7a4TnNzc3R3t7eubW1tXV7aABg99atV5AOHjw4jjrqqFi+fPkOjymVSlEqlbpzGQCgxnTr54Bs3Lgxnn/++Rg6dGhPzQMA9AOFAuSrX/1qPPLII/Hiiy/GggUL4hOf+EQMGDAgLrroomrNBwDUoEJPwbz00ktx0UUXxauvvhoHHnhgfPCDH4xFixbFgQceWK35AIAaVChA5s6dW605AIB+xO+CAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsuhUgM2fOjLq6urjqqqt6aBwAoD+oOECWLFkSt9xyS4wePbon5wEA+oGKAmTjxo1xySWXxJw5c2Lfffft6ZkAgBpXUYBMnTo1zjnnnJg4cWJPzwMA9AP1RU+YO3du/PWvf40lS5bs0vHlcjnK5XLn7Y6OjqKXBABqTKEAaWtriy9/+cvx+9//PgYOHLhL57S2tsaMGTMqGo5iWlpq6zrwDv/ti2/+hK63J0zY3lG9z18iKPYUzNKlS2PdunXxvve9L+rr66O+vj4eeeSR+MlPfhL19fWxdevWd5zT3Nwc7e3tnVtbW1uPDQ8A7J4KPQJy1llnxbJly7rsmzJlShx99NFxzTXXxIABA95xTqlUilKp1L0pAYCaUihAGhsb47jjjuuyb++9947999//HfsBAHbET0IFALIr/C6Yt5s/f34PjAEA9CceAQEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJBdoQCZPXt2jB49OpqamqKpqSnGjx8f999/f7VmAwBqVKEAGT58eMycOTOWLl0ajz/+eJx55plx/vnnx9///vdqzQcA1KD6Igefd955XW5///vfj9mzZ8eiRYvi2GOP7dHBAIDaVShA3mrr1q1x9913x6ZNm2L8+PE7PK5cLke5XO683dHRUeklAYAaUThAli1bFuPHj4///Oc/sc8++8S8efNi1KhROzy+tbU1ZsyY0a0h6Z9aWnp7gt7VV+9/JXP11ftCL/AFxP8p/C6YkSNHxhNPPBGLFy+OL37xizF58uR46qmndnh8c3NztLe3d25tbW3dGhgA2P0VfgSkoaEhjjzyyIiIOPnkk2PJkiVx0003xS233LLd40ulUpRKpe5NCQDUlG7/HJBt27Z1eY0HAMB/U+gRkObm5pg0aVIceuihsWHDhrjjjjti/vz58eCDD1ZrPgCgBhUKkHXr1sXnPve5WL16dQwaNChGjx4dDz74YHz4wx+u1nwAQA0qFCC33nprteYAAPoRvwsGAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdoUCpLW1Nd7//vdHY2NjDBkyJD7+8Y/Hs88+W63ZAIAaVShAHnnkkZg6dWosWrQofv/738cbb7wRH/nIR2LTpk3Vmg8AqEH1RQ5+4IEHuty+/fbbY8iQIbF06dL40Ic+1KODAQC1q1CAvF17e3tEROy33347PKZcLke5XO683dHR0Z1LAgA1oOIA2bZtW1x11VVx6qmnxnHHHbfD41pbW2PGjBmVXqaQlpa+ew67mfnz81xnwoRix1cyV9FrVMF2/878l/vSMmHnf75LdtP1yqnL52YX16vL5ybHP4i1cg26qPhdMFOnTo0nn3wy5s6du9Pjmpubo729vXNra2ur9JIAQI2o6BGQK664Iu69997405/+FMOHD9/psaVSKUqlUkXDAQC1qVCApJTiS1/6UsybNy/mz58fhx9+eLXmAgBqWKEAmTp1atxxxx3x29/+NhobG2PNmjURETFo0KDYa6+9qjIgAFB7Cr0GZPbs2dHe3h4TJkyIoUOHdm533nlnteYDAGpQ4adgAAC6y++CAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsCgfIn/70pzjvvPNi2LBhUVdXF7/5zW+qMBYAUMsKB8imTZvihBNOiFmzZlVjHgCgH6gvesKkSZNi0qRJ1ZgFAOgnvAYEAMiu8CMgRZXL5SiXy523Ozo6qn1JAKCPq3qAtLa2xowZM6p9mYq1tPT2BJnMn1/8nAkTeuzyO1znXp6r11Vy/3Nco+gaV+F+tMwvOENP2ZX70rILx3Q5fMLb9rz9ds9omTD/LTdadu2kCta5y/3Zxct053q7qsv976tyfNPZTb6xVf0pmObm5mhvb+/c2traqn1JAKCPq/ojIKVSKUqlUrUvAwDsRgoHyMaNG2P58uWdt1esWBFPPPFE7LfffnHooYf26HAAQG0qHCCPP/54nHHGGZ23p02bFhERkydPjttvv73HBgMAalfhAJkwYUKklKoxCwDQT/g5IABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALITIABAdgIEAMhOgAAA2QkQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7AQIAJCdAAEAshMgAEB2AgQAyE6AAADZCRAAIDsBAgBkJ0AAgOwECACQnQABALKrKEBmzZoV7373u2PgwIExbty4+Mtf/tLTcwEANaxwgNx5550xbdq0mD59evz1r3+NE044Ic4+++xYt25dNeYDAGpQ4QD58Y9/HJdeemlMmTIlRo0aFTfffHO8613vittuu60a8wEANai+yMGbN2+OpUuXRnNzc+e+PfbYIyZOnBgLFy7c7jnlcjnK5XLn7fb29oiI6OjoqGTenXrLZXi7LZuKn1Pe/ueokk/dDj83PThXj6pkrlpSdI372Xp1FPzHppxpfYrOFdEDs/Whr5VK7v//n5zh35WIPN+oqnRf3vy+nVLqmQ+YCli1alWKiLRgwYIu+7/2ta+lsWPHbvec6dOnp4iw2Ww2m81WA1tbW1uRdNihQo+AVKK5uTmmTZvWeXvbtm2xfv362H///aOurq7al4+Ojo4YMWJEtLW1RVNTU9WvVwusWWWsW2WsW2WsW3HWrDJvrtvKlSujrq4uhg0b1iMft1CAHHDAATFgwIBYu3Ztl/1r166Ngw8+eLvnlEqlKJVKXfYNHjy42JQ9oKmpyRdcQdasMtatMtatMtatOGtWmUGDBvXouhV6EWpDQ0OcfPLJ8dBDD3Xu27ZtWzz00EMxfvz4HhsKAKhthZ+CmTZtWkyePDnGjBkTY8eOjRtvvDE2bdoUU6ZMqcZ8AEANKhwgF154Yfzzn/+Mb3/727FmzZo48cQT44EHHoiDDjqoGvN1W6lUiunTp7/jaSB2zJpVxrpVxrpVxroVZ80qU611q0upp95PAwCwa/wuGAAgOwECAGQnQACA7AQIAJBdzQXI+vXr45JLLommpqYYPHhwfOELX4iNGzf+1/MWLlwYZ555Zuy9997R1NQUH/rQh+L111/PMHHfUOm6RUSklGLSpElRV1cXv/nNb6o7aB9TdN3Wr18fX/rSl2LkyJGx1157xaGHHhpXXnll5+9IqlWzZs2Kd7/73TFw4MAYN25c/OUvf9np8XfffXccffTRMXDgwDj++OPjvvvuyzRp31Jk3ebMmROnnXZa7LvvvrHvvvvGxIkT/+s616KiX2tvmjt3btTV1cXHP/7x6g7YRxVdt3//+98xderUGDp0aJRKpTjqqKOK/z3tkR/o3od89KMfTSeccEJatGhR+vOf/5yOPPLIdNFFF+30nAULFqSmpqbU2tqannzyyfTMM8+kO++8M/3nP//JNHXvq2Td3vTjH/84TZo0KUVEmjdvXnUH7WOKrtuyZcvSJz/5yXTPPfek5cuXp4ceeii9973vTRdccEHGqfOaO3duamhoSLfddlv6+9//ni699NI0ePDgtHbt2u0e/9hjj6UBAwakH/7wh+mpp55K1157bdpzzz3TsmXLMk/eu4qu28UXX5xmzZqV/va3v6Wnn346ff7zn0+DBg1KL730UubJe0/RNXvTihUr0iGHHJJOO+20dP755+cZtg8pum7lcjmNGTMmfexjH0uPPvpoWrFiRZo/f3564oknCl23pgLkqaeeShGRlixZ0rnv/vvvT3V1dWnVqlU7PG/cuHHp2muvzTFin1TpuqWU0t/+9rd0yCGHpNWrV/e7AOnOur3VXXfdlRoaGtIbb7xRjTF73dixY9PUqVM7b2/dujUNGzYstba2bvf4T3/60+mcc87psm/cuHHp8ssvr+qcfU3RdXu7LVu2pMbGxvTLX/6yWiP2OZWs2ZYtW9Ipp5ySfv7zn6fJkyf3ywApum6zZ89ORxxxRNq8eXO3rltTT8EsXLgwBg8eHGPGjOncN3HixNhjjz1i8eLF2z1n3bp1sXjx4hgyZEiccsopcdBBB8Xpp58ejz76aK6xe10l6xYR8dprr8XFF18cs2bN2uHvAqplla7b27W3t0dTU1PU11f9d0Nmt3nz5li6dGlMnDixc98ee+wREydOjIULF273nIULF3Y5PiLi7LPP3uHxtaiSdXu71157Ld54443Yb7/9qjVmn1Lpmn3nO9+JIUOGxBe+8IUcY/Y5lazbPffcE+PHj4+pU6fGQQcdFMcdd1z84Ac/iK1btxa6dk0FyJo1a2LIkCFd9tXX18d+++0Xa9as2e45L7zwQkREtLS0xKWXXhoPPPBAvO9974uzzjornnvuuarP3BdUsm4REVdffXWccsopcf7551d7xD6p0nV7q1deeSW++93vxmWXXVaNEXvdK6+8Elu3bn3HT0o+6KCDdrhGa9asKXR8Lapk3d7ummuuiWHDhr0j5mpVJWv26KOPxq233hpz5szJMWKfVMm6vfDCC/HrX/86tm7dGvfdd19861vfih/96Efxve99r9C1d4sA+frXvx51dXU73Z555pmKPva2bdsiIuLyyy+PKVOmxEknnRQ33HBDjBw5Mm677baevBvZVXPd7rnnnvjjH/8YN954Y88O3QdUc93eqqOjI84555wYNWpUtLS0dH9w+D8zZ86MuXPnxrx582LgwIG9PU6ftGHDhvjsZz8bc+bMiQMOOKC3x9mtbNu2LYYMGRI/+9nP4uSTT44LL7wwvvnNb8bNN99c6OPsFo/5fuUrX4nPf/7zOz3miCOOiIMPPjjWrVvXZf+WLVti/fr1O3yKYOjQoRERMWrUqC77jznmmFi5cmXlQ/cB1Vy3P/7xj/H888/H4MGDu+y/4IIL4rTTTov58+d3Y/LeVc11e9OGDRviox/9aDQ2Nsa8efNizz337O7YfdIBBxwQAwYMiLVr13bZv3bt2h2u0cEHH1zo+FpUybq96frrr4+ZM2fGH/7whxg9enQ1x+xTiq7Z888/Hy+++GKcd955nfve/A9pfX19PPvss/Ge97ynukP3AZV8rQ0dOjT23HPPGDBgQOe+Y445JtasWRObN2+OhoaGXbt4t15B0se8+aLAxx9/vHPfgw8+uNMXBW7bti0NGzbsHS9CPfHEE1Nzc3NV5+0rKlm31atXp2XLlnXZIiLddNNN6YUXXsg1eq+qZN1SSqm9vT194AMfSKeffnratGlTjlF71dixY9MVV1zReXvr1q3pkEMO2emLUM8999wu+8aPH98vX4RaZN1SSum6665LTU1NaeHChTlG7HOKrNnrr7/+jn/Dzj///HTmmWemZcuWpXK5nHP0XlX0a625uTkddthhaevWrZ37brzxxjR06NBC162pAEnpf98WedJJJ6XFixenRx99NL33ve/t8rbIl156KY0cOTItXry4c98NN9yQmpqa0t13352ee+65dO2116aBAwem5cuX98Zd6BWVrNvbRT97F0xKxdetvb09jRs3Lh1//PFp+fLlafXq1Z3bli1beutuVNXcuXNTqVRKt99+e3rqqafSZZddlgYPHpzWrFmTUkrps5/9bPr617/eefxjjz2W6uvr0/XXX5+efvrpNH369H77Ntwi6zZz5szU0NCQfv3rX3f5utqwYUNv3YXsiq7Z2/XXd8EUXbeVK1emxsbGdMUVV6Rnn3023XvvvWnIkCHpe9/7XqHr1lyAvPrqq+miiy5K++yzT2pqakpTpkzp8hdwxYoVKSLSww8/3OW81tbWNHz48PSud70rjR8/Pv35z3/OPHnvqnTd3qo/BkjRdXv44YdTRGx3W7FiRe/ciQx++tOfpkMPPTQ1NDSksWPHpkWLFnX+2emnn54mT57c5fi77rorHXXUUamhoSEde+yx6Xe/+13mifuGIut22GGHbffravr06fkH70VFv9beqr8GSErF123BggVp3LhxqVQqpSOOOCJ9//vfL/yfqLqUUtq1J2sAAHrGbvEuGACgtggQACA7AQIAZCdAAIDsBAgAkJ0AAQCyEyAAQHYCBADIToAAANkJEAAgOwECAGQnQACA7P4HX1mfIBcdpBAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# separate the color and non-color embeddings\n",
    "df_368_color = df_368[df_368['label'] == 1]\n",
    "df_368_non_color = df_368[df_368['label'] == 0]\n",
    "\n",
    "# draw a histogram of the embeddings, with the color words in red and the non-color words in blue\n",
    "plt.hist(df_368_color['embedding'], bins=30, color='red', alpha=0.5)\n",
    "plt.hist(df_368_non_color['embedding'], bins=30, color='blue', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index     color  non_color      diff\n",
      "0     57 -0.068853   0.083865  0.152718\n",
      "1    511  0.096352  -0.060618  0.156970\n",
      "2    685  0.078926  -0.295794  0.374720\n",
      "3    749  0.237888   0.044149  0.193739\n",
      "4    578  0.011691  -0.330732  0.342423\n",
      "5    645 -0.215030  -0.362590  0.147561\n",
      "6    711  0.038971  -0.076753  0.115724\n",
      "7    562  0.252882   0.371339  0.118457\n",
      "8    588  0.073687   0.495521  0.421835\n",
      "9    368  0.109949  -0.148048  0.257997\n"
     ]
    }
   ],
   "source": [
    "# for each of the top 10 slices,\n",
    "# average the embeddings of the color words and the non-color words\n",
    "\n",
    "color_emb_avg = []\n",
    "non_color_emb_avg = []\n",
    "\n",
    "for i in top_10_indices:\n",
    "    df_slice = pd.DataFrame({\n",
    "        # 'word': df_train['word'],\n",
    "        'embedding': df_train[str(i)],\n",
    "        'label': df_train['label']\n",
    "    })\n",
    "\n",
    "    df_slice_color = df_slice[df_slice['label'] == 1]\n",
    "    df_slice_non_color = df_slice[df_slice['label'] == 0]\n",
    "\n",
    "    color_emb_avg.append(np.mean(df_slice_color['embedding']))\n",
    "    non_color_emb_avg.append(np.mean(df_slice_non_color['embedding']))\n",
    "\n",
    "# create a dataframe with the average embeddings\n",
    "df_avg = pd.DataFrame({\n",
    "    'index': top_10_indices,\n",
    "    'color': color_emb_avg,\n",
    "    'non_color': non_color_emb_avg,\n",
    "    'diff': abs(np.array(color_emb_avg) - np.array(non_color_emb_avg))\n",
    "})\n",
    "\n",
    "print(df_avg)\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df_avg.to_csv('llm-outputs/color_bruteforce_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_axis_hist(slice):\n",
    "    df_slice = pd.DataFrame({\n",
    "        # 'word': df_train['word'],\n",
    "        'embedding': df_train[str(slice)],\n",
    "        'label': df_train['label']\n",
    "    })\n",
    "\n",
    "    df_slice_color = df_slice[df_slice['label'] == 1]\n",
    "    df_slice_non_color = df_slice[df_slice['label'] == 0]\n",
    "\n",
    "    # draw a histogram of the embeddings, with the color words in red and the non-color words in blue\n",
    "    plt.hist(df_slice_color['embedding'], bins=30, color='red', alpha=0.5)\n",
    "    plt.hist(df_slice_non_color['embedding'], bins=30, color='blue', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_axis_hist(685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_axis_hist(578)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6861",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
