{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple torch model with 1 fully connected layer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    \"apple\", \"book\", \"car\", \"dog\", \"cat\", \"house\", \"tree\", \"friend\", \"time\", \"money\",\n",
    "    \"heart\", \"sun\", \"moon\", \"sky\", \"water\", \"fire\", \"earth\", \"flower\", \"city\", \"music\",\n",
    "    \"child\", \"parent\", \"school\", \"job\", \"love\", \"smile\", \"day\", \"night\", \"star\", \"cloud\",\n",
    "    \"bird\", \"fish\", \"food\", \"computer\", \"phone\", \"internet\", \"coffee\", \"tea\", \"shoes\", \"hat\",\n",
    "    \"dream\", \"goal\", \"team\", \"game\", \"hope\", \"fear\", \"joy\", \"anger\", \"peace\", \"war\",\n",
    "    \"friendship\", \"family\", \"health\", \"beauty\", \"knowledge\", \"power\", \"nature\", \"history\", \"science\",\n",
    "    \"art\", \"happiness\", \"sadness\", \"color\", \"mind\", \"body\", \"soul\", \"memory\", \"experience\", \"idea\",\n",
    "    \"faith\", \"truth\", \"lie\", \"problem\", \"solution\", \"question\", \"answer\", \"light\", \"darkness\", \"wind\",\n",
    "    \"rain\", \"snow\", \"smell\", \"taste\", \"touch\", \"sound\", \"silence\", \"joy\", \"freedom\", \"future\",\n",
    "    \"past\", \"present\", \"purpose\", \"journey\", \"adventure\", \"discovery\", \"challenge\", \"victory\", \"defeat\"\n",
    "]\n",
    "\n",
    "most_common_words = []\n",
    "\n",
    "# remove duplicates\n",
    "words = list(set(words))\n",
    "# most_common_words = list(set(most_common_words))\n",
    "\n",
    "print(len(words))\n",
    "print(len(most_common_words))\n",
    "\n",
    "assert len(list(set(words + most_common_words))) == len(words) + len(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "non_words = [\n",
    "    \"run\", \"jump\", \"eat\", \"sleep\", \"think\", \"happy\", \"fast\", \"slow\", \"beautiful\", \"smart\",\n",
    "    \"quickly\", \"easily\", \"always\", \"never\", \"under\", \"over\", \"beside\", \"between\", \"through\", \"with\",\n",
    "    \"and\", \"but\", \"or\", \"because\", \"although\", \"well\", \"yes\", \"no\", \"oh\", \"wow\", \"ouch\",\n",
    "    \"go\", \"come\", \"stop\", \"start\", \"win\", \"lose\", \"hot\", \"cold\", \"loud\", \"soft\",\n",
    "    \"bright\", \"dark\", \"high\", \"low\", \"near\", \"far\", \"big\", \"small\", \"old\", \"new\",\n",
    "    \"first\", \"last\", \"next\", \"previous\", \"good\", \"bad\", \"happy\", \"sad\", \"rich\", \"poor\",\n",
    "    \"early\", \"late\", \"hard\", \"easy\", \"simple\", \"complex\", \"right\", \"wrong\", \"true\", \"false\",\n",
    "    \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"up\", \"down\", \"here\", \"there\",\n",
    "    \"now\", \"then\", \"today\", \"tomorrow\", \"yesterday\", \"soon\", \"later\", \"before\", \"after\", \"while\",\n",
    "    \"once\", \"twice\", \"thrice\", \"nevertheless\", \"however\", \"suddenly\", \"finally\", \"quickly\", \"slowly\", \"nowadays\",\n",
    "    \"some\", \"many\", \"few\", \"most\", \"none\", \"all\", \"each\", \"every\", \"somebody\", \"nobody\",\n",
    "    \"something\", \"nothing\", \"somewhere\", \"nowhere\", \"anywhere\", \"everywhere\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"which\", \"what\", \"who\", \"whom\", \"whose\", \"where\", \"when\", \"why\", \"how\", \"whether\",\n",
    "    \"if\", \"unless\", \"because\", \"since\", \"although\", \"though\", \"while\", \"before\", \"after\", \"when\",\n",
    "    \"and\", \"or\", \"but\", \"nor\", \"for\", \"so\", \"yet\", \"either\", \"neither\", \"however\"\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "non_words = list(set(non_words))\n",
    "\n",
    "print(len(non_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of words and returns a list of embeddings\n",
    "def get_embeddings(words):\n",
    "    embeddings = []\n",
    "\n",
    "    for word in words:\n",
    "        input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())\n",
    "    \n",
    "    assert len(embeddings) == len(words)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings(words)\n",
    "most_common_embeddings = get_embeddings(most_common_words)\n",
    "non_embeddings = get_embeddings(non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                          embedding  label\n",
      "0    job  [0.14350789785385132, -0.15940922498703003, -0...      1\n",
      "1   body  [0.7147403359413147, -0.1555342972278595, 0.31...      1\n",
      "2  faith  [0.6398938298225403, 0.13858512043952942, -0.1...      1\n",
      "3  earth  [0.26693275570869446, 0.4444328844547272, -0.1...      1\n",
      "4  water  [0.2036658525466919, 0.2685178220272064, -0.28...      1\n",
      "          word                                          embedding  label\n",
      "181   somebody  [-0.007474187761545181, -0.06840874254703522, ...      0\n",
      "182         in  [-0.8657609224319458, -0.5856645703315735, -0....      0\n",
      "183         on  [-0.11932921409606934, -0.3286181688308716, -0...      0\n",
      "184      later  [-0.2563585937023163, -0.453279972076416, -0.1...      0\n",
      "185  something  [-0.15402115881443024, -0.17753523588180542, -...      0\n",
      "     word                                          embedding  label\n",
      "0    mind  [0.38985970616340637, 0.27778860926628113, -0....      1\n",
      "1   smile  [-0.11715462803840637, 0.03211095184087753, 0....      1\n",
      "2   color  [0.05371911823749542, -0.42612069845199585, -0...      1\n",
      "3  family  [0.14181819558143616, -0.05798260122537613, -0...      1\n",
      "4  defeat  [-0.08488158136606216, 0.21536926925182343, -0...      1\n",
      "     word                                          embedding  label\n",
      "43  small  [-0.5499357581138611, -0.36507585644721985, -0...      0\n",
      "44   well  [0.4333745837211609, 0.05112234503030777, 0.03...      0\n",
      "45  early  [-0.7313716411590576, -0.021845733746886253, -...      0\n",
      "46   what  [-0.06226750835776329, -0.29066941142082214, -...      0\n",
      "47    nor  [-0.0868954211473465, -0.24122412502765656, -0...      0\n"
     ]
    }
   ],
   "source": [
    "# slice the lists into training and test sets\n",
    "words_train = words[:int(len(words)*0.8)]\n",
    "words_test = words[int(len(words)*0.8):]\n",
    "embeddings_train = embeddings[:int(len(embeddings)*0.8)]\n",
    "embeddings_test = embeddings[int(len(embeddings)*0.8):]\n",
    "\n",
    "most_common_words_train = most_common_words[:int(len(most_common_words)*0.8)]\n",
    "most_common_words_test = most_common_words[int(len(most_common_words)*0.8):]\n",
    "most_common_embeddings_train = most_common_embeddings[:int(len(most_common_embeddings)*0.8)]\n",
    "most_common_embeddings_test = most_common_embeddings[int(len(most_common_embeddings)*0.8):]\n",
    "\n",
    "non_words_train = non_words[:int(len(non_words)*0.8)]\n",
    "non_words_test = non_words[int(len(non_words)*0.8):]\n",
    "non_embeddings_train = non_embeddings[:int(len(non_embeddings)*0.8)]\n",
    "non_embeddings_test = non_embeddings[int(len(non_embeddings)*0.8):]\n",
    "\n",
    "# create a dataframe with the training and test sets\n",
    "df_train = pd.DataFrame({\n",
    "    'word': words_train + most_common_words_train + non_words_train,\n",
    "    'embedding': embeddings_train + most_common_embeddings_train + non_embeddings_train,\n",
    "    'label': [1]*len(words_train) + [1]*len(most_common_words_train) + [0]*len(non_words_train)\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'word': words_test + most_common_words_test + non_words_test,\n",
    "    'embedding': embeddings_test + most_common_embeddings_test + non_embeddings_test,\n",
    "    'label': [1]*len(words_test) + [1]*len(most_common_words_test) + [0]*len(non_words_test)\n",
    "})\n",
    "\n",
    "# shuffle the dataframes\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df_train[:5])\n",
    "print(df_train[-5:])\n",
    "print(df_test[:5])\n",
    "print(df_test[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = torch.tensor(tokenized_texts)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train['embedding'], df_train['label'])\n",
    "test_dataset = CustomDataset(df_test['embedding'], df_test['label'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.5994711021582285\n",
      "Epoch 10, loss=0.051087685860693455\n",
      "Epoch 20, loss=0.02002626455699404\n",
      "Epoch 30, loss=0.01178085602199038\n",
      "Epoch 40, loss=0.007276703487150371\n",
      "Epoch 50, loss=0.0053643395367544144\n",
      "Epoch 60, loss=0.003953498953099673\n",
      "Epoch 70, loss=0.003117616792830328\n",
      "Epoch 80, loss=0.002558116990257986\n",
      "Epoch 90, loss=0.0020333628490334377\n",
      "Epoch 100, loss=0.0017162608904376004\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader)}')\n",
    "        # print(combined_loss / len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of variance over different runs\n",
    "Best so far: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 100.00%\n",
      "Accuracy on test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # get data to GPU if possible\n",
    "            data = batch[0].to(device=device)\n",
    "            targets = batch[1].to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    \n",
    "    return num_correct / num_samples\n",
    "\n",
    "print(f'Accuracy on training set: {check_accuracy(train_loader, net)*100:.2f}%')\n",
    "print(f'Accuracy on test set: {check_accuracy(test_loader, net)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n",
      "(10,)\n",
      "[308 752 381 522 667 701 109 282  41 430]\n",
      "[0.00937229 0.00321044 0.00270355 0.00249884 0.00202032 0.00197234\n",
      " 0.001949   0.00188883 0.0018535  0.00183217]\n",
      "(10,)\n",
      "[601 721 161 333 205 346 708 576 131 446]\n",
      "[0.00233819 0.00205217 0.00204614 0.00201515 0.00188911 0.0017302\n",
      " 0.00169615 0.0016606  0.00159734 0.00159162]\n"
     ]
    }
   ],
   "source": [
    "# Find the most important features of the model\n",
    "# get the gradients of the model\n",
    "net.eval()\n",
    "gradients = []\n",
    "for batch in train_loader:\n",
    "    # get data to GPU if possible\n",
    "    data = batch[0].to(device=device)\n",
    "    targets = batch[1].to(device=device)\n",
    "\n",
    "    # forward\n",
    "    scores = net(data)\n",
    "    loss = criterion(scores, targets)\n",
    "    loss.backward()\n",
    "    gradients.append(net.fc1.weight.grad.detach().cpu().numpy())\n",
    "\n",
    "# average the gradients\n",
    "gradients = np.mean(gradients, axis=0)\n",
    "print(gradients.shape)\n",
    "\n",
    "# print(len(gradients))\n",
    "# print(len(gradients[0]))\n",
    "# print(len(gradients[0][0]))\n",
    "\n",
    "# find the indices of the 10 most important features\n",
    "most_important_gradients_0 = gradients[0].argsort()[-10:][::-1]\n",
    "most_important_gradients_1 = gradients[1].argsort()[-10:][::-1]\n",
    "\n",
    "# print the 10 most important features\n",
    "print(most_important_gradients_0.shape)\n",
    "print(most_important_gradients_0)\n",
    "print(gradients[0][most_important_gradients_0])\n",
    "\n",
    "print(most_important_gradients_1.shape)\n",
    "print(most_important_gradients_1)\n",
    "print(gradients[1][most_important_gradients_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131, 522, 282, 667, 161, 41, 430, 308, 701, 446, 576, 708, 333, 205, 721, 601, 346, 109, 752, 381]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "combined_importance_grad = list(set(list(most_important_gradients_0) + list(most_important_gradients_1)))\n",
    "print(combined_importance_grad)\n",
    "print(len(combined_importance_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[672 628 705 558 633 719 605 446 343 158]\n",
      "[0.42129228 0.37351102 0.3513944  0.3392098  0.3341787  0.32714713\n",
      " 0.32431948 0.3039584  0.29635397 0.29567194]\n",
      "(10,)\n",
      "[442 314 603 489 282 289 630 715 112 751]\n",
      "[0.3931066  0.3693206  0.3682808  0.36532345 0.35652688 0.34640998\n",
      " 0.34001842 0.33912355 0.32210195 0.321611  ]\n"
     ]
    }
   ],
   "source": [
    "# Find the most important features for the model\n",
    "# get the weights of the fully connected layer\n",
    "weights = net.fc1.weight.data.cpu().numpy()\n",
    "\n",
    "# find the indices of the 10 most important features\n",
    "most_important_features_0 = weights[0].argsort()[-10:][::-1]\n",
    "most_important_features_1 = weights[1].argsort()[-10:][::-1]\n",
    "\n",
    "# print the 10 most important features and their weights\n",
    "print(most_important_features_0.shape)\n",
    "print(most_important_features_0)\n",
    "print(weights[0][most_important_features_0])\n",
    "\n",
    "print(most_important_features_1.shape)\n",
    "print(most_important_features_1)\n",
    "print(weights[1][most_important_features_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[282, 158, 672, 289, 558, 314, 442, 446, 705, 715, 719, 343, 603, 605, 489, 751, 112, 628, 630, 633]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "combined_importance = list(set(list(most_important_features_0) + list(most_important_features_1)))\n",
    "print(combined_importance)\n",
    "print(len(combined_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_small, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                          embedding  label\n",
      "0    job  [-0.16747981309890747, 0.47371211647987366, 0....      1\n",
      "1   body  [0.19887007772922516, 0.2644270360469818, 0.04...      1\n",
      "2  faith  [-0.09121094644069672, -0.2395203709602356, 0....      1\n",
      "3  earth  [0.118535615503788, -0.03201966732740402, 0.08...      1\n",
      "4  water  [-0.38860055804252625, 0.08342357724905014, 0....      1\n",
      "          word                                          embedding  label\n",
      "181   somebody  [-0.30281540751457214, 0.24308684468269348, 0....      0\n",
      "182         in  [-0.4305441677570343, 0.04467826336622238, 0.1...      0\n",
      "183         on  [-0.3543335199356079, -0.263896107673645, 0.03...      0\n",
      "184      later  [0.024935398250818253, 0.14524027705192566, 0....      0\n",
      "185  something  [-0.48253750801086426, 0.1732560396194458, -0....      0\n",
      "     word                                          embedding  label\n",
      "0    mind  [0.11320915818214417, 0.18943685293197632, 0.0...      1\n",
      "1   smile  [-0.2812722325325012, 0.3908618986606598, 0.02...      1\n",
      "2   color  [-0.20521026849746704, 0.11825429648160934, 0....      1\n",
      "3  family  [0.41470426321029663, -0.053211621940135956, 0...      1\n",
      "4  defeat  [-0.15126396715641022, -0.07105819135904312, 0...      1\n",
      "     word                                          embedding  label\n",
      "43  small  [0.26924052834510803, 0.03430432826280594, -0....      0\n",
      "44   well  [-0.7078829407691956, 0.333453893661499, 0.119...      0\n",
      "45  early  [-0.08105532079935074, 0.3181267976760864, 0.3...      0\n",
      "46   what  [-0.6702903509140015, 0.024309776723384857, 0....      0\n",
      "47    nor  [-0.37598857283592224, 0.3303776979446411, 0.0...      0\n",
      "\n",
      "Length of reduced embedding:  20\n"
     ]
    }
   ],
   "source": [
    "# create datasets with the 20 most important features\n",
    "df_train_small = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['embedding'].apply(lambda x: [x[i] for i in combined_importance]),\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "df_test_small = pd.DataFrame({\n",
    "    'word': df_test['word'],\n",
    "    'embedding': df_test['embedding'].apply(lambda x: [x[i] for i in combined_importance]),\n",
    "    'label': df_test['label']\n",
    "})\n",
    "\n",
    "print(df_train_small[:5])\n",
    "print(df_train_small[-5:])\n",
    "print(df_test_small[:5])\n",
    "print(df_test_small[-5:])\n",
    "\n",
    "print(\"\\nLength of reduced embedding: \", len(df_train_small['embedding'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset_small = CustomDataset(df_train_small['embedding'], df_train_small['label'])\n",
    "test_dataset_small = CustomDataset(df_test_small['embedding'], df_test_small['label'])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader_small = DataLoader(train_dataset_small, batch_size=16, shuffle=True)\n",
    "test_loader_small = DataLoader(test_dataset_small, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.682785709698995\n",
      "Epoch 10, loss=0.6843173752228419\n",
      "Epoch 20, loss=0.6837831139564514\n",
      "Epoch 30, loss=0.6835499952236811\n",
      "Epoch 40, loss=0.6837201118469238\n",
      "Epoch 50, loss=0.684459904829661\n",
      "Epoch 60, loss=0.6836973081032435\n",
      "Epoch 70, loss=0.6828592220942179\n",
      "Epoch 80, loss=0.684044341246287\n",
      "Epoch 90, loss=0.6824019501606623\n",
      "Epoch 100, loss=0.683701162536939\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net2 = Net_small()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net2.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net2.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader_small:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net2(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader_small)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader_small)}')\n",
    "        # print(combined_loss / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                          embedding  label\n",
      "0    job  [0.5783542990684509, -0.08179537951946259, -0....      1\n",
      "1   body  [0.2662411034107208, -0.5605854392051697, 0.19...      1\n",
      "2  faith  [0.3823331594467163, -0.22967687249183655, -0....      1\n",
      "3  earth  [0.22662444412708282, -0.48906248807907104, 0....      1\n",
      "4  water  [-0.036070797592401505, -0.42757871747016907, ...      1\n",
      "          word                                          embedding  label\n",
      "181   somebody  [0.4352976679801941, -0.2624276280403137, -0.3...      0\n",
      "182         in  [0.33432042598724365, -0.49227994680404663, -0...      0\n",
      "183         on  [0.4110128879547119, -0.5570359230041504, -0.3...      0\n",
      "184      later  [0.2996409237384796, -0.44795429706573486, 0.0...      0\n",
      "185  something  [0.4520237445831299, -0.44595324993133545, -0....      0\n",
      "     word                                          embedding  label\n",
      "0    mind  [0.0946241021156311, -0.20241917669773102, 0.1...      1\n",
      "1   smile  [-0.13051141798496246, -2.0061619579792023e-05...      1\n",
      "2   color  [0.3938635587692261, 0.09373700618743896, -0.2...      1\n",
      "3  family  [0.5454207062721252, -0.14188426733016968, 0.4...      1\n",
      "4  defeat  [0.21724477410316467, 0.09596139192581177, -0....      1\n",
      "     word                                          embedding  label\n",
      "43  small  [0.15144889056682587, -0.6423326134681702, 0.2...      0\n",
      "44   well  [0.48836931586265564, -0.486169695854187, -0.7...      0\n",
      "45  early  [0.5112246870994568, -0.3049398958683014, -0.0...      0\n",
      "46   what  [0.7078254818916321, -1.1294100284576416, -0.6...      0\n",
      "47    nor  [0.7216946482658386, -0.44612595438957214, -0....      0\n",
      "\n",
      "Length of reduced embedding:  20\n"
     ]
    }
   ],
   "source": [
    "# create datasets with the 20 most important features by gradients\n",
    "df_train_small_grad = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['embedding'].apply(lambda x: [x[i] for i in combined_importance_grad]),\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "df_test_small_grad = pd.DataFrame({\n",
    "    'word': df_test['word'],\n",
    "    'embedding': df_test['embedding'].apply(lambda x: [x[i] for i in combined_importance_grad]),\n",
    "    'label': df_test['label']\n",
    "})\n",
    "\n",
    "print(df_train_small_grad[:5])\n",
    "print(df_train_small_grad[-5:])\n",
    "print(df_test_small_grad[:5])\n",
    "print(df_test_small_grad[-5:])\n",
    "\n",
    "print(\"\\nLength of reduced embedding: \", len(df_train_small_grad['embedding'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset_small_grad = CustomDataset(df_train_small_grad['embedding'], df_train_small_grad['label'])\n",
    "test_dataset_small_grad = CustomDataset(df_test_small_grad['embedding'], df_test_small_grad['label'])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader_small_grad = DataLoader(train_dataset_small_grad, batch_size=16, shuffle=True)\n",
    "test_loader_small_grad = DataLoader(test_dataset_small_grad, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.6651087254285812\n",
      "Epoch 10, loss=0.6638975292444229\n",
      "Epoch 20, loss=0.6628724982341131\n",
      "Epoch 30, loss=0.6649783452351888\n",
      "Epoch 40, loss=0.6622126897176107\n",
      "Epoch 50, loss=0.6649622370799383\n",
      "Epoch 60, loss=0.6641809791326523\n",
      "Epoch 70, loss=0.6631830135981241\n",
      "Epoch 80, loss=0.6627187331517538\n",
      "Epoch 90, loss=0.6652914732694626\n",
      "Epoch 100, loss=0.6618634164333344\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net3 = Net_small()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net3.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net3.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader_small_grad:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net3(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader_small_grad)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader_small_grad)}')\n",
    "        # print(combined_loss / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6861",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
