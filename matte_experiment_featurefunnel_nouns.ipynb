{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple torch model with 1 fully connected layer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    \"apple\", \"book\", \"car\", \"dog\", \"cat\", \"house\", \"tree\", \"friend\", \"time\", \"money\",\n",
    "    \"heart\", \"sun\", \"moon\", \"sky\", \"water\", \"fire\", \"earth\", \"flower\", \"city\", \"music\",\n",
    "    \"child\", \"parent\", \"school\", \"job\", \"love\", \"smile\", \"day\", \"night\", \"star\", \"cloud\",\n",
    "    \"bird\", \"fish\", \"food\", \"computer\", \"phone\", \"internet\", \"coffee\", \"tea\", \"shoes\", \"hat\",\n",
    "    \"dream\", \"goal\", \"team\", \"game\", \"hope\", \"fear\", \"joy\", \"anger\", \"peace\", \"war\",\n",
    "    \"friendship\", \"family\", \"health\", \"beauty\", \"knowledge\", \"power\", \"nature\", \"history\", \"science\",\n",
    "    \"art\", \"happiness\", \"sadness\", \"color\", \"mind\", \"body\", \"soul\", \"memory\", \"experience\", \"idea\",\n",
    "    \"faith\", \"truth\", \"lie\", \"problem\", \"solution\", \"question\", \"answer\", \"light\", \"darkness\", \"wind\",\n",
    "    \"rain\", \"snow\", \"smell\", \"taste\", \"touch\", \"sound\", \"silence\", \"joy\", \"freedom\", \"future\",\n",
    "    \"past\", \"present\", \"purpose\", \"journey\", \"adventure\", \"discovery\", \"challenge\", \"victory\", \"defeat\"\n",
    "]\n",
    "\n",
    "most_common_words = []\n",
    "\n",
    "# remove duplicates\n",
    "words = list(set(words))\n",
    "# most_common_words = list(set(most_common_words))\n",
    "\n",
    "print(len(words))\n",
    "print(len(most_common_words))\n",
    "\n",
    "assert len(list(set(words + most_common_words))) == len(words) + len(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "non_words = [\n",
    "    \"run\", \"jump\", \"eat\", \"sleep\", \"think\", \"happy\", \"fast\", \"slow\", \"beautiful\", \"smart\",\n",
    "    \"quickly\", \"easily\", \"always\", \"never\", \"under\", \"over\", \"beside\", \"between\", \"through\", \"with\",\n",
    "    \"and\", \"but\", \"or\", \"because\", \"although\", \"well\", \"yes\", \"no\", \"oh\", \"wow\", \"ouch\",\n",
    "    \"go\", \"come\", \"stop\", \"start\", \"win\", \"lose\", \"hot\", \"cold\", \"loud\", \"soft\",\n",
    "    \"bright\", \"dark\", \"high\", \"low\", \"near\", \"far\", \"big\", \"small\", \"old\", \"new\",\n",
    "    \"first\", \"last\", \"next\", \"previous\", \"good\", \"bad\", \"happy\", \"sad\", \"rich\", \"poor\",\n",
    "    \"early\", \"late\", \"hard\", \"easy\", \"simple\", \"complex\", \"right\", \"wrong\", \"true\", \"false\",\n",
    "    \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"up\", \"down\", \"here\", \"there\",\n",
    "    \"now\", \"then\", \"today\", \"tomorrow\", \"yesterday\", \"soon\", \"later\", \"before\", \"after\", \"while\",\n",
    "    \"once\", \"twice\", \"thrice\", \"nevertheless\", \"however\", \"suddenly\", \"finally\", \"quickly\", \"slowly\", \"nowadays\",\n",
    "    \"some\", \"many\", \"few\", \"most\", \"none\", \"all\", \"each\", \"every\", \"somebody\", \"nobody\",\n",
    "    \"something\", \"nothing\", \"somewhere\", \"nowhere\", \"anywhere\", \"everywhere\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"which\", \"what\", \"who\", \"whom\", \"whose\", \"where\", \"when\", \"why\", \"how\", \"whether\",\n",
    "    \"if\", \"unless\", \"because\", \"since\", \"although\", \"though\", \"while\", \"before\", \"after\", \"when\",\n",
    "    \"and\", \"or\", \"but\", \"nor\", \"for\", \"so\", \"yet\", \"either\", \"neither\", \"however\"\n",
    "]\n",
    "\n",
    "# remove duplicates\n",
    "non_words = list(set(non_words))\n",
    "\n",
    "print(len(non_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a list of words and returns a list of embeddings\n",
    "def get_embeddings(words):\n",
    "    embeddings = []\n",
    "\n",
    "    for word in words:\n",
    "        input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())\n",
    "    \n",
    "    assert len(embeddings) == len(words)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings(words)\n",
    "most_common_embeddings = get_embeddings(most_common_words)\n",
    "non_embeddings = get_embeddings(non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                          embedding  label\n",
      "0    job  [0.14350789785385132, -0.15940922498703003, -0...      1\n",
      "1   body  [0.7147403359413147, -0.1555342972278595, 0.31...      1\n",
      "2  faith  [0.6398938298225403, 0.13858512043952942, -0.1...      1\n",
      "3  earth  [0.26693275570869446, 0.4444328844547272, -0.1...      1\n",
      "4  water  [0.2036658525466919, 0.2685178220272064, -0.28...      1\n",
      "          word                                          embedding  label\n",
      "181   somebody  [-0.007474187761545181, -0.06840874254703522, ...      0\n",
      "182         in  [-0.8657609224319458, -0.5856645703315735, -0....      0\n",
      "183         on  [-0.11932921409606934, -0.3286181688308716, -0...      0\n",
      "184      later  [-0.2563585937023163, -0.453279972076416, -0.1...      0\n",
      "185  something  [-0.15402115881443024, -0.17753523588180542, -...      0\n",
      "     word                                          embedding  label\n",
      "0    mind  [0.38985970616340637, 0.27778860926628113, -0....      1\n",
      "1   smile  [-0.11715462803840637, 0.03211095184087753, 0....      1\n",
      "2   color  [0.05371911823749542, -0.42612069845199585, -0...      1\n",
      "3  family  [0.14181819558143616, -0.05798260122537613, -0...      1\n",
      "4  defeat  [-0.08488158136606216, 0.21536926925182343, -0...      1\n",
      "     word                                          embedding  label\n",
      "43  small  [-0.5499357581138611, -0.36507585644721985, -0...      0\n",
      "44   well  [0.4333745837211609, 0.05112234503030777, 0.03...      0\n",
      "45  early  [-0.7313716411590576, -0.021845733746886253, -...      0\n",
      "46   what  [-0.06226750835776329, -0.29066941142082214, -...      0\n",
      "47    nor  [-0.0868954211473465, -0.24122412502765656, -0...      0\n"
     ]
    }
   ],
   "source": [
    "# slice the lists into training and test sets\n",
    "words_train = words[:int(len(words)*0.8)]\n",
    "words_test = words[int(len(words)*0.8):]\n",
    "embeddings_train = embeddings[:int(len(embeddings)*0.8)]\n",
    "embeddings_test = embeddings[int(len(embeddings)*0.8):]\n",
    "\n",
    "most_common_words_train = most_common_words[:int(len(most_common_words)*0.8)]\n",
    "most_common_words_test = most_common_words[int(len(most_common_words)*0.8):]\n",
    "most_common_embeddings_train = most_common_embeddings[:int(len(most_common_embeddings)*0.8)]\n",
    "most_common_embeddings_test = most_common_embeddings[int(len(most_common_embeddings)*0.8):]\n",
    "\n",
    "non_words_train = non_words[:int(len(non_words)*0.8)]\n",
    "non_words_test = non_words[int(len(non_words)*0.8):]\n",
    "non_embeddings_train = non_embeddings[:int(len(non_embeddings)*0.8)]\n",
    "non_embeddings_test = non_embeddings[int(len(non_embeddings)*0.8):]\n",
    "\n",
    "# create a dataframe with the training and test sets\n",
    "df_train = pd.DataFrame({\n",
    "    'word': words_train + most_common_words_train + non_words_train,\n",
    "    'embedding': embeddings_train + most_common_embeddings_train + non_embeddings_train,\n",
    "    'label': [1]*len(words_train) + [1]*len(most_common_words_train) + [0]*len(non_words_train)\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'word': words_test + most_common_words_test + non_words_test,\n",
    "    'embedding': embeddings_test + most_common_embeddings_test + non_embeddings_test,\n",
    "    'label': [1]*len(words_test) + [1]*len(most_common_words_test) + [0]*len(non_words_test)\n",
    "})\n",
    "\n",
    "# shuffle the dataframes\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df_train[:5])\n",
    "print(df_train[-5:])\n",
    "print(df_test[:5])\n",
    "print(df_test[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = torch.tensor(tokenized_texts)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train['embedding'], df_train['label'])\n",
    "test_dataset = CustomDataset(df_test['embedding'], df_test['label'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.4711984694004059\n",
      "Epoch 10, loss=0.04769730878372987\n",
      "Epoch 20, loss=0.01954430671563993\n",
      "Epoch 30, loss=0.012031516913945476\n",
      "Epoch 40, loss=0.0076595131734696524\n",
      "Epoch 50, loss=0.0050835938794383155\n",
      "Epoch 60, loss=0.003941135480999947\n",
      "Epoch 70, loss=0.0030330109681623676\n",
      "Epoch 80, loss=0.0024938517793392143\n",
      "Epoch 90, loss=0.002030371077125892\n",
      "Epoch 100, loss=0.0016996313740188878\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader)}')\n",
    "        # print(combined_loss / len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of variance over different runs\n",
    "Best so far: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 100.00%\n",
      "Accuracy on test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # get data to GPU if possible\n",
    "            data = batch[0].to(device=device)\n",
    "            targets = batch[1].to(device=device)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    \n",
    "    return num_correct / num_samples\n",
    "\n",
    "print(f'Accuracy on training set: {check_accuracy(train_loader, net)*100:.2f}%')\n",
    "print(f'Accuracy on test set: {check_accuracy(test_loader, net)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n",
      "(10,)\n",
      "[308 289 522 282 163   4  41  34 314 313]\n",
      "[0.00462142 0.00220568 0.00209771 0.00206503 0.00171991 0.00157391\n",
      " 0.00152331 0.00146067 0.0013911  0.00134368]\n",
      "(10,)\n",
      "[191 161 733 158 601 576 170 217 558 626]\n",
      "[0.00191559 0.00170402 0.00164448 0.00149729 0.0014471  0.00139613\n",
      " 0.00137255 0.00134782 0.00133149 0.00127643]\n"
     ]
    }
   ],
   "source": [
    "# Find the most important features of the model\n",
    "# get the gradients of the model\n",
    "net.eval()\n",
    "gradients = []\n",
    "for batch in train_loader:\n",
    "    # get data to GPU if possible\n",
    "    data = batch[0].to(device=device)\n",
    "    targets = batch[1].to(device=device)\n",
    "\n",
    "    # forward\n",
    "    scores = net(data)\n",
    "    loss = criterion(scores, targets)\n",
    "    loss.backward()\n",
    "    gradients.append(net.fc1.weight.grad.detach().cpu().numpy())\n",
    "\n",
    "# average the gradients\n",
    "gradients = np.mean(gradients, axis=0)\n",
    "print(gradients.shape)\n",
    "\n",
    "# print(len(gradients))\n",
    "# print(len(gradients[0]))\n",
    "# print(len(gradients[0][0]))\n",
    "\n",
    "# find the indices of the 10 most important features\n",
    "most_important_gradients_0 = gradients[0].argsort()[-10:][::-1]\n",
    "most_important_gradients_1 = gradients[1].argsort()[-10:][::-1]\n",
    "\n",
    "# print the 10 most important features\n",
    "print(most_important_gradients_0.shape)\n",
    "print(most_important_gradients_0)\n",
    "print(gradients[0][most_important_gradients_0])\n",
    "\n",
    "print(most_important_gradients_1.shape)\n",
    "print(most_important_gradients_1)\n",
    "print(gradients[1][most_important_gradients_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 522, 282, 158, 289, 34, 163, 161, 41, 170, 558, 308, 313, 314, 191, 576, 217, 601, 733, 626]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "combined_importance_grad = list(set(list(most_important_gradients_0) + list(most_important_gradients_1)))\n",
    "print(combined_importance_grad)\n",
    "print(len(combined_importance_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[158 558 343 446 705 486 628 582 379 672]\n",
      "[0.36490867 0.34004512 0.33436078 0.3275797  0.32661593 0.32173577\n",
      " 0.32137144 0.31152323 0.3093089  0.30298623]\n",
      "(10,)\n",
      "[289 489 442   5 715 282 603 630 610 565]\n",
      "[0.36971068 0.36326203 0.33986703 0.33446872 0.33414614 0.32822368\n",
      " 0.32525712 0.32286963 0.32209966 0.32026997]\n"
     ]
    }
   ],
   "source": [
    "# Find the most important features for the model\n",
    "# get the weights of the fully connected layer\n",
    "weights = net.fc1.weight.data.cpu().numpy()\n",
    "\n",
    "# find the indices of the 10 most important features\n",
    "most_important_features_0 = weights[0].argsort()[-10:][::-1]\n",
    "most_important_features_1 = weights[1].argsort()[-10:][::-1]\n",
    "\n",
    "# print the 10 most important features and their weights\n",
    "print(most_important_features_0.shape)\n",
    "print(most_important_features_0)\n",
    "print(weights[0][most_important_features_0])\n",
    "\n",
    "print(most_important_features_1.shape)\n",
    "print(most_important_features_1)\n",
    "print(weights[1][most_important_features_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 282, 158, 672, 289, 558, 565, 442, 446, 705, 582, 715, 343, 603, 610, 486, 489, 628, 630, 379]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "combined_importance = list(set(list(most_important_features_0) + list(most_important_features_1)))\n",
    "print(combined_importance)\n",
    "print(len(combined_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_small(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_small, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu on the output of the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                          embedding  label\n",
      "0    job  [0.16212625801563263, -0.16747981309890747, 0....      1\n",
      "1   body  [0.3314345180988312, 0.19887007772922516, 0.26...      1\n",
      "2  faith  [0.11599896103143692, -0.09121094644069672, -0...      1\n",
      "3  earth  [0.33062461018562317, 0.118535615503788, -0.03...      1\n",
      "4  water  [0.5099238753318787, -0.38860055804252625, 0.0...      1\n",
      "          word                                          embedding  label\n",
      "181   somebody  [0.23361219465732574, -0.30281540751457214, 0....      0\n",
      "182         in  [-0.019756067544221878, -0.4305441677570343, 0...      0\n",
      "183         on  [0.3970641493797302, -0.3543335199356079, -0.2...      0\n",
      "184      later  [0.26222217082977295, 0.024935398250818253, 0....      0\n",
      "185  something  [-0.2308577299118042, -0.48253750801086426, 0....      0\n",
      "     word                                          embedding  label\n",
      "0    mind  [0.2802393436431885, 0.11320915818214417, 0.18...      1\n",
      "1   smile  [0.11796220391988754, -0.2812722325325012, 0.3...      1\n",
      "2   color  [0.4616974890232086, -0.20521026849746704, 0.1...      1\n",
      "3  family  [0.0861976221203804, 0.41470426321029663, -0.0...      1\n",
      "4  defeat  [0.08433376997709274, -0.15126396715641022, -0...      1\n",
      "     word                                          embedding  label\n",
      "43  small  [-0.028903763741254807, 0.26924052834510803, 0...      0\n",
      "44   well  [0.08194858580827713, -0.7078829407691956, 0.3...      0\n",
      "45  early  [0.11512470990419388, -0.08105532079935074, 0....      0\n",
      "46   what  [0.0487818568944931, -0.6702903509140015, 0.02...      0\n",
      "47    nor  [-0.24048839509487152, -0.37598857283592224, 0...      0\n",
      "\n",
      "Length of reduced embedding:  20\n"
     ]
    }
   ],
   "source": [
    "# create datasets with the 20 most important features\n",
    "df_train_small = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['embedding'].apply(lambda x: [x[i] for i in combined_importance]),\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "df_test_small = pd.DataFrame({\n",
    "    'word': df_test['word'],\n",
    "    'embedding': df_test['embedding'].apply(lambda x: [x[i] for i in combined_importance]),\n",
    "    'label': df_test['label']\n",
    "})\n",
    "\n",
    "print(df_train_small[:5])\n",
    "print(df_train_small[-5:])\n",
    "print(df_test_small[:5])\n",
    "print(df_test_small[-5:])\n",
    "\n",
    "print(\"\\nLength of reduced embedding: \", len(df_train_small['embedding'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset_small = CustomDataset(df_train_small['embedding'], df_train_small['label'])\n",
    "test_dataset_small = CustomDataset(df_test_small['embedding'], df_test_small['label'])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader_small = DataLoader(train_dataset_small, batch_size=16, shuffle=True)\n",
    "test_loader_small = DataLoader(test_dataset_small, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.6962780604759852\n",
      "Epoch 10, loss=0.6959789196650187\n",
      "Epoch 20, loss=0.6961021721363068\n",
      "Epoch 30, loss=0.6960124870141348\n",
      "Epoch 40, loss=0.6960191975037257\n",
      "Epoch 50, loss=0.6960007150967916\n",
      "Epoch 60, loss=0.696659043431282\n",
      "Epoch 70, loss=0.6952183991670609\n",
      "Epoch 80, loss=0.6960041423638662\n",
      "Epoch 90, loss=0.695601205031077\n",
      "Epoch 100, loss=0.6965534736712774\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net2 = Net_small()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net2.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net2.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader_small:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net2(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader_small)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader_small)}')\n",
    "        # print(combined_loss / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                          embedding  label\n",
      "0    job  [0.34226033091545105, -0.08179537951946259, -0...      1\n",
      "1   body  [0.10487144440412521, -0.5605854392051697, 0.1...      1\n",
      "2  faith  [0.43051886558532715, -0.22967687249183655, -0...      1\n",
      "3  earth  [0.6749990582466125, -0.48906248807907104, 0.1...      1\n",
      "4  water  [0.5786231160163879, -0.42757871747016907, -0....      1\n",
      "          word                                          embedding  label\n",
      "181   somebody  [0.0014382358640432358, -0.2624276280403137, -...      0\n",
      "182         in  [0.1521024852991104, -0.49227994680404663, -0....      0\n",
      "183         on  [0.2419023960828781, -0.5570359230041504, -0.3...      0\n",
      "184      later  [0.09961800277233124, -0.44795429706573486, 0....      0\n",
      "185  something  [0.29623791575431824, -0.44595324993133545, -0...      0\n",
      "     word                                          embedding  label\n",
      "0    mind  [0.36257031559944153, -0.20241917669773102, 0....      1\n",
      "1   smile  [0.5048218965530396, -2.0061619579792023e-05, ...      1\n",
      "2   color  [1.162677526473999, 0.09373700618743896, -0.20...      1\n",
      "3  family  [0.4889158308506012, -0.14188426733016968, 0.4...      1\n",
      "4  defeat  [0.22156865894794464, 0.09596139192581177, -0....      1\n",
      "     word                                          embedding  label\n",
      "43  small  [0.42144277691841125, -0.6423326134681702, 0.2...      0\n",
      "44   well  [0.33904269337654114, -0.486169695854187, -0.7...      0\n",
      "45  early  [0.48754242062568665, -0.3049398958683014, -0....      0\n",
      "46   what  [0.017239708453416824, -1.1294100284576416, -0...      0\n",
      "47    nor  [0.45213598012924194, -0.44612595438957214, -0...      0\n",
      "\n",
      "Length of reduced embedding:  20\n"
     ]
    }
   ],
   "source": [
    "# create datasets with the 20 most important features by gradients\n",
    "df_train_small_grad = pd.DataFrame({\n",
    "    'word': df_train['word'],\n",
    "    'embedding': df_train['embedding'].apply(lambda x: [x[i] for i in combined_importance_grad]),\n",
    "    'label': df_train['label']\n",
    "})\n",
    "\n",
    "df_test_small_grad = pd.DataFrame({\n",
    "    'word': df_test['word'],\n",
    "    'embedding': df_test['embedding'].apply(lambda x: [x[i] for i in combined_importance_grad]),\n",
    "    'label': df_test['label']\n",
    "})\n",
    "\n",
    "print(df_train_small_grad[:5])\n",
    "print(df_train_small_grad[-5:])\n",
    "print(df_test_small_grad[:5])\n",
    "print(df_test_small_grad[-5:])\n",
    "\n",
    "print(\"\\nLength of reduced embedding: \", len(df_train_small_grad['embedding'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset_small_grad = CustomDataset(df_train_small_grad['embedding'], df_train_small_grad['label'])\n",
    "test_dataset_small_grad = CustomDataset(df_test_small_grad['embedding'], df_test_small_grad['label'])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader_small_grad = DataLoader(train_dataset_small_grad, batch_size=16, shuffle=True)\n",
    "test_loader_small_grad = DataLoader(test_dataset_small_grad, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.6974899619817734\n",
      "Epoch 10, loss=0.7004535098870596\n",
      "Epoch 20, loss=0.6999430259068807\n",
      "Epoch 30, loss=0.7004847377538681\n",
      "Epoch 40, loss=0.6987025936444601\n",
      "Epoch 50, loss=0.6984369705120722\n",
      "Epoch 60, loss=0.6974718024333318\n",
      "Epoch 70, loss=0.6973331769307455\n",
      "Epoch 80, loss=0.698237473766009\n",
      "Epoch 90, loss=0.6975740840037664\n",
      "Epoch 100, loss=0.700429230928421\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "net3 = Net_small()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net3.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net3.train()\n",
    "    combined_loss = 0\n",
    "    for batch in train_loader_small_grad:\n",
    "        # get data to GPU if possible\n",
    "        data = batch[0].to(device=device)\n",
    "        targets = batch[1].to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = net3(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        combined_loss += loss.item()\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print initial loss\n",
    "    if epoch == 0:\n",
    "        print(f'Initial loss: {combined_loss/len(train_loader_small_grad)}')\n",
    "        \n",
    "    # print average loss per epoch every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss={combined_loss/len(train_loader_small_grad)}')\n",
    "        # print(combined_loss / len(train_loader))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6861",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
