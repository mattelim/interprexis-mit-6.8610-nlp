{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer + BERT → Data-processing → OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing an environment like conda is recommended. This notebook last ran on Python 3.8.18 without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: transformers in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (4.35.0)\n",
      "Requirement already satisfied: numpy in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: pandas in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: nltk in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: fsspec in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate transformers numpy pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# load in coco classes from 'coco-classes.json'\n",
    "import json\n",
    "with open('coco-classes.json') as f:\n",
    "  coco_classes = json.load(f)\n",
    "print(coco_classes)\n",
    "print(len(coco_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PennTreebank Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Penn Treebank POS tagset \n",
    "\n",
    "1. CC  Coordinating conjunction  \n",
    "2. CD  Cardinal number           \n",
    "3. DT  Determiner                \n",
    "4. EX  Existential there  \t     \n",
    "5. FW  Foreign word              \n",
    "6. IN  Preposition/subord. conjunction \t   \n",
    "7. JJ  Adjective                  \n",
    "8. JJR Adjective, comparative    \n",
    "9. JJS Adjective, superlative    \n",
    "10. LS  List item marker          \n",
    "11. MD  Modal                     \n",
    "12. NN  Noun, singular or mass    \n",
    "13. NNS Noun, plural              \n",
    "14. NNP Proper noun, singular     \n",
    "15. NNPS Proper noun, plural      \n",
    "16. PDT Predeterminer             \n",
    "17. POS Possessive ending         \n",
    "18. PRP Personal pronoun          \n",
    "19. PP  Possessive pronoun        \n",
    "20. RB  Adverb                    \n",
    "21. RBR Adverb, comparative       \n",
    "22. RBS Adverb, superlative       \n",
    "23. RP  Particle                  \n",
    "24. SYM Symbol \t\t\t             \n",
    "25. TO  to \n",
    "26. UH  Interjection \n",
    "27. VB  Verb, base form \n",
    "28. VBD Verb, past tense \n",
    "29. VBG Verb, gerund/present participle \n",
    "30. VBN Verb, past participle \n",
    "31. VBP Verb, non-3rd ps. sing. present\n",
    "32. VBZ Verb, 3rd ps. sing. present \n",
    "33. WDT wh-determiner \n",
    "34. WP  wh-pronoun \n",
    "35. WP  Possessive wh-pronoun \n",
    "36. WRB wh-adverb \n",
    "37. \\#  Pound sign \n",
    "38. $  Dollar sign \n",
    "39. .  Sentence-final punctuation \n",
    "40. ,  Comma \n",
    "41. :  Colon, semi-colon \n",
    "42. (  Left bracket character \n",
    "43. )  Right bracket character \n",
    "44. \"  Straight double quote \n",
    "45. `  Left open single quote \n",
    "46. \"  Left open double quote \n",
    "47. '  Right close single quote \n",
    "48. \"  Right close double quote\n",
    "\n",
    "For examples: https://www.sketchengine.eu/penn-treebank-tagset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/mattelim/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3914\n",
      "Number of tagged words: 100676\n",
      "\n",
      "Sample of the dataset:\n",
      "Sentence 1: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "POS tags: [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "\n",
      "Sentence 2: ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.']\n",
      "POS tags: [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]\n",
      "\n",
      "Sentence 3: ['Rudolph', 'Agnew', ',', '55', 'years', 'old', 'and', 'former', 'chairman', 'of', 'Consolidated', 'Gold', 'Fields', 'PLC', ',', 'was', 'named', '*-1', 'a', 'nonexecutive', 'director', 'of', 'this', 'British', 'industrial', 'conglomerate', '.']\n",
      "POS tags: [('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('*-1', '-NONE-'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('of', 'IN'), ('this', 'DT'), ('British', 'JJ'), ('industrial', 'JJ'), ('conglomerate', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "Pandas DataFrame:\n",
      "     Word  POS\n",
      "0  Pierre  NNP\n",
      "1  Vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n"
     ]
    }
   ],
   "source": [
    "# Download the Penn Treebank dataset\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Load the Penn Treebank dataset\n",
    "ptb_sentences = treebank.sents()\n",
    "ptb_tagged_words = treebank.tagged_words()\n",
    "\n",
    "# Display some information about the dataset\n",
    "print(f\"Number of sentences: {len(ptb_sentences)}\")\n",
    "print(f\"Number of tagged words: {len(ptb_tagged_words)}\")\n",
    "\n",
    "# Display the first few sentences and their POS tags\n",
    "print(\"\\nSample of the dataset:\")\n",
    "current_word_position = 0\n",
    "for i in range(3):\n",
    "    print(f\"Sentence {i + 1}: {ptb_sentences[i]}\")\n",
    "    ending_word_position = current_word_position + len(ptb_sentences[i])\n",
    "    print(f\"POS tags: {ptb_tagged_words[current_word_position:ending_word_position]}\")\n",
    "    current_word_position = ending_word_position\n",
    "    print()\n",
    "\n",
    "# Convert the dataset to Pandas DataFrame for exploration\n",
    "columns = ['Word', 'POS']\n",
    "ptb_df = pd.DataFrame(data={'Word': [word for (word, _) in ptb_tagged_words],\n",
    "                            'POS': [pos for (_, pos) in ptb_tagged_words]}, columns=columns)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nPandas DataFrame:\")\n",
    "print(ptb_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100676, 2)\n"
     ]
    }
   ],
   "source": [
    "print(ptb_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to csv\n",
    "ptb_df.to_csv('ptb_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13781, 3)\n"
     ]
    }
   ],
   "source": [
    "# reduce dataframe to unique words\n",
    "# ptb_df_unique = ptb_df.drop_duplicates(subset=['Word'])\n",
    "# print(ptb_df_unique.shape)\n",
    "\n",
    "# reduce dataframe to unique words, count the number of times each word appears and add as a column, while keeping the POS column\n",
    "ptb_df_unique = ptb_df.groupby(['Word', 'POS']).size().reset_index(name='count')\n",
    "# ptb_df_unique = ptb_df.drop_duplicates(subset=['Word'])\n",
    "print(ptb_df_unique.shape)\n",
    "\n",
    "# save the dataframe to csv\n",
    "ptb_df_unique.to_csv('ptb_df_unique.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13341, 3)\n",
      "(13337, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove '-NONE-' POS tags\n",
    "ptb_df_unique = ptb_df_unique[ptb_df_unique['POS'] != '-NONE-']\n",
    "print(ptb_df_unique.shape)\n",
    "\n",
    "# Remove '-LRB-' '-RRB-' POS tags\n",
    "ptb_df_unique = ptb_df_unique[ptb_df_unique['POS'] != '-LRB-']\n",
    "ptb_df_unique = ptb_df_unique[ptb_df_unique['POS'] != '-RRB-']\n",
    "print(ptb_df_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 3)\n",
      "  Word POS  count\n",
      "0    #   #     16\n",
      "1    $   $    718\n",
      "2  US$   $      4\n",
      "3   C$   $      2\n",
      "4   ''  ''    684\n",
      "5    '  ''     10\n",
      "6    ,   ,   4885\n",
      "7   Wa   ,      1\n",
      "8    .   .   3828\n",
      "9    ?   .     40\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe that contains the 5 most frequent words of each POS tag\n",
    "ptb_df_top5 = ptb_df_unique.groupby('POS').apply(lambda x: x.nlargest(5, 'count')).reset_index(drop=True)\n",
    "print(ptb_df_top5.shape)\n",
    "print(ptb_df_top5.head(10))\n",
    "\n",
    "# save the dataframe to csv\n",
    "ptb_df_top5.to_csv('ptb_df_top5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get 100 most common words\n",
    "# ptb_df_unique.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "# ptb_df_unique_top100 = ptb_df_unique.head(100)\n",
    "# print(ptb_df_unique_top100.shape)\n",
    "\n",
    "# # save the dataframe to csv\n",
    "# ptb_df_unique_top100.to_csv('ptb_df_unique_top100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "{'companion', 'toast', 'lounge', 'watch', 'haul', 'combination', 'majesty', 'extinguish', 'nutrient', 'bark', 'rest', 'hold', 'drainage', 'chill', 'time', 'drink', 'eat', 'journey', 'surf', 'shipment', 'measure', 'devour', 'cutlery', 'catch', 'drive', 'cooling', 'sail', 'flutter', 'moo', 'descend', 'commute', 'score', 'indulgence', 'cutting', 'containment', 'communication', 'transportation', 'glide', 'slice', 'drying', 'tote', 'nurture', 'breakfast', 'cut', 'throw', 'brush', 'competition', 'signal', 'carry', 'mobility', 'dine', 'conformity', 'Thanksgiving', 'comfort', 'call', 'soar', 'pause', 'input', 'hygiene', 'click', 'health', 'entertainment', 'halt', 'crunch', 'softness', 'control', 'loyalty', 'formality', 'individuality', 'gallop', 'protection', 'gathering', 'pattern', 'walk', 'productivity', 'purr', 'sip', 'decoration', 'bite', 'accessory', 'reflection', 'rinse', 'uncork', 'skate', 'grace', 'support', 'scoop', 'tick', 'flight', 'type', 'read', 'bake', 'navigation', 'poke', 'descent', 'voyage', 'heating', 'thrill', 'consume', 'fly', 'hug', 'knowledge', 'shield', 'transit', 'solitude', 'heat', 'fasten', 'celebration', 'hit', 'flush', 'adventure', 'peel', 'song', 'elegance', 'recreation', 'roar', 'trumpet', 'container', 'aroma', 'childhood', 'measurement', 'sanitation', 'vibration', 'dry', 'growth', 'safety', 'swing', 'crave', 'ride', 'sharpness', 'stride', 'graze', 'excitement', 'sit', 'indulge', 'acidity', 'prong', 'chop', 'munch'}\n"
     ]
    }
   ],
   "source": [
    "# Generated using ChatGPT 3.5, not foolproof\n",
    "\n",
    "verb_equivalents = {\n",
    "    'person': 'walk',\n",
    "    'bicycle': 'ride',\n",
    "    'car': 'drive',\n",
    "    'motorcycle': 'ride',\n",
    "    'airplane': 'fly',\n",
    "    'bus': 'transit',\n",
    "    'train': 'commute',\n",
    "    'truck': 'haul',\n",
    "    'boat': 'sail',\n",
    "    'traffic light': 'control',\n",
    "    'fire hydrant': 'extinguish',\n",
    "    'stop sign': 'halt',\n",
    "    'parking meter': 'measure',\n",
    "    'bench': 'sit',\n",
    "    'bird': 'soar',\n",
    "    'cat': 'purr',\n",
    "    'dog': 'bark',\n",
    "    'horse': 'gallop',\n",
    "    'sheep': 'graze',\n",
    "    'cow': 'moo',\n",
    "    'elephant': 'trumpet',\n",
    "    'bear': 'roar',\n",
    "    'zebra': 'stride',\n",
    "    'giraffe': 'graze',\n",
    "    'backpack': 'carry',\n",
    "    'umbrella': 'shield',\n",
    "    'handbag': 'tote',\n",
    "    'tie': 'fasten',\n",
    "    'suitcase': 'carry',\n",
    "    'frisbee': 'throw',\n",
    "    'skis': 'descend',\n",
    "    'snowboard': 'glide',\n",
    "    'sports ball': 'throw',\n",
    "    'kite': 'flutter',\n",
    "    'baseball bat': 'swing',\n",
    "    'baseball glove': 'catch',\n",
    "    'skateboard': 'skate',\n",
    "    'surfboard': 'surf',\n",
    "    'tennis racket': 'hit',\n",
    "    'bottle': 'uncork',\n",
    "    'wine glass': 'sip',\n",
    "    'cup': 'drink',\n",
    "    'fork': 'poke',\n",
    "    'knife': 'slice',\n",
    "    'spoon': 'scoop',\n",
    "    'bowl': 'eat',\n",
    "    'banana': 'peel',\n",
    "    'apple': 'bite',\n",
    "    'sandwich': 'devour',\n",
    "    'orange': 'peel',\n",
    "    'broccoli': 'munch',\n",
    "    'carrot': 'chop',\n",
    "    'hot dog': 'consume',\n",
    "    'pizza': 'devour',\n",
    "    'donut': 'crave',\n",
    "    'cake': 'indulge',\n",
    "    'chair': 'sit',\n",
    "    'couch': 'lounge',\n",
    "    'potted plant': 'nurture',\n",
    "    'bed': 'rest',\n",
    "    'dining table': 'dine',\n",
    "    'toilet': 'flush',\n",
    "    'tv': 'watch',\n",
    "    'laptop': 'type',\n",
    "    'mouse': 'click',\n",
    "    'remote': 'control',\n",
    "    'keyboard': 'type',\n",
    "    'cell phone': 'call',\n",
    "    'microwave': 'heat',\n",
    "    'oven': 'bake',\n",
    "    'toaster': 'toast',\n",
    "    'sink': 'rinse',\n",
    "    'refrigerator': 'chill',\n",
    "    'book': 'read',\n",
    "    'clock': 'tick',\n",
    "    'vase': 'hold',\n",
    "    'scissors': 'cut',\n",
    "    'teddy bear': 'hug',\n",
    "    'hair drier': 'dry',\n",
    "    'toothbrush': 'brush'\n",
    "}\n",
    "\n",
    "abstract_equivalents = {\n",
    "    'person': 'individuality',\n",
    "    'bicycle': 'mobility',\n",
    "    'car': 'transportation',\n",
    "    'motorcycle': 'vibration',\n",
    "    'airplane': 'flight',\n",
    "    'bus': 'transit',\n",
    "    'train': 'journey',\n",
    "    'truck': 'shipment',\n",
    "    'boat': 'voyage',\n",
    "    'traffic light': 'signal',\n",
    "    'fire hydrant': 'safety',\n",
    "    'stop sign': 'pause',\n",
    "    'parking meter': 'measurement',\n",
    "    'bench': 'reflection',\n",
    "    'bird': 'song',\n",
    "    'cat': 'companion',\n",
    "    'dog': 'loyalty',\n",
    "    'horse': 'grace',\n",
    "    'sheep': 'conformity',\n",
    "    'cow': 'moo',\n",
    "    'elephant': 'majesty',\n",
    "    'bear': 'solitude',\n",
    "    'zebra': 'pattern',\n",
    "    'giraffe': 'elegance',\n",
    "    'backpack': 'adventure',\n",
    "    'umbrella': 'protection',\n",
    "    'handbag': 'accessory',\n",
    "    'tie': 'formality',\n",
    "    'suitcase': 'journey',\n",
    "    'frisbee': 'recreation',\n",
    "    'skis': 'glide',\n",
    "    'snowboard': 'descent',\n",
    "    'sports ball': 'competition',\n",
    "    'kite': 'soar',\n",
    "    'baseball bat': 'swing',\n",
    "    'baseball glove': 'protection',\n",
    "    'skateboard': 'thrill',\n",
    "    'surfboard': 'excitement',\n",
    "    'tennis racket': 'score',\n",
    "    'bottle': 'containment',\n",
    "    'wine glass': 'celebration',\n",
    "    'cup': 'containment',\n",
    "    'fork': 'prong',\n",
    "    'knife': 'sharpness',\n",
    "    'spoon': 'cutlery',\n",
    "    'bowl': 'container',\n",
    "    'banana': 'softness',\n",
    "    'apple': 'crunch',\n",
    "    'sandwich': 'combination',\n",
    "    'orange': 'acidity',\n",
    "    'broccoli': 'nutrient',\n",
    "    'carrot': 'health',\n",
    "    'hot dog': 'indulgence',\n",
    "    'pizza': 'aroma',\n",
    "    'donut': 'indulgence',\n",
    "    'cake': 'celebration',\n",
    "    'chair': 'support',\n",
    "    'couch': 'comfort',\n",
    "    'potted plant': 'growth',\n",
    "    'bed': 'rest',\n",
    "    'dining table': 'gathering',\n",
    "    'toilet': 'sanitation',\n",
    "    'tv': 'entertainment',\n",
    "    'laptop': 'productivity',\n",
    "    'mouse': 'navigation',\n",
    "    'remote': 'control',\n",
    "    'keyboard': 'input',\n",
    "    'cell phone': 'communication',\n",
    "    'microwave': 'heating',\n",
    "    'oven': 'Thanksgiving',\n",
    "    'toaster': 'breakfast',\n",
    "    'sink': 'drainage',\n",
    "    'refrigerator': 'cooling',\n",
    "    'book': 'knowledge',\n",
    "    'clock': 'time',\n",
    "    'vase': 'decoration',\n",
    "    'scissors': 'cutting',\n",
    "    'teddy bear': 'childhood',\n",
    "    'hair drier': 'drying',\n",
    "    'toothbrush': 'hygiene'\n",
    "}\n",
    "\n",
    "# create a unique set using the values from both dictionaries\n",
    "unique_equivalents = set(list(verb_equivalents.values()) + list(abstract_equivalents.values()))\n",
    "print(len(unique_equivalents))\n",
    "print(unique_equivalents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI ChatGPT 3.5 generated POS tags\n",
    "\n",
    "word_pos_dict = {\n",
    "    'companion': 'NN',\n",
    "    'toast': 'NN',\n",
    "    'lounge': 'NN',\n",
    "    'watch': 'VB',\n",
    "    'haul': 'VB',\n",
    "    'combination': 'NN',\n",
    "    'majesty': 'NN',\n",
    "    'extinguish': 'VB',\n",
    "    'nutrient': 'NN',\n",
    "    'bark': 'NN',\n",
    "    'rest': 'NN',\n",
    "    'hold': 'VB',\n",
    "    'drainage': 'NN',\n",
    "    'chill': 'VB',\n",
    "    'time': 'NN',\n",
    "    'drink': 'NN',\n",
    "    'eat': 'VB',\n",
    "    'journey': 'NN',\n",
    "    'surf': 'VB',\n",
    "    'shipment': 'NN',\n",
    "    'measure': 'NN',\n",
    "    'devour': 'VB',\n",
    "    'cutlery': 'NN',\n",
    "    'catch': 'NN',\n",
    "    'drive': 'VB',\n",
    "    'cooling': 'VBG',\n",
    "    'sail': 'VB',\n",
    "    'flutter': 'NN',\n",
    "    'moo': 'NN',\n",
    "    'descend': 'VB',\n",
    "    'commute': 'NN',\n",
    "    'score': 'NN',\n",
    "    'indulgence': 'NN',\n",
    "    'cutting': 'NN',\n",
    "    'containment': 'NN',\n",
    "    'communication': 'NN',\n",
    "    'transportation': 'NN',\n",
    "    'glide': 'VB',\n",
    "    'slice': 'NN',\n",
    "    'drying': 'NN',\n",
    "    'tote': 'NN',\n",
    "    'nurture': 'NN',\n",
    "    'breakfast': 'NN',\n",
    "    'cut': 'NN',\n",
    "    'throw': 'VB',\n",
    "    'brush': 'NN',\n",
    "    'competition': 'NN',\n",
    "    'signal': 'NN',\n",
    "    'carry': 'VB',\n",
    "    'mobility': 'NN',\n",
    "    'dine': 'VB',\n",
    "    'conformity': 'NN',\n",
    "    'Thanksgiving': 'NNP',\n",
    "    'comfort': 'NN',\n",
    "    'call': 'VB',\n",
    "    'soar': 'VB',\n",
    "    'pause': 'NN',\n",
    "    'input': 'NN',\n",
    "    'hygiene': 'NN',\n",
    "    'click': 'NN',\n",
    "    'health': 'NN',\n",
    "    'entertainment': 'NN',\n",
    "    'halt': 'NN',\n",
    "    'crunch': 'NN',\n",
    "    'softness': 'NN',\n",
    "    'control': 'NN',\n",
    "    'loyalty': 'NN',\n",
    "    'formality': 'NN',\n",
    "    'individuality': 'NN',\n",
    "    'gallop': 'NN',\n",
    "    'protection': 'NN',\n",
    "    'gathering': 'NN',\n",
    "    'pattern': 'NN',\n",
    "    'walk': 'VB',\n",
    "    'productivity': 'NN',\n",
    "    'purr': 'NN',\n",
    "    'sip': 'VB',\n",
    "    'decoration': 'NN',\n",
    "    'bite': 'NN',\n",
    "    'accessory': 'NN',\n",
    "    'reflection': 'NN',\n",
    "    'rinse': 'VB',\n",
    "    'uncork': 'VB',\n",
    "    'skate': 'VB',\n",
    "    'grace': 'NN',\n",
    "    'support': 'NN',\n",
    "    'scoop': 'NN',\n",
    "    'tick': 'NN',\n",
    "    'flight': 'NN',\n",
    "    'type': 'VB',\n",
    "    'read': 'VB',\n",
    "    'bake': 'VB',\n",
    "    'navigation': 'NN',\n",
    "    'poke': 'VB',\n",
    "    'descent': 'NN',\n",
    "    'voyage': 'NN',\n",
    "    'heating': 'VBG',\n",
    "    'thrill': 'NN',\n",
    "    'consume': 'VB',\n",
    "    'fly': 'VB',\n",
    "    'hug': 'NN',\n",
    "    'knowledge': 'NN',\n",
    "    'shield': 'NN',\n",
    "    'transit': 'NN',\n",
    "    'solitude': 'NN',\n",
    "    'heat': 'NN',\n",
    "    'fasten': 'VB',\n",
    "    'celebration': 'NN',\n",
    "    'hit': 'VB',\n",
    "    'flush': 'VB',\n",
    "    'adventure': 'NN',\n",
    "    'peel': 'VB',\n",
    "    'song': 'NN',\n",
    "    'elegance': 'NN',\n",
    "    'recreation': 'NN',\n",
    "    'roar': 'NN',\n",
    "    'trumpet': 'NN',\n",
    "    'container': 'NN',\n",
    "    'aroma': 'NN',\n",
    "    'childhood': 'NN',\n",
    "    'measurement': 'NN',\n",
    "    'sanitation': 'NN',\n",
    "    'vibration': 'NN',\n",
    "    'dry': 'VB',\n",
    "    'growth': 'NN',\n",
    "    'safety': 'NN',\n",
    "    'swing': 'NN',\n",
    "    'crave': 'VB',\n",
    "    'ride': 'VB',\n",
    "    'sharpness': 'NN',\n",
    "    'stride': 'NN',\n",
    "    'graze': 'NN',\n",
    "    'excitement': 'NN',\n",
    "    'sit': 'VB',\n",
    "    'indulge': 'VB',\n",
    "    'acidity': 'NN',\n",
    "    'prong': 'NN',\n",
    "    'chop': 'VB',\n",
    "    'munch': 'NN'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>companion</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toast</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lounge</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch</td>\n",
       "      <td>VB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haul</td>\n",
       "      <td>VB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word POS  count\n",
       "0  companion  NN      0\n",
       "1      toast  NN      0\n",
       "2     lounge  NN      0\n",
       "3      watch  VB      0\n",
       "4       haul  VB      0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from the dictionary, with the key as 'Word' and the value as 'POS'\n",
    "# for now, set the 'count' column to 0\n",
    "word_pos_df = pd.DataFrame(list(word_pos_dict.items()), columns=['Word', 'POS'])\n",
    "word_pos_df['count'] = 0\n",
    "print(word_pos_df.shape)\n",
    "word_pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>motorcycle</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airplane</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS  count\n",
       "0      person  NN      0\n",
       "1     bicycle  NN      0\n",
       "2         car  NN      0\n",
       "3  motorcycle  NN      0\n",
       "4    airplane  NN      0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe using coco classes, with POS tags set to NN and count set to 0\n",
    "coco_classes_df = pd.DataFrame(coco_classes, columns=['Word'])\n",
    "coco_classes_df['POS'] = 'NN'\n",
    "coco_classes_df['count'] = 0\n",
    "print(coco_classes_df.shape)\n",
    "coco_classes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 3)\n",
      "        Word POS  count\n",
      "0  companion  NN      0\n",
      "1      toast  NN      0\n",
      "2     lounge  NN      0\n",
      "3      watch  VB      0\n",
      "4       haul  VB      0\n",
      "(383, 3)\n",
      "        Word POS  count\n",
      "0  companion  NN      0\n",
      "1      toast  NN      0\n",
      "2     lounge  NN      0\n",
      "3      watch  VB      0\n",
      "4       haul  VB      0\n"
     ]
    }
   ],
   "source": [
    "# Combine the word_pos_df, coco_classes_df dataframes\n",
    "combined_df = pd.concat([word_pos_df, coco_classes_df])\n",
    "\n",
    "# Remove duplicates\n",
    "combined_df = combined_df.drop_duplicates(subset=['Word'])\n",
    "\n",
    "# Combine the combined_df and ptb_df_top5, prioritizing the ptb_df_top5 dataframe if there are duplicates\n",
    "combined_df = pd.concat([combined_df, ptb_df_top5])\n",
    "print(combined_df.shape)\n",
    "print(combined_df.head())\n",
    "combined_df = combined_df.drop_duplicates(subset=['Word'], keep='last')\n",
    "print(combined_df.shape)\n",
    "print(combined_df.head())\n",
    "\n",
    "# save the dataframe to csv\n",
    "combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a dataframe from the unique set, with 'Word', 'POS', and 'count' columns\n",
    "# # for now, set 'POS' to 'TBD' and 'count' to 0\n",
    "# equivalents_df = pd.DataFrame(unique_equivalents, columns=['Word'])\n",
    "# equivalents_df['POS'] = 'TBD'\n",
    "# equivalents_df['count'] = 0\n",
    "# print(equivalents_df.shape)\n",
    "# print(equivalents_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine equivalents_df and ptb_df_top5\n",
    "# combined_df = pd.concat([ptb_df_top5, equivalents_df])\n",
    "# print(combined_df.shape)\n",
    "# print(combined_df.head())\n",
    "\n",
    "# # save the dataframe to csv\n",
    "# combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a superset of set of unique words from both dictionaries and the top 5 words from each POS tag and the original coco classes\n",
    "# superset = set(list(unique_equivalents) + list(ptb_df_top5['Word']) + list(coco_classes))\n",
    "# print(len(superset))\n",
    "# print(superset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n",
      "['companion', 'toast', 'lounge', 'watch', 'haul']\n"
     ]
    }
   ],
   "source": [
    "# Convert combined_df['Word'] to list\n",
    "combined_list = combined_df['Word'].tolist()\n",
    "print(len(combined_list))\n",
    "print(combined_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding for each class\n",
    "# ❗️ note: I am only getting the embedding for the first token in each class\n",
    "# ❓ question: are we interested in the final contextual embedding for each class? currently, we're looking at the final hidden state.\n",
    "embeddings = []\n",
    "for i in range(len(combined_list)):\n",
    "# for i in range(1):\n",
    "    input_ids = torch.tensor(tokenizer.encode(combined_list[i])).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]\n",
    "    # skip the first and last token, which is the [CLS] and [SEP] tokens\n",
    "    # take the mean of other tokens (that form the word)    \n",
    "    embeddings.append(torch.mean(last_hidden_states[0][1:-1], dim=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round each val in embedding to 3 decimal places\n",
    "embeddings = [list(np.around(np.array(e),3)) for e in embeddings]\n",
    "\n",
    "# create string of all classes and their embeddings & save to text file\n",
    "# ❗️ note: only taking first 10 axes for now due to context window length\n",
    "# with open(\"output.txt\", \"w\") as text_file:\n",
    "#     for i in range(len(combined_list)):\n",
    "#         class_str = f\"{combined_list[i]}: {embeddings[i][:10]}\\n\"\n",
    "#         text_file.write(class_str)\n",
    "with open(\"output.txt\", \"w\") as text_file:\n",
    "    for i in range(len(combined_list)):\n",
    "        class_str = f\"{combined_list[i]}: {embeddings[i][0]}\\n\"\n",
    "        text_file.write(class_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383, 769)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>companion</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toast</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lounge</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.933</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haul</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word      0      1      2      3      4      5      6      7      8  \\\n",
       "0  companion  0.128 -0.082  0.109 -0.012  0.317  0.262 -0.016  0.128  0.357   \n",
       "1      toast  0.209  0.411  0.026 -0.000 -0.243 -0.325  0.148  0.238 -0.120   \n",
       "2     lounge  0.759 -0.116  0.116  0.113  0.460 -0.212  0.115 -0.032  0.137   \n",
       "3      watch  0.401 -0.003 -0.061 -0.406  0.933 -0.140 -0.186  0.286 -0.170   \n",
       "4       haul  0.580  0.031  0.311  0.048 -0.102  0.081  0.047  0.423 -0.187   \n",
       "\n",
       "   ...    758    759    760    761    762    763    764    765    766    767  \n",
       "0  ...  0.395 -0.107 -0.026 -0.061  0.358 -0.138 -0.067  0.236  0.385 -0.115  \n",
       "1  ...  0.373 -0.174  0.119 -0.036  0.445 -0.029  0.145  0.169  0.364 -0.097  \n",
       "2  ...  0.259  0.070 -0.059 -0.114  0.441 -0.096 -0.074  0.196  0.215 -0.254  \n",
       "3  ...  0.459 -0.067 -0.266 -0.318  0.173 -0.109  0.219 -0.010  0.404 -0.161  \n",
       "4  ...  0.505  0.096  0.095 -0.009  0.453  0.003  0.337  0.259  0.064  0.092  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert embedding list to dataframe\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(embeddings)\n",
    "df.insert(0, 'word', combined_list)\n",
    "print(df.shape)\n",
    "df.head()  # Display the first 5 rows to check the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI ChatGPT 3.5 generated test set\n",
    "\n",
    "daily_life_objects = ['coffee mug', 'newspaper', 'shoes', 'headphones', 'umbrella stand', 'trash can', 'escalator', 'delivery van', 'gardening hose', 'street sign', 'mailbox', 'garage door', 'picnic table', 'seagull sculpture', 'houseplant', 'lap desk', 'home office chair', 'calendar', 'wallet', 'sunglasses', 'notebook', 'desktop computer', 'printer', 'office desk lamp', 'USB drive', 'water bottle', 'wine opener', 'mason jar', 'serving spoon', 'chopsticks', 'plate', 'napkin', 'apple slicer', 'cooking spatula', 'baking pan', 'cookie jar', 'tea kettle', 'candle', 'throw pillow', 'blanket', 'house slippers', 'bathroom scale', 'vanity mirror', 'alarm clock', 'picture frame', 'cactus plant', 'bookshelf', 'wall clock', 'wristwatch', 'reading glasses', 'hairbrush', 'hair tie', 'hand mirror', 'shaving razor', 'toilet paper', 'tissue box', 'paper towel holder', 'flashlight', 'laptop sleeve', 'computer mouse pad', 'USB cable', 'keyboard cover', 'wireless router', 'smartphone stand', 'kitchen apron', 'oven mitts', 'pot holder', 'cutting board', 'salt and pepper shakers', 'napkin holder', 'dish rack', 'wine rack', 'picture album', 'canvas tote bag', 'office phone', 'desk organizer', 'magnetic board']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might be better to just use the GUI. If we want to directly manipulate the outputs we may need to do some precise prompt engineering. OpenAI has a JSON feature that we could look into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (0.28.1)\n",
      "Collecting openai\n",
      "  Downloading openai-1.2.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from openai)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Collecting sniffio>=1.1 (from anyio<4,>=3.5.0->openai)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup (from anyio<4,>=3.5.0->openai)\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: certifi in /Users/mattelim/miniforge3/envs/6861/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.10.1-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.2.3-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install OpenAI api\n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api key from secrets.json\n",
    "import openai\n",
    "\n",
    "try:\n",
    "    with open(\"secrets.json\") as f:\n",
    "        secrets = json.load(f)\n",
    "    my_api_key = secrets[\"openai\"]\n",
    "    print(\"API key loaded.\")\n",
    "    openai.api_key = my_api_key\n",
    "except FileNotFoundError:\n",
    "    print(\"Secrets file not found. YOU NEED THEM TO RUN THIS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"There are 6 dimensions.\\n\\nRow 1: 3D shape. The values for this dimension vary significantly across the labels, indicating that it encodes information about the three-dimensional shape of the objects.\\nRow 2: Mobility. The values for this dimension are mostly positive, suggesting that it encodes information about the mobility or movement associated with the objects.\\nRow 3: Edibility. The values for this dimension are a mix of positive and negative, but they are generally low, indicating that it encodes information about the edibility of the objects.\\nRow 4: Size. The values for this dimension range from negative to positive, suggesting that it encodes information about the size or scale of the objects.\\nRow 5: Consumer goods. The values for this dimension are mostly negative, indicating that it encodes information about whether the objects are commonly used consumer goods.\\nRow 6: Natural vs. Man-made. The values for this dimension vary significantly across the labels, suggesting that it encodes information about whether the objects are natural or man-made.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"For the following lists, the first list contains words that have been put into DistilBERT – let us call this a label. Each of the subsequent lists contains the embedding value from DistilBERT for one dimension (out of 768) across the labels. By comparing the values for each label for each list, please interpret the likely concepts that each list, that is the dimension/axis of the embedding, encodes. Each of the 50 rows should encode a different concept. \\n\\n First, count the number of lists excluding the first (the labels list) and report on the number. \\n '''There are <N> dimensions''' \\n\\n Then, the main output should take this form for row 'n', from 'Row 1' to 'Row N': \\n '''Row n: <encoded concept>. <one sentence rationale for interpretation>''' \\n\\n {slice_0}\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\n",
    "# log the stringified output into a txt file by appending it to the end of the file\n",
    "with open(\"output.txt\", \"a\") as f:\n",
    "  f.write(str(completion.choices[0].message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
